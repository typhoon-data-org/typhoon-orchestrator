{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Elegant YAML DAGS for Data Pipelines Deploy to your existing Airflow. Why Typhoon Write Airflow DAGS faster Make creating reliable data pipelines easy for the whole team Simplicity and re-usability; a toolkit designed to be loved by Data Engineers Deploying to your existing Airflow with zero risk and no migration work Workflow : Typhoon YAML DAG --> transpile --> Airflow DAG Key Principles Elegant : YAML; low-code and easy to pick up. Data sharing - data flows between tasks making it intuitive and easy to build tasks. Composability - Functions combine like Lego. Effortless to extend for more sources and connections. Components - reduce complex tasks (e.g. CSV\u2192S3\u2192Snowflake) to 1 re-usable task. UI : Component UI for sharing DAG configuration with your DWH, Analyst or Data Sci. teams. Rich Cli : Inspired by other great command line interfaces and instantly familiar. Intelligent bash/zsh completion. Testable Tasks - automate DAG task tests. Testable Python - test functions or full DAGs with PyTest. Flexible deployment : deploy to Airflow - large reduction in effort, without breaking existing production. Layers Pristine : Pre-built (OSS) components and UI that can be shared to your team Core : Python (OSS) core Extensible and hackable. Components allow you to share extensions widely in the Pristine layer Installing Typhoon - quick start Hello World - 5 min walkthrough Example DAG Typhoon DAG (YAML) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 name : favorite_authors schedule_interval : rate(1 day) tasks : choose_favorites : function : typhoon.flow_control.branch args : branches : - J. K. Rowling - George R. R. Martin - James Clavell get_author : input : choose_favorites function : functions.open_library_api.get_author args : author : !Py $BATCH write_author_json : input : get_author function : typhoon.filesystem.write_data args : hook : !Hook data_lake data : !MultiStep - !Py $BATCH['docs'] - !Py typhoon.data.json_array_to_json_records($1) path : !MultiStep - !Py $BATCH['docs'][0]['key'] - !Py f'/authors/{$1}.json' create_intermediate_dirs : True Equivalent Airflow DAG (python) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 import datetime import typhoon.contrib.functions as typhoon_functions # for a fair comparison import typhoon.contrib.transformations as typhoon_transformations from airflow import DAG from airflow.hooks.base_hook import BaseHook from airflow.operators.python_operator import PythonOperator from functions import open_library_api from out.new_people.typhoon.contrib.hooks.filesystem_hooks import LocalStorageHook def get_author ( author : str , ** context ): result = open_library_api . get_author ( requested_author = author , ) context [ 'ti' ] . xcom_push ( 'result' , list ( result )) def write_author_json ( source_task_id , ** context ): conn_params = BaseHook . get_connection ( 'data_lake' ) hook = LocalStorageHook ( conn_params ) # Note how we're hardcoding the class create_intermediate_dirs = True batches = context [ 'ti' ] . xcom_pull ( task_ids = source_task_id , key = 'result' ) for batch in batches : data = typhoon_transformations . data . json_array_to_json_records ( batch [ 'docs' ]) _key = batch [ 'docs' ][ 0 ][ 'key' ] path = f '/authors/ { _key } .json' typhoon_functions . filesystem . write_data ( hook = hook , data = data , path = path , create_intermediate_dirs = create_intermediate_dirs , ) with DAG ( dag_id = 'favorite_authors' , default_args = { 'owner' : 'typhoon' }, schedule_interval = '*/1 * * * *' , start_date = datetime . datetime ( 2021 , 3 , 25 , 21 , 10 ) ) as dag : for author in [ 'J. K. Rowling' , 'George R. R. Martin' , 'James Clavell' ]: get_author_task_id = f 'get_author_ { author } ' get_author_task = PythonOperator ( task_id = get_author_task_id , python_callable = get_author , op_kwargs = { 'author' : author , }, provide_context = True ) dag >> get_author_task write_author_json = PythonOperator ( task_id = f 'write_author_json_ { author } ' , python_callable = write_author_json , op_kwargs = { 'source_task_id' : get_author_task_id , }, provide_context = True ) get_author_task >> write_author_json Getting the works of my favorite authors from Open Library API Using with Airflow Building the above DAG using typhoon dag build --all . Airflow UI will then show: Auto-Completion Composing DAGs is really fast in VS Code with code completion. Quick start with VS Code. If you want the dag schema and the component schema to be generated after every change to your code (functions, transformations and connections) you need to: install the extension Run on Save by emeraldwalk and edit the path to your typhoon executable in generate_schemas.sh . You can find out the path by running the following command in the terminal: which typhoon . install the extension YAML by redhat. Component UI The Component UI is a dynamic UI (Streamlit app) based on a Component DAG. This means you can make a component and your team can then generate specific DAGs from this template. e.g. DB -> S3 -> Snowflake. They can then use this for any relational DB to export tables to Snowflake. Give your team autonomy by sharing templated flows they can configure. Shell & Cli The Interactive Shell is really useful for running tasks and understanding the data structure at each point. Here is a short demo of running the get_author task and seeing the data it returns which can then be explored. Inspired by others; instantly familiar.","title":"Overview"},{"location":"index.html#_1","text":"Why Typhoon Write Airflow DAGS faster Make creating reliable data pipelines easy for the whole team Simplicity and re-usability; a toolkit designed to be loved by Data Engineers Deploying to your existing Airflow with zero risk and no migration work Workflow : Typhoon YAML DAG --> transpile --> Airflow DAG Key Principles Elegant : YAML; low-code and easy to pick up. Data sharing - data flows between tasks making it intuitive and easy to build tasks. Composability - Functions combine like Lego. Effortless to extend for more sources and connections. Components - reduce complex tasks (e.g. CSV\u2192S3\u2192Snowflake) to 1 re-usable task. UI : Component UI for sharing DAG configuration with your DWH, Analyst or Data Sci. teams. Rich Cli : Inspired by other great command line interfaces and instantly familiar. Intelligent bash/zsh completion. Testable Tasks - automate DAG task tests. Testable Python - test functions or full DAGs with PyTest. Flexible deployment : deploy to Airflow - large reduction in effort, without breaking existing production. Layers Pristine : Pre-built (OSS) components and UI that can be shared to your team Core : Python (OSS) core Extensible and hackable. Components allow you to share extensions widely in the Pristine layer Installing Typhoon - quick start Hello World - 5 min walkthrough","title":""},{"location":"index.html#example-dag","text":"Typhoon DAG (YAML) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 name : favorite_authors schedule_interval : rate(1 day) tasks : choose_favorites : function : typhoon.flow_control.branch args : branches : - J. K. Rowling - George R. R. Martin - James Clavell get_author : input : choose_favorites function : functions.open_library_api.get_author args : author : !Py $BATCH write_author_json : input : get_author function : typhoon.filesystem.write_data args : hook : !Hook data_lake data : !MultiStep - !Py $BATCH['docs'] - !Py typhoon.data.json_array_to_json_records($1) path : !MultiStep - !Py $BATCH['docs'][0]['key'] - !Py f'/authors/{$1}.json' create_intermediate_dirs : True Equivalent Airflow DAG (python) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 import datetime import typhoon.contrib.functions as typhoon_functions # for a fair comparison import typhoon.contrib.transformations as typhoon_transformations from airflow import DAG from airflow.hooks.base_hook import BaseHook from airflow.operators.python_operator import PythonOperator from functions import open_library_api from out.new_people.typhoon.contrib.hooks.filesystem_hooks import LocalStorageHook def get_author ( author : str , ** context ): result = open_library_api . get_author ( requested_author = author , ) context [ 'ti' ] . xcom_push ( 'result' , list ( result )) def write_author_json ( source_task_id , ** context ): conn_params = BaseHook . get_connection ( 'data_lake' ) hook = LocalStorageHook ( conn_params ) # Note how we're hardcoding the class create_intermediate_dirs = True batches = context [ 'ti' ] . xcom_pull ( task_ids = source_task_id , key = 'result' ) for batch in batches : data = typhoon_transformations . data . json_array_to_json_records ( batch [ 'docs' ]) _key = batch [ 'docs' ][ 0 ][ 'key' ] path = f '/authors/ { _key } .json' typhoon_functions . filesystem . write_data ( hook = hook , data = data , path = path , create_intermediate_dirs = create_intermediate_dirs , ) with DAG ( dag_id = 'favorite_authors' , default_args = { 'owner' : 'typhoon' }, schedule_interval = '*/1 * * * *' , start_date = datetime . datetime ( 2021 , 3 , 25 , 21 , 10 ) ) as dag : for author in [ 'J. K. Rowling' , 'George R. R. Martin' , 'James Clavell' ]: get_author_task_id = f 'get_author_ { author } ' get_author_task = PythonOperator ( task_id = get_author_task_id , python_callable = get_author , op_kwargs = { 'author' : author , }, provide_context = True ) dag >> get_author_task write_author_json = PythonOperator ( task_id = f 'write_author_json_ { author } ' , python_callable = write_author_json , op_kwargs = { 'source_task_id' : get_author_task_id , }, provide_context = True ) get_author_task >> write_author_json Getting the works of my favorite authors from Open Library API","title":"Example DAG"},{"location":"index.html#using-with-airflow","text":"Building the above DAG using typhoon dag build --all . Airflow UI will then show:","title":"Using with Airflow"},{"location":"index.html#auto-completion","text":"Composing DAGs is really fast in VS Code with code completion.","title":"Auto-Completion"},{"location":"index.html#quick-start-with-vs-code","text":"If you want the dag schema and the component schema to be generated after every change to your code (functions, transformations and connections) you need to: install the extension Run on Save by emeraldwalk and edit the path to your typhoon executable in generate_schemas.sh . You can find out the path by running the following command in the terminal: which typhoon . install the extension YAML by redhat.","title":"Quick start with VS Code."},{"location":"index.html#component-ui","text":"The Component UI is a dynamic UI (Streamlit app) based on a Component DAG. This means you can make a component and your team can then generate specific DAGs from this template. e.g. DB -> S3 -> Snowflake. They can then use this for any relational DB to export tables to Snowflake. Give your team autonomy by sharing templated flows they can configure.","title":"Component UI"},{"location":"index.html#shell-cli","text":"The Interactive Shell is really useful for running tasks and understanding the data structure at each point. Here is a short demo of running the get_author task and seeing the data it returns which can then be explored. Inspired by others; instantly familiar.","title":"Shell &amp; Cli"},{"location":"dag-development-improving.html","text":"Improving our DAG In the previous section we created a DAG that reads from a local directory and writes to a SQLite file. We will now improve on that DAG by introducing new concepts that will help us make it more flexible and testable. Variables Variables let us define a value outside our DAG. In general it is a good idea not to hard code magic values into our code, and DAGs in Typhoon are no exception. This makes it easier to change or tune the value without requiring us to re-deploy our DAG. Similar to Airflow's variables If you're familiar with Airflow you'll notice that it's the same concept as Airflow variables. As much as possible Typhoon tries to depart from tried and tested solutions only when it makes . Variables and connections are something that for the most part just works in Airflow, so we saw no reason to change it adding more cognitive burden on developers. With that said it's worth pointing out the differences. In Typhoon variables have types associated with them. If a variable is of type JSON or YAML it will get loaded into a python object. If you need it to be loaded as a string prefer to choose the TEXT type. SQL as variable Let's move our insert statement into a variable so we can change it in the future if we need to. We'll change the line query : INSERT INTO typhoon_tutorial VALUES (?, ?, ?) for: query : $VARIABLE.tutorial_insert_query We named our variable insert_query but we could have given it any name. Typhoon status If we run typhoon status now we will see the following warning: \u2022 Found variables in DAGs that are not defined in the metadata database - Variable tutorial_insert_query is not set. Try typhoon variable add --var-id tutorial_insert_query --var-type VAR_TYPE --contents VALUE None It's useful to run typhoon status often as it will warn us about problems on our project. Lets define the variable now as suggested: Adding the variable We will also turn the variable into a Jinja template which we will render later. typhoon variable add --var-id tutorial_insert_query --var-type jinja --contents \"INSERT INTO TABLE typhoon_tutorial VALUES ({{ file_name }}, {{ num_words }}, {{ ts }})\" Adding variables from STDIN We can add variables from STDIN by leaving out the --contents argument. This will prompt us to give the value for the variable. We can also pipe the value UNIX style. Eg: cat query_file.sql | typhoon variable add --var-id tutorial_insert_query --var-type jinja or typhoon variable add --var-id tutorial_insert_query2 --var-type string < query_file.sql We can now run typhoon status again to see there is no issue, and check our newly created variable with: typhoon variable ls -l Web UI DAG Editor The web UI has an experimental DAG editor with intelligent code completion for things like variable and connection names, as well as functions, transformations and node names. However it's sometimes a little behind the CLI and can have some bugs caused by syntax checking. It will be re-written at some point in the future, but in the mean time check it out with typhoon webserver . It's really useful when it works correctly. Connections In our functions we hardcoded the logic to read from the filesystem and to write to SQLite. This is not ideal for a few reasons: Limited re-usability: Overly specific functions means we are unlikely to find them useful in other DAGs. Limited flexibility: If the logic stays the same but the data source changes (eg: Postgres instead of SQLite) we need to modify the function. Security: Hard-coding connection details in functions is never a good idea. Passing them as parameters in DAGs is not much better as you probably want to version control your DAGs. To solve this Typhoon provides Connections which let you define a type (eg: SQLite) and connection parameters. Connections are stored in the database and are used to instantiate hooks. The connection type determines which hook class needs to be instantiated. Lets now change both the source and destination to use connections. Similarities with Airflow Connections in Typhoon are also very similar to Airflow's connections, down to naming of their parameters and the inclusion of an extra field for parameters that don't fit in the default ones. We depart from Airflow in some ways however: In contrast to Airflow it lets the user define new connection types that are first class (ie: you get code completion for them in the CLI and you can select them in the UI). It attempts to group similar connections in interfaces/protocols so that they can be swapped for one another, simplifying testing or data source changes. Airflow does this somewhat, for example with DbApiHook, but we take this concept a lot further by grouping other similar data sources such as filesystems (Local Storage, FTP, SFTP, S3 etc can all be used interchangeably). Of course you can always access the underlying connection if you need specific functionality that is not easily re-usable. FileSystem Hook To read from the directory we will use a FileSystemHook . Specifically a LocalStorageHook though as we'll see this can be changed later. We'll also use a pre-defined Typhoon function called list_directory to list files in the directory and another one called read_data to read data from these files. Changing the ls node We need to change the line ls : function : functions.tutorial.get_files_in_directory config : directory : /tmp/typhoon/landing/ to: ls : function : typhoon.filesystem.list_directory config : hook : $HOOK.tutorial_fs path : landing Notice how we no longer define the full path in the DAG. FileSystem hooks have a base path that gets prepended to it (eg: In the S3 hook that's the bucket, in an FTP hook you might want a different base directory for testing and another for production etc.). Adding the hook We will add the hook to our connections.yml file: tutorial_fs : local : conn_type : local_storage extra : base_path : /tmp/typhoon/ Then add it to our metadata database: typhoon connection add --conn-id tutorial_fs --conn-env local Reading the files We will add an additional node to read the file contents using another predefined Typhoon function: read_file : function : typhoon.filesystem.read_data config : hook : $hook.tutorial_fs Changing the count_words node and function We will change our function to receive a text and just count the words, removing the file reading logic from it. It is generally a good idea for a function to do one and only one thing. We will also take a dict of metadata so we can keep track of where the text came from. T = TypeVar ( 'T' ) def words_in_text ( text : str , metadata : T ) -> Tuple [ int , T ]: num_words = len ( re . split ( '\\s+' , text )) return num_words , metadata count_words : function : functions.tutorial.words_in_text DB-API Hook We will continue making our DAG more generic by using the Typhoon function execute_query, which takes a DbApiHook , which can be any hook supporting the DB-API interface . Almost every database library in Python supports it, so we will be able to use them interchangeably (if the SQL query is valid in both databases). Changing the write_to_db node We will change write_to_db : function : functions.tutorial.execute_on_sqlite config : sqlite_db : /tmp/typhoon/tutorial.db query : $VARIABLE.tutorial_insert_query to: write_to_db : function : typhoon.relational.execute_query config : hook : $HOOK.tutorial_db query : $VARIABLE.tutorial_insert_query Adding the hook First we define the connection details in the connections.yml file: tutorial_db : dev : conn_type : sqlite extra : database : /tmp/typhoon/tutorial.db Then we add the connection with the CLI: typhoon connection add --conn-id tutorial_db --conn-env local Edges: putting it all together Finally to make all of this come together we need toWe need to change the edges. The final DAG will be: name : tutorial schedule_interval : \"@hourly\" nodes : ls : function : typhoon.filesystem.list_directory config : hook : $HOOK.tutorial_fs path : landing read_file : function : typhoon.filesystem.read_data config : hook : $hook.tutorial_fs count_words : function : functions.tutorial.words_in_text write_to_db : function : typhoon.relational.execute_query config : hook : $HOOK.tutorial_db query : $VARIABLE.tutorial_insert_query edges : e1 : source : ls adapter : file_path => APPLY : $BATCH destination : count_words metadata : file_path => APPLY : $ADAPTER.file_path e2 : source : count_words adapter : parameters => APPLY : \"[$BATCH[0], $BATCH_METADATA.file_path, $DAG_CONTEXT.execution_date.isoformat()]\" destination : write_to_db","title":"Improving our DAG"},{"location":"dag-development-improving.html#improving-our-dag","text":"In the previous section we created a DAG that reads from a local directory and writes to a SQLite file. We will now improve on that DAG by introducing new concepts that will help us make it more flexible and testable.","title":"Improving our DAG"},{"location":"dag-development-improving.html#variables","text":"Variables let us define a value outside our DAG. In general it is a good idea not to hard code magic values into our code, and DAGs in Typhoon are no exception. This makes it easier to change or tune the value without requiring us to re-deploy our DAG. Similar to Airflow's variables If you're familiar with Airflow you'll notice that it's the same concept as Airflow variables. As much as possible Typhoon tries to depart from tried and tested solutions only when it makes . Variables and connections are something that for the most part just works in Airflow, so we saw no reason to change it adding more cognitive burden on developers. With that said it's worth pointing out the differences. In Typhoon variables have types associated with them. If a variable is of type JSON or YAML it will get loaded into a python object. If you need it to be loaded as a string prefer to choose the TEXT type.","title":"Variables"},{"location":"dag-development-improving.html#sql-as-variable","text":"Let's move our insert statement into a variable so we can change it in the future if we need to. We'll change the line query : INSERT INTO typhoon_tutorial VALUES (?, ?, ?) for: query : $VARIABLE.tutorial_insert_query We named our variable insert_query but we could have given it any name. Typhoon status If we run typhoon status now we will see the following warning: \u2022 Found variables in DAGs that are not defined in the metadata database - Variable tutorial_insert_query is not set. Try typhoon variable add --var-id tutorial_insert_query --var-type VAR_TYPE --contents VALUE None It's useful to run typhoon status often as it will warn us about problems on our project. Lets define the variable now as suggested:","title":"SQL as variable"},{"location":"dag-development-improving.html#adding-the-variable","text":"We will also turn the variable into a Jinja template which we will render later. typhoon variable add --var-id tutorial_insert_query --var-type jinja --contents \"INSERT INTO TABLE typhoon_tutorial VALUES ({{ file_name }}, {{ num_words }}, {{ ts }})\" Adding variables from STDIN We can add variables from STDIN by leaving out the --contents argument. This will prompt us to give the value for the variable. We can also pipe the value UNIX style. Eg: cat query_file.sql | typhoon variable add --var-id tutorial_insert_query --var-type jinja or typhoon variable add --var-id tutorial_insert_query2 --var-type string < query_file.sql We can now run typhoon status again to see there is no issue, and check our newly created variable with: typhoon variable ls -l Web UI DAG Editor The web UI has an experimental DAG editor with intelligent code completion for things like variable and connection names, as well as functions, transformations and node names. However it's sometimes a little behind the CLI and can have some bugs caused by syntax checking. It will be re-written at some point in the future, but in the mean time check it out with typhoon webserver . It's really useful when it works correctly.","title":"Adding the variable"},{"location":"dag-development-improving.html#connections","text":"In our functions we hardcoded the logic to read from the filesystem and to write to SQLite. This is not ideal for a few reasons: Limited re-usability: Overly specific functions means we are unlikely to find them useful in other DAGs. Limited flexibility: If the logic stays the same but the data source changes (eg: Postgres instead of SQLite) we need to modify the function. Security: Hard-coding connection details in functions is never a good idea. Passing them as parameters in DAGs is not much better as you probably want to version control your DAGs. To solve this Typhoon provides Connections which let you define a type (eg: SQLite) and connection parameters. Connections are stored in the database and are used to instantiate hooks. The connection type determines which hook class needs to be instantiated. Lets now change both the source and destination to use connections. Similarities with Airflow Connections in Typhoon are also very similar to Airflow's connections, down to naming of their parameters and the inclusion of an extra field for parameters that don't fit in the default ones. We depart from Airflow in some ways however: In contrast to Airflow it lets the user define new connection types that are first class (ie: you get code completion for them in the CLI and you can select them in the UI). It attempts to group similar connections in interfaces/protocols so that they can be swapped for one another, simplifying testing or data source changes. Airflow does this somewhat, for example with DbApiHook, but we take this concept a lot further by grouping other similar data sources such as filesystems (Local Storage, FTP, SFTP, S3 etc can all be used interchangeably). Of course you can always access the underlying connection if you need specific functionality that is not easily re-usable.","title":"Connections"},{"location":"dag-development-improving.html#filesystem-hook","text":"To read from the directory we will use a FileSystemHook . Specifically a LocalStorageHook though as we'll see this can be changed later. We'll also use a pre-defined Typhoon function called list_directory to list files in the directory and another one called read_data to read data from these files.","title":"FileSystem Hook"},{"location":"dag-development-improving.html#changing-the-ls-node","text":"We need to change the line ls : function : functions.tutorial.get_files_in_directory config : directory : /tmp/typhoon/landing/ to: ls : function : typhoon.filesystem.list_directory config : hook : $HOOK.tutorial_fs path : landing Notice how we no longer define the full path in the DAG. FileSystem hooks have a base path that gets prepended to it (eg: In the S3 hook that's the bucket, in an FTP hook you might want a different base directory for testing and another for production etc.).","title":"Changing the ls node"},{"location":"dag-development-improving.html#adding-the-hook","text":"We will add the hook to our connections.yml file: tutorial_fs : local : conn_type : local_storage extra : base_path : /tmp/typhoon/ Then add it to our metadata database: typhoon connection add --conn-id tutorial_fs --conn-env local","title":"Adding the hook"},{"location":"dag-development-improving.html#reading-the-files","text":"We will add an additional node to read the file contents using another predefined Typhoon function: read_file : function : typhoon.filesystem.read_data config : hook : $hook.tutorial_fs","title":"Reading the files"},{"location":"dag-development-improving.html#changing-the-count_words-node-and-function","text":"We will change our function to receive a text and just count the words, removing the file reading logic from it. It is generally a good idea for a function to do one and only one thing. We will also take a dict of metadata so we can keep track of where the text came from. T = TypeVar ( 'T' ) def words_in_text ( text : str , metadata : T ) -> Tuple [ int , T ]: num_words = len ( re . split ( '\\s+' , text )) return num_words , metadata count_words : function : functions.tutorial.words_in_text","title":"Changing the count_words node and function"},{"location":"dag-development-improving.html#db-api-hook","text":"We will continue making our DAG more generic by using the Typhoon function execute_query, which takes a DbApiHook , which can be any hook supporting the DB-API interface . Almost every database library in Python supports it, so we will be able to use them interchangeably (if the SQL query is valid in both databases).","title":"DB-API Hook"},{"location":"dag-development-improving.html#changing-the-write_to_db-node","text":"We will change write_to_db : function : functions.tutorial.execute_on_sqlite config : sqlite_db : /tmp/typhoon/tutorial.db query : $VARIABLE.tutorial_insert_query to: write_to_db : function : typhoon.relational.execute_query config : hook : $HOOK.tutorial_db query : $VARIABLE.tutorial_insert_query","title":"Changing the write_to_db node"},{"location":"dag-development-improving.html#adding-the-hook_1","text":"First we define the connection details in the connections.yml file: tutorial_db : dev : conn_type : sqlite extra : database : /tmp/typhoon/tutorial.db Then we add the connection with the CLI: typhoon connection add --conn-id tutorial_db --conn-env local","title":"Adding the hook"},{"location":"dag-development-improving.html#edges-putting-it-all-together","text":"Finally to make all of this come together we need toWe need to change the edges. The final DAG will be: name : tutorial schedule_interval : \"@hourly\" nodes : ls : function : typhoon.filesystem.list_directory config : hook : $HOOK.tutorial_fs path : landing read_file : function : typhoon.filesystem.read_data config : hook : $hook.tutorial_fs count_words : function : functions.tutorial.words_in_text write_to_db : function : typhoon.relational.execute_query config : hook : $HOOK.tutorial_db query : $VARIABLE.tutorial_insert_query edges : e1 : source : ls adapter : file_path => APPLY : $BATCH destination : count_words metadata : file_path => APPLY : $ADAPTER.file_path e2 : source : count_words adapter : parameters => APPLY : \"[$BATCH[0], $BATCH_METADATA.file_path, $DAG_CONTEXT.execution_date.isoformat()]\" destination : write_to_db","title":"Edges: putting it all together"},{"location":"dag-development-tutorial.html","text":"DAG Development Tutorial Let's start from scratch with a very basic DAG. Every hour we will read all the files under the directory /tmp/typhoon/landing/ , count the number of words and write that number in a SQLite database. The SQLite table we write to will have the following shape: file_name num_words ts a.txt 13 2020-01-06T13:01:15 b.txt 20 2020-01-06T13:01:17 c.txt 10 2020-01-06T13:01:19 Learning example This is just an example for learning purposes. In a production deployment this would not be a useful DAG since in a Lambda instance there won't be any data under tmp . We also won't be able to retrieve the SQLite database because it's persisted to a local file, which will disappear once the lambda finishes running. However, it is still a good example to learn the basics in a local environment as it's a simple example and not that far off from real applications. Just replace the function that reads locally with a function that reads from an FTP location and the function that writes to SQLite with a function that writes to any other remote database. Functions and transformations When trying to solve a problem in python it is useful to write some functions and test them until we get our solution right, whether it is in a jupyter notebook or REPL. Typhoon encourages this kind of interactive development as we can trivially incorporate these functions into our DAGs. Reading from a directory Our first function will search inside a directory and yield the .txt file paths in batches: from typing import Iterator from pathlib import Path def get_files_in_directory ( directory : str ) -> Iterator [ str ]: yield from map ( str , Path ( directory ) . glob ( '*.txt' )) Let's place this code inside a file called functions/tutorial.py . Pathlib If this is the first time you see the pathlib library check it out , it is one of the nicest additions to Python 3. Counting number of lines Given a file path we will read it and count the number of words in it. A naive implementation might be the following: import re from typing import Tuple from pathlib import Path def words_in_file ( file_path : str ) -> Tuple [ str , int ]: contents = Path ( file_path ) . read_text () num_words = len ( re . split ( '\\s+' , contents )) return file_path , num_words We just split the contents when there's one or more separators and count the length of the resulting list. Notice that we also returned the file_path as well as the number of words, since it's likely that we will need it upstream. This is a common and useful pattern in Typhoon functions. Let's also place this code inside the previous file called functions/tutorial.py . Writing to SQLite Once we have this information we want to write it to a SQLite table in tmp/typhoon/tutorial.db that we will call typhoon_tutorial . However if we write a function that does exactly that there's very little chance we will ever use it again. With very little effort we can make a function that can execute any query against a SQLite database. This is much more likely to be useful in the future. import sqlite3 from contextlib import closing from typing import Sequence def execute_on_sqlite ( sqlite_db : str , query : str , parameters : Sequence ): conn = sqlite3 . connect ( sqlite_db ) with closing ( conn . cursor ()) as cursor : cursor . execute ( query , parameters ) conn . commit () Finally we will also place this code inside the previous file called functions/tutorial.py . Split functions in different files Usually we would split functions in files that are descriptive of what they do so they are organised in modules. Eg: filesystem.get_files_in_directory . Since this is just a tutorial we keep them in the same file for simplicity. Testing As you may have noticed, the fact that we wrote our code as regular python functions that just take some parameters and return some values means that it is trivially testable. Our testing framework of choice for this example is Pytest, but any other framework would work equally well. \"Pure\" functions We can't say that our functions are pure in the strict meaning of the term since they have side effects, but they share some advantages such as having no global state and being repeatable as much as possible (ie: given the same input returning the same output). import sqlite3 from contextlib import closing from functions.tutorial import get_files_in_directory , words_in_file , execute_on_sqlite def test_get_files_in_directory ( tmp_path ): file1 = tmp_path / 'a.txt' file1 . write_text ( 'foo' ) file2 = tmp_path / 'b.txt' file2 . write_text ( 'bar' ) assert set ( get_files_in_directory ( str ( tmp_path ))) == { str ( file1 ), str ( file2 )} def test_words_in_file ( tmp_path ): test_file = ( tmp_path / 'a.txt' ) test_file . write_text ( 'To be, or not to be, that is the question' ) assert words_in_file ( str ( test_file )) == ( str ( test_file ), 10 ) def test_execute_on_sqlite ( tmp_path ): sqlite_db = str ( tmp_path / 'test.db' ) conn = sqlite3 . connect ( sqlite_db ) with closing ( conn . cursor ()) as cursor : cursor . execute ( '''CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)''' ) insert_query = '''INSERT INTO stocks VALUES (?,?,?,?,?)''' insert_query_params = ( '2006-01-05' , 'BUY' , 'RHAR' , 11 , 35.14 ) execute_on_sqlite ( sqlite_db , insert_query , insert_query_params ) with closing ( conn . cursor ()) as cursor : cursor . execute ( 'SELECT * FROM stocks' ) assert cursor . fetchone () == insert_query_params conn . close () We can create directory tests/ and put it in a file named tutorial_tests.py . Run them with pytest and check that they pass. Now we can be confident that our functions work as intended. Running tests To run the tests go to the base directory of the Typhoon project and run PYTHONPATH=\"PYTHONPATH:$PWD\" python -m pytest tests . This will fix the python path so the imports work correctly in the tests. DAG Now that we have all the necessary functions we just need to write the DAG. Overview To get an intuitive understanding of what DAG definitions do we will first show you how we would write the code in a regular python script that we want to run from the console. After that we will write the real Typhoon DAG code so the reader can compare. Not real code This is just an intuitive understanding of what the DAG compiles to. It's a useful model to have even though it's not entirely accurate as we will see later. Take a moment to understand this code, it will help you as a reference once we introduce the DAG code. from datetime import datetime DIRECTORY = '/tmp/typhoon/landing/' SQLITE_DB = '/tmp/typhoon/tutorial.db' INSERT_QUERY = 'INSERT INTO TABLE typhoon_tutorial VALUES (?, ?, ?)' result = get_files_in_directory ( directory = DIRECTORY ) for batch in result : result = words_in_file ( file_path = batch ) parameters = [ batch [ 0 ], batch [ 1 ], datetime . now () . isoformat ()] execute_on_sqlite ( sqlite_db = SQLITE_DB , query = INSERT_QUERY , parameters = parameters ) Graph representation We can visualize these tasks in a Direct Acyclig Graph (DAG). This is how we could define the DAG: graph LR subgraph DAG ls -- e1 --> count_words count_words -- e2 --> write_to_db end Typhoon uses graph terminology in DAGs, defining tasks in terms of nodes and edges . However, at run-time ls will yield a batch for each file it finds. Assuming if finds a.txt , b.txt and c.txt then it will yield three batches and there will be a count_words task for each. The count words node just returns one value/batch so it won't branch any further. We can visualize the run-time shape of this DAG as the following: graph TB subgraph DAG ls-- a.txt -->count_words ls-- b.txt -->count_words2 ls-- c.txt -->count_words3 subgraph Branch one count_words -- a.txt, 12 --> write_to_db end subgraph Branch two count_words2 -- b.txt, 9 --> write_to_db2 end subgraph Branch three count_words3 -- c.txt, 17 --> write_to_db3 end end DAG Code Now that we have an idea of what we want to do let's write the real DAG code that does the same. name : tutorial schedule_interval : \"@hourly\" nodes : ls : function : functions.tutorial.get_files_in_directory config : directory : /tmp/typhoon/landing/ count_words : function : functions.tutorial.words_in_file write_to_db : function : functions.tutorial.execute_on_sqlite config : sqlite_db : /tmp/typhoon/tutorial.db insert_query : INSERT INTO TABLE typhoon_tutorial VALUES (?, ?, ?) edges : e1 : source : ls adapter : file_path => APPLY : $BATCH destination : count_words e2 : source : count_words adapter : parameters => APPLY : \"[$BATCH[0], $BATCH[1], $DAG_CONTEXT.execution_date.isoformat()]\" destination : write_to_db Save this code in dags/tutorial.yml . Config Vs Adapter Notice how in config we are passing the static parameters, while in the adapter we are constructing the dynamic parameters from the source node's batch. Think of config as parameters that are static configuration, while adapter adapts the output of the source node to the dynamic input parameter of the destination node. Testing the edges It can seem like the downside to having a YAML file is that code in the adapters can't be unit tested like functions can. To fix that Typhoon lets you test edges on the CLI by providing an input batch. Our DAG does not have much transformation logic, but it does have some in edge e2 which we can test with: typhoon dag edge test --dag-name tutorial --edge-name e2 --input \"['a.txt', 3]\" --eval Eval Without the --eval flag our input batch would be a string. We need it to evaluate that string as python code. Testing different execution dates Since the result uses the execution date we can pass it as a parameter to test with --execution-date '2019-01-02' . It accepts dates with any of the following formats %Y-%m-%d , %Y-%m-%dT%H:%M:%S or %Y-%m-%d %H:%M:%S . Since we didn't use any in the previous example it used the current time. Running the DAG Creating the SQLite table In order for the DAG to work our database needs to have the typhoon_tutorial table defined. > sqlite3 /tmp/typhoon/tutorial.db SQLite version 3.30.1 2019-10-10 20:19:45 Enter \".help\" for usage hints. sqlite> CREATE TABLE typhoon_tutorial (file_name TEXT, num_words INT, ts TEXT); DAG run We can now run the DAG to check that it works as expected. typhoon dag run --dag-name tutorial Result: Running tutorial from local build... Cleaning out directory... Build all DAGs... Writing file to /Users/biellls/Desktop/typhoon/hello_world/out/template.yml Writing file to /Users/biellls/Desktop/typhoon/hello_world/out/tutorial/requirements.txt Typhoon package is in editable mode. Copying to lambda package... Setting up user defined code as symlink for debugging... Finished building DAGs Checking results Lets read the table to make sure we inserted the data correctly: > sqlite3 /tmp/typhoon/tutorial.db SQLite version 3.30.1 2019-10-10 20:19:45 Enter \".help\" for usage hints. sqlite> .headers on sqlite> .mode column sqlite> select * from typhoon_tutorial; file_name num_words ts -------------------------- ---------- -------------------------- /tmp/typhoon/landing/a.txt 5 2020-01-17T18:36:39.489866","title":"DAG Development Tutorial"},{"location":"dag-development-tutorial.html#dag-development-tutorial","text":"Let's start from scratch with a very basic DAG. Every hour we will read all the files under the directory /tmp/typhoon/landing/ , count the number of words and write that number in a SQLite database. The SQLite table we write to will have the following shape: file_name num_words ts a.txt 13 2020-01-06T13:01:15 b.txt 20 2020-01-06T13:01:17 c.txt 10 2020-01-06T13:01:19 Learning example This is just an example for learning purposes. In a production deployment this would not be a useful DAG since in a Lambda instance there won't be any data under tmp . We also won't be able to retrieve the SQLite database because it's persisted to a local file, which will disappear once the lambda finishes running. However, it is still a good example to learn the basics in a local environment as it's a simple example and not that far off from real applications. Just replace the function that reads locally with a function that reads from an FTP location and the function that writes to SQLite with a function that writes to any other remote database.","title":"DAG Development Tutorial"},{"location":"dag-development-tutorial.html#functions-and-transformations","text":"When trying to solve a problem in python it is useful to write some functions and test them until we get our solution right, whether it is in a jupyter notebook or REPL. Typhoon encourages this kind of interactive development as we can trivially incorporate these functions into our DAGs.","title":"Functions and transformations"},{"location":"dag-development-tutorial.html#reading-from-a-directory","text":"Our first function will search inside a directory and yield the .txt file paths in batches: from typing import Iterator from pathlib import Path def get_files_in_directory ( directory : str ) -> Iterator [ str ]: yield from map ( str , Path ( directory ) . glob ( '*.txt' )) Let's place this code inside a file called functions/tutorial.py . Pathlib If this is the first time you see the pathlib library check it out , it is one of the nicest additions to Python 3.","title":"Reading from a directory"},{"location":"dag-development-tutorial.html#counting-number-of-lines","text":"Given a file path we will read it and count the number of words in it. A naive implementation might be the following: import re from typing import Tuple from pathlib import Path def words_in_file ( file_path : str ) -> Tuple [ str , int ]: contents = Path ( file_path ) . read_text () num_words = len ( re . split ( '\\s+' , contents )) return file_path , num_words We just split the contents when there's one or more separators and count the length of the resulting list. Notice that we also returned the file_path as well as the number of words, since it's likely that we will need it upstream. This is a common and useful pattern in Typhoon functions. Let's also place this code inside the previous file called functions/tutorial.py .","title":"Counting number of lines"},{"location":"dag-development-tutorial.html#writing-to-sqlite","text":"Once we have this information we want to write it to a SQLite table in tmp/typhoon/tutorial.db that we will call typhoon_tutorial . However if we write a function that does exactly that there's very little chance we will ever use it again. With very little effort we can make a function that can execute any query against a SQLite database. This is much more likely to be useful in the future. import sqlite3 from contextlib import closing from typing import Sequence def execute_on_sqlite ( sqlite_db : str , query : str , parameters : Sequence ): conn = sqlite3 . connect ( sqlite_db ) with closing ( conn . cursor ()) as cursor : cursor . execute ( query , parameters ) conn . commit () Finally we will also place this code inside the previous file called functions/tutorial.py . Split functions in different files Usually we would split functions in files that are descriptive of what they do so they are organised in modules. Eg: filesystem.get_files_in_directory . Since this is just a tutorial we keep them in the same file for simplicity.","title":"Writing to SQLite"},{"location":"dag-development-tutorial.html#testing","text":"As you may have noticed, the fact that we wrote our code as regular python functions that just take some parameters and return some values means that it is trivially testable. Our testing framework of choice for this example is Pytest, but any other framework would work equally well. \"Pure\" functions We can't say that our functions are pure in the strict meaning of the term since they have side effects, but they share some advantages such as having no global state and being repeatable as much as possible (ie: given the same input returning the same output). import sqlite3 from contextlib import closing from functions.tutorial import get_files_in_directory , words_in_file , execute_on_sqlite def test_get_files_in_directory ( tmp_path ): file1 = tmp_path / 'a.txt' file1 . write_text ( 'foo' ) file2 = tmp_path / 'b.txt' file2 . write_text ( 'bar' ) assert set ( get_files_in_directory ( str ( tmp_path ))) == { str ( file1 ), str ( file2 )} def test_words_in_file ( tmp_path ): test_file = ( tmp_path / 'a.txt' ) test_file . write_text ( 'To be, or not to be, that is the question' ) assert words_in_file ( str ( test_file )) == ( str ( test_file ), 10 ) def test_execute_on_sqlite ( tmp_path ): sqlite_db = str ( tmp_path / 'test.db' ) conn = sqlite3 . connect ( sqlite_db ) with closing ( conn . cursor ()) as cursor : cursor . execute ( '''CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)''' ) insert_query = '''INSERT INTO stocks VALUES (?,?,?,?,?)''' insert_query_params = ( '2006-01-05' , 'BUY' , 'RHAR' , 11 , 35.14 ) execute_on_sqlite ( sqlite_db , insert_query , insert_query_params ) with closing ( conn . cursor ()) as cursor : cursor . execute ( 'SELECT * FROM stocks' ) assert cursor . fetchone () == insert_query_params conn . close () We can create directory tests/ and put it in a file named tutorial_tests.py . Run them with pytest and check that they pass. Now we can be confident that our functions work as intended. Running tests To run the tests go to the base directory of the Typhoon project and run PYTHONPATH=\"PYTHONPATH:$PWD\" python -m pytest tests . This will fix the python path so the imports work correctly in the tests.","title":"Testing"},{"location":"dag-development-tutorial.html#dag","text":"Now that we have all the necessary functions we just need to write the DAG.","title":"DAG"},{"location":"dag-development-tutorial.html#overview","text":"To get an intuitive understanding of what DAG definitions do we will first show you how we would write the code in a regular python script that we want to run from the console. After that we will write the real Typhoon DAG code so the reader can compare. Not real code This is just an intuitive understanding of what the DAG compiles to. It's a useful model to have even though it's not entirely accurate as we will see later. Take a moment to understand this code, it will help you as a reference once we introduce the DAG code. from datetime import datetime DIRECTORY = '/tmp/typhoon/landing/' SQLITE_DB = '/tmp/typhoon/tutorial.db' INSERT_QUERY = 'INSERT INTO TABLE typhoon_tutorial VALUES (?, ?, ?)' result = get_files_in_directory ( directory = DIRECTORY ) for batch in result : result = words_in_file ( file_path = batch ) parameters = [ batch [ 0 ], batch [ 1 ], datetime . now () . isoformat ()] execute_on_sqlite ( sqlite_db = SQLITE_DB , query = INSERT_QUERY , parameters = parameters )","title":"Overview"},{"location":"dag-development-tutorial.html#graph-representation","text":"We can visualize these tasks in a Direct Acyclig Graph (DAG). This is how we could define the DAG: graph LR subgraph DAG ls -- e1 --> count_words count_words -- e2 --> write_to_db end Typhoon uses graph terminology in DAGs, defining tasks in terms of nodes and edges . However, at run-time ls will yield a batch for each file it finds. Assuming if finds a.txt , b.txt and c.txt then it will yield three batches and there will be a count_words task for each. The count words node just returns one value/batch so it won't branch any further. We can visualize the run-time shape of this DAG as the following: graph TB subgraph DAG ls-- a.txt -->count_words ls-- b.txt -->count_words2 ls-- c.txt -->count_words3 subgraph Branch one count_words -- a.txt, 12 --> write_to_db end subgraph Branch two count_words2 -- b.txt, 9 --> write_to_db2 end subgraph Branch three count_words3 -- c.txt, 17 --> write_to_db3 end end","title":"Graph representation"},{"location":"dag-development-tutorial.html#dag-code","text":"Now that we have an idea of what we want to do let's write the real DAG code that does the same. name : tutorial schedule_interval : \"@hourly\" nodes : ls : function : functions.tutorial.get_files_in_directory config : directory : /tmp/typhoon/landing/ count_words : function : functions.tutorial.words_in_file write_to_db : function : functions.tutorial.execute_on_sqlite config : sqlite_db : /tmp/typhoon/tutorial.db insert_query : INSERT INTO TABLE typhoon_tutorial VALUES (?, ?, ?) edges : e1 : source : ls adapter : file_path => APPLY : $BATCH destination : count_words e2 : source : count_words adapter : parameters => APPLY : \"[$BATCH[0], $BATCH[1], $DAG_CONTEXT.execution_date.isoformat()]\" destination : write_to_db Save this code in dags/tutorial.yml . Config Vs Adapter Notice how in config we are passing the static parameters, while in the adapter we are constructing the dynamic parameters from the source node's batch. Think of config as parameters that are static configuration, while adapter adapts the output of the source node to the dynamic input parameter of the destination node.","title":"DAG Code"},{"location":"dag-development-tutorial.html#testing-the-edges","text":"It can seem like the downside to having a YAML file is that code in the adapters can't be unit tested like functions can. To fix that Typhoon lets you test edges on the CLI by providing an input batch. Our DAG does not have much transformation logic, but it does have some in edge e2 which we can test with: typhoon dag edge test --dag-name tutorial --edge-name e2 --input \"['a.txt', 3]\" --eval Eval Without the --eval flag our input batch would be a string. We need it to evaluate that string as python code. Testing different execution dates Since the result uses the execution date we can pass it as a parameter to test with --execution-date '2019-01-02' . It accepts dates with any of the following formats %Y-%m-%d , %Y-%m-%dT%H:%M:%S or %Y-%m-%d %H:%M:%S . Since we didn't use any in the previous example it used the current time.","title":"Testing the edges"},{"location":"dag-development-tutorial.html#running-the-dag","text":"","title":"Running the DAG"},{"location":"dag-development-tutorial.html#creating-the-sqlite-table","text":"In order for the DAG to work our database needs to have the typhoon_tutorial table defined. > sqlite3 /tmp/typhoon/tutorial.db SQLite version 3.30.1 2019-10-10 20:19:45 Enter \".help\" for usage hints. sqlite> CREATE TABLE typhoon_tutorial (file_name TEXT, num_words INT, ts TEXT);","title":"Creating the SQLite table"},{"location":"dag-development-tutorial.html#dag-run","text":"We can now run the DAG to check that it works as expected. typhoon dag run --dag-name tutorial Result: Running tutorial from local build... Cleaning out directory... Build all DAGs... Writing file to /Users/biellls/Desktop/typhoon/hello_world/out/template.yml Writing file to /Users/biellls/Desktop/typhoon/hello_world/out/tutorial/requirements.txt Typhoon package is in editable mode. Copying to lambda package... Setting up user defined code as symlink for debugging... Finished building DAGs","title":"DAG run"},{"location":"dag-development-tutorial.html#checking-results","text":"Lets read the table to make sure we inserted the data correctly: > sqlite3 /tmp/typhoon/tutorial.db SQLite version 3.30.1 2019-10-10 20:19:45 Enter \".help\" for usage hints. sqlite> .headers on sqlite> .mode column sqlite> select * from typhoon_tutorial; file_name num_words ts -------------------------- ---------- -------------------------- /tmp/typhoon/landing/a.txt 5 2020-01-17T18:36:39.489866","title":"Checking results"},{"location":"examples/airflow-examples.html","text":"","title":"Airflow examples"},{"location":"examples/automating-pandas.html","text":"","title":"Automating pandas"},{"location":"examples/hello-world.html","text":"Hello World - a non-trivial example This tutorial will take you through how to write a simple (but non-trivial) Hello World example. This is the final DAG we will build. We will step through each part below. The DAG takes a YAML list of dictonaties (our data) and writes each dictionary to a file. It stores the file by run_date (hourly). name : hello_world schedule_interval : rate(1 hours) granularity : hour tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True Creating a Task name : hello_world schedule_interval : rate(10 minutes) granularity : hour Very simply this sets you flow name (no spaces) and the schedule interval in a rate to run at. It will use a timestamp truncated to the hour for the intervals. tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot Here we are setting up our tasks in our flow. This is a DAG structure similar to many other workflow tools (e.g. Task A \u2192 Task B \u2192 Task C). Lets examine our first task, send_data , which simply outputs 3 'files' containing CSV formatted strings represented as a YAML list of dictionaries. We use flow_control.branch to yield our 3 branches to the next node. Connections & typhoon status Lets start now by building this very basic task, and inspecting the output using a few options. First in your terminal, in the directory you have initialised your project, use typhoon-cli to run typhoon status . We encourage you to run this after your steps to see if any errors, or unset variables are there. Here we can see that we are initially missing a connection file without connections so lets add this. You need to add a connections.yml file to the root directory of your typhoon project. In this case we are calling our data_lake which is a local filesystem in local and a cloud S3 bucket in prod . To choose our environment we simply swap connections. We will also add an echo connection which we will use in a moment. echo : local : conn_type : echo data_lake : prod : conn_type : s3 extra : base_path : / bucket : test-corp-data-lake region_name : eu-west-1 login : <your login> passowrd : <your key> local : conn_type : local_storage extra : base_path : /tmp/data_lake Now we have this lets add our missing connections. We can get help on the client by using typhoon \u2014-help And then typhoon connection -\u2014help Finally, we can use typhoon connection add --help So now we can see how to add our connections from the connections.yml file. Note the local environment. typhoon connection add --conn-id data_lake --conn-env local typhoon connection add --conn-id echo --conn-env local Now again typhoon status A Ok! Build, Run and debug Let's build our hello_world DAG and see some output. As a quick way lets add on echo task after our send_data task: echo_my_output : input : send_data function : typhoon.debug.echo args : mydata : !Py $BATCH $BATCH is the data packet as a consistent default reference. It can be any type so depending on your data can be shaped differently. e.g. $BATCH['key], $BATCH[0] etc. Lets build it in typhoon-cli! We can skip the help as you can now navigate the cli help the same way as for connections ( hint: typhoon dag --help ) typhoon dag build hello_world ... and run it typhoon dag run --dag-name hello_world Output of the YAML list as a list of dictionaries Now we can see what will be passed to the next node (which in our case is just echoing to the prompt). Writing to a file write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True Our next task write_data receives each branch (asynchronously - more on this later for performance) from its input task. Setting a task as an input creates an edge for the data to flow linking them: send_data \u2192 write_data Next you notice we are writing to a filesystem using a standard python function filesystem.write_data . Into this function we are passing 3 arguments, the connection hook, a transformation of the data, and the path (similar to airflow ones - reusable?), Introducing our built in context variables $BATCH: This is the data package passed to the next node. $BATCH_NUM: This is the iterator number if you are batching (queries for example). $HOOK: This represents your hook - you can use !Hook my_conn as syntax sugar or !Py $HOOK.myconn $VARIABLE: This is how you can access saved variables e.g. lists of tables, query templates etc. An example might be !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) $DAG_CONTEXT.interval_start & $DAG_CONTEXT.interval_end: Execution interval for example: $DAG_CONTEXT.interval_start \u2192 '2021-05-23 10:00:00' (inclusive) $DAG_CONTEXT.interval_end \u2192 '2021-05-23 11:00:00' (exclusive) Passing data The notation !Py indicates that what comes after is evaluated as normal python code. $BATCH is the data packet as a consistent default reference. It can be any type so depending on your data can be shaped differently e.g. $BATCH['key], $BATCH[0] etc. In our case remember each branch yields a list of dictionaries with two keys [{'filename': 'users.txt', 'contents':'['John', 'Amy', 'Adam', 'Jane']},{'filename':...}] The most complex item here is a !Multistep process showing how you can do multiple small python transformations in series in a a nice readable way (or you can put it in one line of course): !Py $BATCH['filename'] will evaluate to \"users.txt' !Py $DAG_CONTEXT.interval_end will evaluate to the timestamp of the DAG run. This is a built in context variable. !Py f'/store/{$2}/{$1}' finally this references the first two lines (1, 2) and uses a normal Python f-string to make the path. Evaluating to '/store/ 2021-03-13T12:00:00/users.txt' Lastly we want to deliver the right data, which is the 'contents' key: data: !Py $BATCH['contents'] Here is our result! Animals and fruits, along with our users!","title":"Hello World"},{"location":"examples/hello-world.html#hello-world-a-non-trivial-example","text":"This tutorial will take you through how to write a simple (but non-trivial) Hello World example. This is the final DAG we will build. We will step through each part below. The DAG takes a YAML list of dictonaties (our data) and writes each dictionary to a file. It stores the file by run_date (hourly). name : hello_world schedule_interval : rate(1 hours) granularity : hour tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True","title":"Hello World - a non-trivial example"},{"location":"examples/hello-world.html#creating-a-task","text":"name : hello_world schedule_interval : rate(10 minutes) granularity : hour Very simply this sets you flow name (no spaces) and the schedule interval in a rate to run at. It will use a timestamp truncated to the hour for the intervals. tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot Here we are setting up our tasks in our flow. This is a DAG structure similar to many other workflow tools (e.g. Task A \u2192 Task B \u2192 Task C). Lets examine our first task, send_data , which simply outputs 3 'files' containing CSV formatted strings represented as a YAML list of dictionaries. We use flow_control.branch to yield our 3 branches to the next node.","title":"Creating a Task"},{"location":"examples/hello-world.html#connections-typhoon-status","text":"Lets start now by building this very basic task, and inspecting the output using a few options. First in your terminal, in the directory you have initialised your project, use typhoon-cli to run typhoon status . We encourage you to run this after your steps to see if any errors, or unset variables are there. Here we can see that we are initially missing a connection file without connections so lets add this. You need to add a connections.yml file to the root directory of your typhoon project. In this case we are calling our data_lake which is a local filesystem in local and a cloud S3 bucket in prod . To choose our environment we simply swap connections. We will also add an echo connection which we will use in a moment. echo : local : conn_type : echo data_lake : prod : conn_type : s3 extra : base_path : / bucket : test-corp-data-lake region_name : eu-west-1 login : <your login> passowrd : <your key> local : conn_type : local_storage extra : base_path : /tmp/data_lake Now we have this lets add our missing connections. We can get help on the client by using typhoon \u2014-help And then typhoon connection -\u2014help Finally, we can use typhoon connection add --help So now we can see how to add our connections from the connections.yml file. Note the local environment. typhoon connection add --conn-id data_lake --conn-env local typhoon connection add --conn-id echo --conn-env local Now again typhoon status A Ok!","title":"Connections &amp; typhoon status"},{"location":"examples/hello-world.html#build-run-and-debug","text":"Let's build our hello_world DAG and see some output. As a quick way lets add on echo task after our send_data task: echo_my_output : input : send_data function : typhoon.debug.echo args : mydata : !Py $BATCH $BATCH is the data packet as a consistent default reference. It can be any type so depending on your data can be shaped differently. e.g. $BATCH['key], $BATCH[0] etc. Lets build it in typhoon-cli! We can skip the help as you can now navigate the cli help the same way as for connections ( hint: typhoon dag --help ) typhoon dag build hello_world ... and run it typhoon dag run --dag-name hello_world Output of the YAML list as a list of dictionaries Now we can see what will be passed to the next node (which in our case is just echoing to the prompt).","title":"Build, Run and debug"},{"location":"examples/hello-world.html#writing-to-a-file","text":"write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True Our next task write_data receives each branch (asynchronously - more on this later for performance) from its input task. Setting a task as an input creates an edge for the data to flow linking them: send_data \u2192 write_data Next you notice we are writing to a filesystem using a standard python function filesystem.write_data . Into this function we are passing 3 arguments, the connection hook, a transformation of the data, and the path (similar to airflow ones - reusable?),","title":"Writing to a file"},{"location":"examples/hello-world.html#introducing-our-built-in-context-variables","text":"$BATCH: This is the data package passed to the next node. $BATCH_NUM: This is the iterator number if you are batching (queries for example). $HOOK: This represents your hook - you can use !Hook my_conn as syntax sugar or !Py $HOOK.myconn $VARIABLE: This is how you can access saved variables e.g. lists of tables, query templates etc. An example might be !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) $DAG_CONTEXT.interval_start & $DAG_CONTEXT.interval_end: Execution interval for example: $DAG_CONTEXT.interval_start \u2192 '2021-05-23 10:00:00' (inclusive) $DAG_CONTEXT.interval_end \u2192 '2021-05-23 11:00:00' (exclusive)","title":"Introducing our built in context variables"},{"location":"examples/hello-world.html#passing-data","text":"The notation !Py indicates that what comes after is evaluated as normal python code. $BATCH is the data packet as a consistent default reference. It can be any type so depending on your data can be shaped differently e.g. $BATCH['key], $BATCH[0] etc. In our case remember each branch yields a list of dictionaries with two keys [{'filename': 'users.txt', 'contents':'['John', 'Amy', 'Adam', 'Jane']},{'filename':...}] The most complex item here is a !Multistep process showing how you can do multiple small python transformations in series in a a nice readable way (or you can put it in one line of course): !Py $BATCH['filename'] will evaluate to \"users.txt' !Py $DAG_CONTEXT.interval_end will evaluate to the timestamp of the DAG run. This is a built in context variable. !Py f'/store/{$2}/{$1}' finally this references the first two lines (1, 2) and uses a normal Python f-string to make the path. Evaluating to '/store/ 2021-03-13T12:00:00/users.txt' Lastly we want to deliver the right data, which is the 'contents' key: data: !Py $BATCH['contents'] Here is our result! Animals and fruits, along with our users!","title":"Passing data"},{"location":"examples/mysql-to-snowflake.html","text":"MySql to Snowflake Now we know the basics, lets make something more useful. For brevity the full connections YAML is included below for you to copy and replace your keys. However, we will be quite thorough to make sure each step is clear. In this example, I need to take two tables ['clients', 'sales'] from our production MySql (replace with your favourite RDBMS) to Snowflake (replace with your favourite cloud DWH). Sample data for MySQL is included at the bottom. We are going to load our data into a VARIANT field in Snowflake as this pattern is very fault tolerant . Overview of what we want to do: List the tables from Typhoon variables. Extract each table from MySQL Write each table to S3 in JSON Creating a function to output to JSON Switching our connection to S3 Copy the files into snowflake DAG : list_tables \u2192 extract_tables \u2192 write_data_S3 \u2192 copy_to_snowflake_stage We will learn how to sample and test the flows as we build. Here is the final expected output: name : dwh_flow schedule_interval : rate(1 hours) granularity : hour tasks : list_tables : function : typhoon.flow_control.branch args : branches : - clients - sales extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Hook transaction_db batch_size : 10 query_params : interval_start : !Py $DAG_CONTEXT.interval_start interval_end : !Py $DAG_CONTEXT.interval_end metadata : table_name : !Py $BATCH query : !MultiStep - !Py table_name=$BATCH - !Py f\"SELECT * FROM {table_name} where creation_timestamp between %(interval_start)s and %(interval_end)s\" write_data_S3 : input : extract_tables function : typhoon.filesystem.write_data args : hook : !Py $HOOK.data_lake data : !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) path : !Py f\"data_{$BATCH.metadata['table_name']}_batch_num_\" + str($BATCH_NUM) + \"_\" + str($DAG_CONTEXT.interval_end).replace(\":\", \"_\") + \".json\" metadata : !Py $BATCH.metadata copy_to_snowflake_stage : input : write_data_S3 function : typhoon.snowflake.copy_into args : hook : !Py $HOOK.snowflake table : !Py $BATCH.metadata['table_name'] stage_name : stagetestcorpdatalake s3_path : '' Lets create a file in the /my_project_root/dags folder called dwh_flow.yml . List_tables name : dwh_flow schedule_interval : rate(1 hours) add granularity : hour tasks : list_tables : function : typhoon.flow_control.branch args : branches : - clients - sales echo_my_tables : input : list_tables function : typhoon.debug.echo args : mydata : !Py $BATCH Life is simple when you only have two tables - if only! Let's extract these hourly. We will improve this with variables later. Let's run typhoon status , build it and run it: typhoon dag build dwh_flow typhoon dag run --dag-name dwh_flow You can see we are simply outputting the tables in the list. Extracting from MySQL extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Py $HOOK.transaction_db batch_size : 10 metadata : table_name : !Py $BATCH query : !Py \"SELECT * FROM {table_name}\" .format(table_name=$BATCH) Ok, here there are a few things going on: We are applying relational.execute_query on each table to query on our $HOOK.transaction_db connection. This is pointing to our MySQL database that records our customer transactions. We are saving the table names for use downstream in the flow in our metadata dictionary. We are formatting our query using normal python. Again, later in this tutorial we will use Variables to store our dynamic SQL . Ok, so we will output the whole table each hour. Perhaps not so smart , so lets add something more reasonable. extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Hook echodb batch_size : 10 query_params : interval_start : !Py $DAG_CONTEXT.interval_start interval_end : !Py $DAG_CONTEXT.interval_end metadata : table_name : !Py $BATCH query : !MultiStep - !Py table_name=$BATCH - !Py f\"SELECT * FROM {table_name} where creation_timestamp between %(interval_start)s and %(interval_end)s\" Here we are making two changes. The obvious one is to apply the $DAG_CONTEXT.interval_start and $DAG_CONTEXT.interval_end as query parameters in the query (note its MySQL paramstyle). This means we will pull those transactions created within the hour interval. Much more sensible! Note: you could of course treat this like the table as a string format (but not the other way around; you cannot use query parameters for the table or schema). The second thing is we are temporarily using a new hook: echodb . This is similar to our echo task but will show us the sql that would be passed to MySQL. We need to add the connection (make sure its in the connection.yml which is available to copy at the bottom). typhoon connection add --conn-id echodb --conn-env local Looks correct SQL. Remember to change your hook back to hook: !Py $HOOK.transaction_db Lets add our Mysql hook and run to an echo task: typhoon connection add --conn-id transaction_db --conn-env prod Because we are developing we might want to select a specific execution date: typhoon dag build dwh_flow typhoon status typhoon dag run --dag-name dwh_flow --execution-date 2021-05-20T14:00:00 Our (faked) data from the MySql echoed Do so now its outputting our selected tables within the time interval in batches of 10 rows (of course you would raise this). Next lets land this in files to S3. Write each table to S3 in JSON write_data_S3 : input : extract_tables function : typhoon.filesystem.write_data args : hook : !Py $HOOK.data_lake data : !Py $BATCH.batch path : !Py f\"data_{$BATCH.metadata['table_name']}_batch_num_\" + str($BATCH_NUM) + \"_\" + str($DAG_CONTEXT.ts).replace(\":\", \"_\") + \".json\" metadata : !Py $BATCH.metadata Ok, now its starting to look familiar (I hope). Our function filesystem.write_data will write to a filesystem. Notice this is the same connection YAML used in hello_world. All our functions, connections and components are re-usable. Lets start with the local environment to test it to a local filepath. typhoon connection add --conn-id data_lake --conn-env **local** Running this we should now see the following (see the connection for where this will land): So we must transform the data to bytes. In fact we need to write it into JSON so that we can use Snowflake's VARIANT column for ingestion. We strongly recommend this pattern. We are passing this function data from the extract_tables task above. MySQL outputs some tuples so we need a transformation function that will turn these tuples into JSON (1 JSON object per row). (Optional) reating a function to output to JSON Info This step is optional as it already is included in typhoon. So its only to illustrate how to roll your own function. So, let's make our own function for this. In the transformations folder we can add function to the data file (or you can make a new one): Typhoon is easily extensible. from io import StringIO , BytesIO from typing import Union import simplejson as json def list_of_tuples_to_json ( data : Union [ list ], field_names : list ): list_of_dicts = [ dict ( zip ( field_names , tup )) for tup in data ] d_out = BytesIO () for d in list_of_dicts : d_out . write ( json . dumps ( d , cls = TestEncoder , namedtuple_as_object = True ) . encode ()) d_out . seek ( 0 ) return d_out Then we can call this in the DAG by replacing data with: data: !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) So now, before we write to the file we are transforming our tuples to return rows of json doc as bytes. Building and running the DAG now gives us: So lets see how we can debug the Python code directly. One key advantage of developing in Typhoon is that the YAML compiles to very readable, normal python functions. You can debug in an IDE of your choice as normal or run from prompt. Building a DAG compiles to the 'out' folder Note that we can set the example task to debug with in dwh_flow.py . For example here we are setting a test execution date. by altering the example_event datetime. if __name__ == '__main__' : import os ** example_event = { 'time' : '2021-05-20T14:00:00Z' } ** example_event_task = { 'type' : 'task' , 'dag_name' : '' , 'task_name' : 'list_tables_branches' , 'trigger' : 'dag' , 'attempt' : 1 , 'args' : [], 'kwargs' : { 'dag_context' : DagContext . from_cron_and_event_time ( schedule_interval = 'rate(1 hours)' , event_time = example_event [ 'time' ], granularity = 'hour' , ) . dict () }, } dwh_flow_main ( example_event , None ) We can find the point in the code that is relevant for our JSON config [ 'data' ] = transformations . data . list_of_tuples_to_json ( list ( batch . batch ), batch . columns ) So you can develop your function to correctly encode the datetime: from io import StringIO , BytesIO from typing import Union import pandas as pd import simplejson as json from datetime import datetime class DateTimeEncoder ( json . JSONEncoder ): def default ( self , obj ): if hasattr ( obj , 'isoformat' ): return obj . isoformat () else : return json . JSONEncoder . default ( self , obj ) def list_of_tuples_to_json ( data : Union [ list ], field_names : list ): list_of_dicts = [ dict ( zip ( field_names , tup )) for tup in data ] d_out = BytesIO () for d in list_of_dicts : d_out . write ( json . dumps ( d , cls = DateTimeEncoder , namedtuple_as_object = True ) . encode ()) d_out . write ( str ( ' \\n ' ) . encode ()) d_out . seek ( 0 ) return d_out Then we should add this to our YAML: data: !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) This is passing $BATCH.batch and the $BATCH.columns to this function that we will add. It is very simply transforming tuples from the resultset to JSON using our new function above. Now if we run it again, either in the cli or the dwh_flow.py directly we should get an output to our local filesystem like: Our (fake) data exported as JSON in batches of 10 rows Switching to S3 The final step of this section is to switch the connection to our S3 bucket (i.e. production): typhoon connection add --conn-id data_lake --conn-env **prod** When we re-run the DAG we will see the results are now landing in S3 (see YAML connections at end of this for hints on getting the connection right). And in the bucket : Copy data to Snowflake Ok, now for the final step - we need to upload these files to snowflake. To prepare snowflake to receive from S3 you need to set up an S3 Stage ( See Snowflake docs) . Let's add our final task to the DAG YAML: copy_to_snowflake_stage : input : write_data_S3 function : typhoon.snowflake.copy_into args : hook : !Py $HOOK.snowflake table : !Py $BATCH.metadata['table_name'] stage_name : stagetestcorpdatalake s3_path : '' Before we run it we need to create our staging tables. We are using a VARIANT loading field to land the data: CREATE OR REPLACE TABLE clients ( data_var variant , etl_id varchar ( 100 ), etl_timestamp datetime , etl_filename varchar ( 100 ) ) ; CREATE OR REPLACE TABLE sales ( data_var variant , etl_id varchar ( 100 ), etl_timestamp datetime , etl_filename varchar ( 100 ) ) ; We need to add our connection (see YAML at the end for the connection): typhoon connection add --conn-id snowflake --conn-env prod Now we can build our flow for the final time and run it typhoon dag build dwh_flow typhoon dag run --dag-name dwh_flow --execution-date 2021-05-20T14:00:00 ... and we have data in Snowflake! Some potential optimisations We are running COPY for each file we land. This isn't optimal, even though snowflake is protecting against duplicating the data. We can tackle this chaining together DAGS by writing a 'success file' . Listing tables in a variable More templating Creating a Component! We will cover these in other blogs. For now...well done you made it! Helper Assets YAML connections echo : local : conn_type : echo echodb : local : conn_type : echodb schema : main data_lake : prod : conn_type : s3 extra : base_path : / bucket : your-bucket-here region_name : your-region-here login : SECRETLOGIN passowrd : SECRETKEY local : conn_type : local_storage extra : base_path : /tmp/data_lake transaction_db : dev : conn_type : sqlite schema : main extra : database : /tmp/web_trans_local.db prod : conn_type : sqlalchemy login : your-login-name password : your-password host : localhost port : 3306 schema : prod_web_ecom extra : dialect : mysql driver : mysqldb database : prod_web_ecom snowflake : prod : conn_type : snowflake login : your-login password : your-password schema : staging extra : account : your-account region : your-region database : your-db warehouse : WAREHOUSE_NAME role : your-role Mock data Sample MySQL Data.zip","title":"MySql to Snowflake"},{"location":"examples/mysql-to-snowflake.html#mysql-to-snowflake","text":"Now we know the basics, lets make something more useful. For brevity the full connections YAML is included below for you to copy and replace your keys. However, we will be quite thorough to make sure each step is clear. In this example, I need to take two tables ['clients', 'sales'] from our production MySql (replace with your favourite RDBMS) to Snowflake (replace with your favourite cloud DWH). Sample data for MySQL is included at the bottom. We are going to load our data into a VARIANT field in Snowflake as this pattern is very fault tolerant . Overview of what we want to do: List the tables from Typhoon variables. Extract each table from MySQL Write each table to S3 in JSON Creating a function to output to JSON Switching our connection to S3 Copy the files into snowflake DAG : list_tables \u2192 extract_tables \u2192 write_data_S3 \u2192 copy_to_snowflake_stage We will learn how to sample and test the flows as we build. Here is the final expected output: name : dwh_flow schedule_interval : rate(1 hours) granularity : hour tasks : list_tables : function : typhoon.flow_control.branch args : branches : - clients - sales extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Hook transaction_db batch_size : 10 query_params : interval_start : !Py $DAG_CONTEXT.interval_start interval_end : !Py $DAG_CONTEXT.interval_end metadata : table_name : !Py $BATCH query : !MultiStep - !Py table_name=$BATCH - !Py f\"SELECT * FROM {table_name} where creation_timestamp between %(interval_start)s and %(interval_end)s\" write_data_S3 : input : extract_tables function : typhoon.filesystem.write_data args : hook : !Py $HOOK.data_lake data : !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) path : !Py f\"data_{$BATCH.metadata['table_name']}_batch_num_\" + str($BATCH_NUM) + \"_\" + str($DAG_CONTEXT.interval_end).replace(\":\", \"_\") + \".json\" metadata : !Py $BATCH.metadata copy_to_snowflake_stage : input : write_data_S3 function : typhoon.snowflake.copy_into args : hook : !Py $HOOK.snowflake table : !Py $BATCH.metadata['table_name'] stage_name : stagetestcorpdatalake s3_path : '' Lets create a file in the /my_project_root/dags folder called dwh_flow.yml .","title":"MySql to Snowflake"},{"location":"examples/mysql-to-snowflake.html#list_tables","text":"name : dwh_flow schedule_interval : rate(1 hours) add granularity : hour tasks : list_tables : function : typhoon.flow_control.branch args : branches : - clients - sales echo_my_tables : input : list_tables function : typhoon.debug.echo args : mydata : !Py $BATCH Life is simple when you only have two tables - if only! Let's extract these hourly. We will improve this with variables later. Let's run typhoon status , build it and run it: typhoon dag build dwh_flow typhoon dag run --dag-name dwh_flow You can see we are simply outputting the tables in the list.","title":"List_tables"},{"location":"examples/mysql-to-snowflake.html#extracting-from-mysql","text":"extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Py $HOOK.transaction_db batch_size : 10 metadata : table_name : !Py $BATCH query : !Py \"SELECT * FROM {table_name}\" .format(table_name=$BATCH) Ok, here there are a few things going on: We are applying relational.execute_query on each table to query on our $HOOK.transaction_db connection. This is pointing to our MySQL database that records our customer transactions. We are saving the table names for use downstream in the flow in our metadata dictionary. We are formatting our query using normal python. Again, later in this tutorial we will use Variables to store our dynamic SQL . Ok, so we will output the whole table each hour. Perhaps not so smart , so lets add something more reasonable. extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Hook echodb batch_size : 10 query_params : interval_start : !Py $DAG_CONTEXT.interval_start interval_end : !Py $DAG_CONTEXT.interval_end metadata : table_name : !Py $BATCH query : !MultiStep - !Py table_name=$BATCH - !Py f\"SELECT * FROM {table_name} where creation_timestamp between %(interval_start)s and %(interval_end)s\" Here we are making two changes. The obvious one is to apply the $DAG_CONTEXT.interval_start and $DAG_CONTEXT.interval_end as query parameters in the query (note its MySQL paramstyle). This means we will pull those transactions created within the hour interval. Much more sensible! Note: you could of course treat this like the table as a string format (but not the other way around; you cannot use query parameters for the table or schema). The second thing is we are temporarily using a new hook: echodb . This is similar to our echo task but will show us the sql that would be passed to MySQL. We need to add the connection (make sure its in the connection.yml which is available to copy at the bottom). typhoon connection add --conn-id echodb --conn-env local Looks correct SQL. Remember to change your hook back to hook: !Py $HOOK.transaction_db Lets add our Mysql hook and run to an echo task: typhoon connection add --conn-id transaction_db --conn-env prod Because we are developing we might want to select a specific execution date: typhoon dag build dwh_flow typhoon status typhoon dag run --dag-name dwh_flow --execution-date 2021-05-20T14:00:00 Our (faked) data from the MySql echoed Do so now its outputting our selected tables within the time interval in batches of 10 rows (of course you would raise this). Next lets land this in files to S3.","title":"Extracting from MySQL"},{"location":"examples/mysql-to-snowflake.html#write-each-table-to-s3-in-json","text":"write_data_S3 : input : extract_tables function : typhoon.filesystem.write_data args : hook : !Py $HOOK.data_lake data : !Py $BATCH.batch path : !Py f\"data_{$BATCH.metadata['table_name']}_batch_num_\" + str($BATCH_NUM) + \"_\" + str($DAG_CONTEXT.ts).replace(\":\", \"_\") + \".json\" metadata : !Py $BATCH.metadata Ok, now its starting to look familiar (I hope). Our function filesystem.write_data will write to a filesystem. Notice this is the same connection YAML used in hello_world. All our functions, connections and components are re-usable. Lets start with the local environment to test it to a local filepath. typhoon connection add --conn-id data_lake --conn-env **local** Running this we should now see the following (see the connection for where this will land): So we must transform the data to bytes. In fact we need to write it into JSON so that we can use Snowflake's VARIANT column for ingestion. We strongly recommend this pattern. We are passing this function data from the extract_tables task above. MySQL outputs some tuples so we need a transformation function that will turn these tuples into JSON (1 JSON object per row).","title":"Write each table to S3 in JSON"},{"location":"examples/mysql-to-snowflake.html#optional-reating-a-function-to-output-to-json","text":"Info This step is optional as it already is included in typhoon. So its only to illustrate how to roll your own function. So, let's make our own function for this. In the transformations folder we can add function to the data file (or you can make a new one): Typhoon is easily extensible. from io import StringIO , BytesIO from typing import Union import simplejson as json def list_of_tuples_to_json ( data : Union [ list ], field_names : list ): list_of_dicts = [ dict ( zip ( field_names , tup )) for tup in data ] d_out = BytesIO () for d in list_of_dicts : d_out . write ( json . dumps ( d , cls = TestEncoder , namedtuple_as_object = True ) . encode ()) d_out . seek ( 0 ) return d_out Then we can call this in the DAG by replacing data with: data: !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) So now, before we write to the file we are transforming our tuples to return rows of json doc as bytes. Building and running the DAG now gives us: So lets see how we can debug the Python code directly. One key advantage of developing in Typhoon is that the YAML compiles to very readable, normal python functions. You can debug in an IDE of your choice as normal or run from prompt. Building a DAG compiles to the 'out' folder Note that we can set the example task to debug with in dwh_flow.py . For example here we are setting a test execution date. by altering the example_event datetime. if __name__ == '__main__' : import os ** example_event = { 'time' : '2021-05-20T14:00:00Z' } ** example_event_task = { 'type' : 'task' , 'dag_name' : '' , 'task_name' : 'list_tables_branches' , 'trigger' : 'dag' , 'attempt' : 1 , 'args' : [], 'kwargs' : { 'dag_context' : DagContext . from_cron_and_event_time ( schedule_interval = 'rate(1 hours)' , event_time = example_event [ 'time' ], granularity = 'hour' , ) . dict () }, } dwh_flow_main ( example_event , None ) We can find the point in the code that is relevant for our JSON config [ 'data' ] = transformations . data . list_of_tuples_to_json ( list ( batch . batch ), batch . columns ) So you can develop your function to correctly encode the datetime: from io import StringIO , BytesIO from typing import Union import pandas as pd import simplejson as json from datetime import datetime class DateTimeEncoder ( json . JSONEncoder ): def default ( self , obj ): if hasattr ( obj , 'isoformat' ): return obj . isoformat () else : return json . JSONEncoder . default ( self , obj ) def list_of_tuples_to_json ( data : Union [ list ], field_names : list ): list_of_dicts = [ dict ( zip ( field_names , tup )) for tup in data ] d_out = BytesIO () for d in list_of_dicts : d_out . write ( json . dumps ( d , cls = DateTimeEncoder , namedtuple_as_object = True ) . encode ()) d_out . write ( str ( ' \\n ' ) . encode ()) d_out . seek ( 0 ) return d_out Then we should add this to our YAML: data: !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) This is passing $BATCH.batch and the $BATCH.columns to this function that we will add. It is very simply transforming tuples from the resultset to JSON using our new function above. Now if we run it again, either in the cli or the dwh_flow.py directly we should get an output to our local filesystem like: Our (fake) data exported as JSON in batches of 10 rows","title":"(Optional) reating a function to output to JSON"},{"location":"examples/mysql-to-snowflake.html#switching-to-s3","text":"The final step of this section is to switch the connection to our S3 bucket (i.e. production): typhoon connection add --conn-id data_lake --conn-env **prod** When we re-run the DAG we will see the results are now landing in S3 (see YAML connections at end of this for hints on getting the connection right). And in the bucket :","title":"Switching to S3"},{"location":"examples/mysql-to-snowflake.html#copy-data-to-snowflake","text":"Ok, now for the final step - we need to upload these files to snowflake. To prepare snowflake to receive from S3 you need to set up an S3 Stage ( See Snowflake docs) . Let's add our final task to the DAG YAML: copy_to_snowflake_stage : input : write_data_S3 function : typhoon.snowflake.copy_into args : hook : !Py $HOOK.snowflake table : !Py $BATCH.metadata['table_name'] stage_name : stagetestcorpdatalake s3_path : '' Before we run it we need to create our staging tables. We are using a VARIANT loading field to land the data: CREATE OR REPLACE TABLE clients ( data_var variant , etl_id varchar ( 100 ), etl_timestamp datetime , etl_filename varchar ( 100 ) ) ; CREATE OR REPLACE TABLE sales ( data_var variant , etl_id varchar ( 100 ), etl_timestamp datetime , etl_filename varchar ( 100 ) ) ; We need to add our connection (see YAML at the end for the connection): typhoon connection add --conn-id snowflake --conn-env prod Now we can build our flow for the final time and run it typhoon dag build dwh_flow typhoon dag run --dag-name dwh_flow --execution-date 2021-05-20T14:00:00 ... and we have data in Snowflake! Some potential optimisations We are running COPY for each file we land. This isn't optimal, even though snowflake is protecting against duplicating the data. We can tackle this chaining together DAGS by writing a 'success file' . Listing tables in a variable More templating Creating a Component! We will cover these in other blogs. For now...well done you made it!","title":"Copy data to Snowflake"},{"location":"examples/mysql-to-snowflake.html#helper-assets","text":"","title":"Helper Assets"},{"location":"examples/mysql-to-snowflake.html#yaml-connections","text":"echo : local : conn_type : echo echodb : local : conn_type : echodb schema : main data_lake : prod : conn_type : s3 extra : base_path : / bucket : your-bucket-here region_name : your-region-here login : SECRETLOGIN passowrd : SECRETKEY local : conn_type : local_storage extra : base_path : /tmp/data_lake transaction_db : dev : conn_type : sqlite schema : main extra : database : /tmp/web_trans_local.db prod : conn_type : sqlalchemy login : your-login-name password : your-password host : localhost port : 3306 schema : prod_web_ecom extra : dialect : mysql driver : mysqldb database : prod_web_ecom snowflake : prod : conn_type : snowflake login : your-login password : your-password schema : staging extra : account : your-account region : your-region database : your-db warehouse : WAREHOUSE_NAME role : your-role","title":"YAML connections"},{"location":"examples/mysql-to-snowflake.html#mock-data","text":"Sample MySQL Data.zip","title":"Mock data"},{"location":"examples/pandas-examples.html","text":"Simple Pandas Examples Using Pandas means we get access to a hugely powerful and well known library. It's simply the easiest way to transform data many complex tasks: time-series analysis complex merging cleaning pivoting / melting etc. Here is a very trivial example to illustrate two ways you can access Pandas in a DAG. From this you can integrate existing code or quickly add Pandas into your automations. Direct usage in DAG YAML It's very easy to use Pandas directly in !MultiStep transformations without even creating a function. Here we have loaded a mock data set of test scores (attached) and created a small function to load the json. name : pandas_example schedule_interval : rate(1 hour) tasks : load_json : function : typhoon.filesystem.read_data args : hook : !Hook data_lake path : /test_scores.json pandas_transform : input : load_json function : typhoon.debug.echo args : data : !MultiStep - !Py transformations.data.json_loads_to_dict($BATCH.data) - !Py typhoon.data.to_dataframe($1) - !Py $2.groupby('Country').mean()['Score'] - !Py $2.merge($3, how='inner', on='Country') - !Py $4.rename(columns={\"Score_x\":\"Score\",\"Score_y\":\"Country_median_score\"}) The simple function to load the json: def json_loads_to_dict ( data : Union [ str , bytes ]) -> dict : import json return json . loads ( data ) Data set - mock test scores Usage in functions We can also wrap this into a function: def df_add_country_median ( data : pd . DataFrame ) -> pd . DataFrame : df = data . groupby ( 'Country' ) . mean ()[ 'Score' ] data = data . merge ( df , how = 'inner' , on = 'Country' ) return data . rename ( columns = { \"Score_x\" : \"Score\" , \"Score_y\" : \"Country_median_score\" }) This can then simplify the DAG: pandas_transform : input : load_json function : typhoon.debug.echo args : data : !MultiStep - !Py transformations.data.json_loads_to_dict($BATCH.data) - !Py typhoon.data.to_dataframe($1) - !Py transformations.data.df_add_country_median($2)","title":"Pandas automation"},{"location":"examples/pandas-examples.html#simple-pandas-examples","text":"Using Pandas means we get access to a hugely powerful and well known library. It's simply the easiest way to transform data many complex tasks: time-series analysis complex merging cleaning pivoting / melting etc. Here is a very trivial example to illustrate two ways you can access Pandas in a DAG. From this you can integrate existing code or quickly add Pandas into your automations.","title":"Simple Pandas Examples"},{"location":"examples/pandas-examples.html#direct-usage-in-dag-yaml","text":"It's very easy to use Pandas directly in !MultiStep transformations without even creating a function. Here we have loaded a mock data set of test scores (attached) and created a small function to load the json. name : pandas_example schedule_interval : rate(1 hour) tasks : load_json : function : typhoon.filesystem.read_data args : hook : !Hook data_lake path : /test_scores.json pandas_transform : input : load_json function : typhoon.debug.echo args : data : !MultiStep - !Py transformations.data.json_loads_to_dict($BATCH.data) - !Py typhoon.data.to_dataframe($1) - !Py $2.groupby('Country').mean()['Score'] - !Py $2.merge($3, how='inner', on='Country') - !Py $4.rename(columns={\"Score_x\":\"Score\",\"Score_y\":\"Country_median_score\"}) The simple function to load the json: def json_loads_to_dict ( data : Union [ str , bytes ]) -> dict : import json return json . loads ( data ) Data set - mock test scores","title":"Direct usage in DAG YAML"},{"location":"examples/pandas-examples.html#usage-in-functions","text":"We can also wrap this into a function: def df_add_country_median ( data : pd . DataFrame ) -> pd . DataFrame : df = data . groupby ( 'Country' ) . mean ()[ 'Score' ] data = data . merge ( df , how = 'inner' , on = 'Country' ) return data . rename ( columns = { \"Score_x\" : \"Score\" , \"Score_y\" : \"Country_median_score\" }) This can then simplify the DAG: pandas_transform : input : load_json function : typhoon.debug.echo args : data : !MultiStep - !Py transformations.data.json_loads_to_dict($BATCH.data) - !Py typhoon.data.to_dataframe($1) - !Py transformations.data.df_add_country_median($2)","title":"Usage in functions"},{"location":"getting-started/connections.html","text":"Connections During development, we may want to use different connections to test our DAGs. Also, for the same connections we might want different details per environment . For example, we may start development writing to a local file and after that is working correctly switch the connection to S3 to finish testing. Finally we want to move this to the production S3 bucket in a seamless way. data_lake workflow local -> disk dev -> dev S3 bucket prod -> production S3 bucket For this specific purpose there is a file called connections.yml . This is where we will define all our connection details for our project for all environments. Connections YAML For example in this project we will write data using a connection called data_lake which has two connection environments, local which writes to a local file and test which writes to S3. It is possible to use them interchangeably since they both implement the same interface: FileSystemHook . connections.yml: data_lake : local : conn_type : local_storage extra : base_path : /tmp/data_lake/ test : conn_type : s3 extra : bucket : my-typhoon-test-bucket connections.yml is not versioned The connections.yml file may contain passwords so it should never be versioned. That is why it's included in the .gitignore file for projects generated with the CLI. Adding the connection Following the advice we got from the typhoon status command we will now add the connection to the metadata database. We will add the local data_lake connection with the command: typhoon connection add --conn-id data_lake --conn-env local # Check that it was added typhoon connection ls -l # add the echo one too for completeness typhoon connection add --conn-id echo --conn-env local If we run the status command again we will see that everything is ok in our project now: typhoon status Connections types (Hooks) Typhoon hooks represent the same thing as airlflow hooks , so if you are familiar it's an easy concept. They are the interface to external platforms and databases. You use the connections.yml to choose which hook to use and configure it. You can extend typhoon to connect to new platforms by adding Hooks. It comes with many popular ones already. Hooks available: File System - local, S3, GC storage, Ftp DB API - most DBs can use this, e.g. MySql, MSSQL, Postgres, Redshift Snowflake specific flavour = SQLAlchemy - most DBs can use this AWS - AWS Session, DynamoDB Singer - You can use any singer taps in your DAG as a task. singer can connect to Any you can add/extend your own, of course.","title":"Adding connections"},{"location":"getting-started/connections.html#connections","text":"During development, we may want to use different connections to test our DAGs. Also, for the same connections we might want different details per environment . For example, we may start development writing to a local file and after that is working correctly switch the connection to S3 to finish testing. Finally we want to move this to the production S3 bucket in a seamless way. data_lake workflow local -> disk dev -> dev S3 bucket prod -> production S3 bucket For this specific purpose there is a file called connections.yml . This is where we will define all our connection details for our project for all environments.","title":"Connections"},{"location":"getting-started/connections.html#connections-yaml","text":"For example in this project we will write data using a connection called data_lake which has two connection environments, local which writes to a local file and test which writes to S3. It is possible to use them interchangeably since they both implement the same interface: FileSystemHook . connections.yml: data_lake : local : conn_type : local_storage extra : base_path : /tmp/data_lake/ test : conn_type : s3 extra : bucket : my-typhoon-test-bucket connections.yml is not versioned The connections.yml file may contain passwords so it should never be versioned. That is why it's included in the .gitignore file for projects generated with the CLI.","title":"Connections YAML"},{"location":"getting-started/connections.html#adding-the-connection","text":"Following the advice we got from the typhoon status command we will now add the connection to the metadata database. We will add the local data_lake connection with the command: typhoon connection add --conn-id data_lake --conn-env local # Check that it was added typhoon connection ls -l # add the echo one too for completeness typhoon connection add --conn-id echo --conn-env local If we run the status command again we will see that everything is ok in our project now: typhoon status","title":"Adding the connection"},{"location":"getting-started/connections.html#connections-types-hooks","text":"Typhoon hooks represent the same thing as airlflow hooks , so if you are familiar it's an easy concept. They are the interface to external platforms and databases. You use the connections.yml to choose which hook to use and configure it. You can extend typhoon to connect to new platforms by adding Hooks. It comes with many popular ones already. Hooks available: File System - local, S3, GC storage, Ftp DB API - most DBs can use this, e.g. MySql, MSSQL, Postgres, Redshift Snowflake specific flavour = SQLAlchemy - most DBs can use this AWS - AWS Session, DynamoDB Singer - You can use any singer taps in your DAG as a task. singer can connect to Any you can add/extend your own, of course.","title":"Connections types (Hooks)"},{"location":"getting-started/creating-a-dag.html","text":"Creating a DAG Simply, the DAG is the workflow task you are composing. This is a DAG structure similar to many other workflow tools (e.g. Task A \u2192 Task B \u2192 Task C). However, the writing of a DAG in Typhoon can be much quicker and more intuitive due to the simpler YAML syntax and that it shares data. Concepts: tasks: this defines each node (or step) in the DAG workflow (i.e. do something). functions : this is the operation to carry out (e.g. read data, write data, branches, if, etc.). transformations : stored functions that are re-usable and testable (e.g. gzip, dicts_to_csv). components : re-usable sets of multi-task DAGS (e.g. glob_compress_and_copy, if, db_to_swowflake). Can be used as standalone or in a task. DAG code We can check an example DAG in dags/hello_world.yml . You can also check it with the CLI by running typhoon dag definition --dag-name hello_world . Tip To get some editor completion and hints you can configure the JSON schema in dag_schema.json as the schema for the DAG YAML file. This can be done in VS Code with the Red Hat YAML extension or in PyCharm natively . name : hello_world schedule_interval : rate(1 hours) # Can also be a Cron expression granularity : hour tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True This DAG has three branches, each with a file name and list (YAML list of dictionaries). This will write 3 files; users.txt, animals.txt and fruits.csv. Typically, this first step might be a database, set of CSVs or an API. This is just a trivial example including data. Skim the code For a step-by-step on this example - please start on our example Hello World walkthrough . DAG basic concepts The DAGs are composed in YAML . They are compiled to python. This means that once you have written your YAML and you build it the output is a normal, testable python file. It transpiles to Airflow compatible DAGS also. Basic components: name: ... schedule_interval: can be a Rate or a Cron expression to express how often it should run. granularity: minutes, hour, hours, day, days ... or a Cron expression. tasks: this defines each node (or step) in the DAG workflow (i.e. do something) input: here we are connecting the input from the previous task (A -> B; B.input = A). function: this is the operation to carry out (e.g. read data, write data, branches, if, etc.). Full function list for reference. args: these are the specific arguments for the function (so they will differ). Common ones are below: hook: this is the connection to use that you have added from your connectioons.yml path: this is the path, typical in filesystem hooks to read / write files data: this is the batch of data we are passing between tasks, referenced by $BATCH Syntax sugars: - !Hook: e.g. !Hook data_lake is equivilent of $HOOK.data_lake . I.e. to get the connection. - !MultiStep: allows you to make multi-line scripts easily (see example). Useful for chaining together a few Pandas transformations for example. - !Py: this evaluates the line as normal python (allowing total flexibility therefore to apply your own transformations). Running the DAG We can run this with: typhoon dag run --dag-name hello_world We can check the files that have been written to /tmp/data_lake/store/ where /tmp/data_lake/ was the base directory defined for our connection (go back to connections.yml to check) and the DAG wrote to /store/[FILE_NAME] .","title":"Creating your DAG"},{"location":"getting-started/creating-a-dag.html#creating-a-dag","text":"Simply, the DAG is the workflow task you are composing. This is a DAG structure similar to many other workflow tools (e.g. Task A \u2192 Task B \u2192 Task C). However, the writing of a DAG in Typhoon can be much quicker and more intuitive due to the simpler YAML syntax and that it shares data. Concepts: tasks: this defines each node (or step) in the DAG workflow (i.e. do something). functions : this is the operation to carry out (e.g. read data, write data, branches, if, etc.). transformations : stored functions that are re-usable and testable (e.g. gzip, dicts_to_csv). components : re-usable sets of multi-task DAGS (e.g. glob_compress_and_copy, if, db_to_swowflake). Can be used as standalone or in a task.","title":"Creating a DAG"},{"location":"getting-started/creating-a-dag.html#dag-code","text":"We can check an example DAG in dags/hello_world.yml . You can also check it with the CLI by running typhoon dag definition --dag-name hello_world . Tip To get some editor completion and hints you can configure the JSON schema in dag_schema.json as the schema for the DAG YAML file. This can be done in VS Code with the Red Hat YAML extension or in PyCharm natively . name : hello_world schedule_interval : rate(1 hours) # Can also be a Cron expression granularity : hour tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True This DAG has three branches, each with a file name and list (YAML list of dictionaries). This will write 3 files; users.txt, animals.txt and fruits.csv. Typically, this first step might be a database, set of CSVs or an API. This is just a trivial example including data. Skim the code For a step-by-step on this example - please start on our example Hello World walkthrough .","title":"DAG code"},{"location":"getting-started/creating-a-dag.html#dag-basic-concepts","text":"The DAGs are composed in YAML . They are compiled to python. This means that once you have written your YAML and you build it the output is a normal, testable python file. It transpiles to Airflow compatible DAGS also. Basic components: name: ... schedule_interval: can be a Rate or a Cron expression to express how often it should run. granularity: minutes, hour, hours, day, days ... or a Cron expression. tasks: this defines each node (or step) in the DAG workflow (i.e. do something) input: here we are connecting the input from the previous task (A -> B; B.input = A). function: this is the operation to carry out (e.g. read data, write data, branches, if, etc.). Full function list for reference. args: these are the specific arguments for the function (so they will differ). Common ones are below: hook: this is the connection to use that you have added from your connectioons.yml path: this is the path, typical in filesystem hooks to read / write files data: this is the batch of data we are passing between tasks, referenced by $BATCH Syntax sugars: - !Hook: e.g. !Hook data_lake is equivilent of $HOOK.data_lake . I.e. to get the connection. - !MultiStep: allows you to make multi-line scripts easily (see example). Useful for chaining together a few Pandas transformations for example. - !Py: this evaluates the line as normal python (allowing total flexibility therefore to apply your own transformations).","title":"DAG basic concepts"},{"location":"getting-started/creating-a-dag.html#running-the-dag","text":"We can run this with: typhoon dag run --dag-name hello_world We can check the files that have been written to /tmp/data_lake/store/ where /tmp/data_lake/ was the base directory defined for our connection (go back to connections.yml to check) and the DAG wrote to /store/[FILE_NAME] .","title":"Running the DAG"},{"location":"getting-started/creating-components.html","text":"Creating Components @todo - extended guide on making a Pandas example @todo - extended guide on making a csv exporter to ftp @todo - extended guide on making a csv import to db","title":"Creating Components"},{"location":"getting-started/creating-components.html#creating-components","text":"@todo - extended guide on making a Pandas example @todo - extended guide on making a csv exporter to ftp @todo - extended guide on making a csv import to db","title":"Creating Components"},{"location":"getting-started/creating-hooks.html","text":"Hooks - Connection interfaces Typhoon hooks represent the same thing as airlflow hooks , so if you are familiar it's an easy concept. They are the interface to external platforms and databases. Why create a new Hook? You may want to access data sources or APIs that don't exist already in any of: Core or Community contributed Typhoon Hooks File system, AWS, Snowflake SQLAlchemy / Python DB API 2.0 allowing hundreds of DBs without much custom code. Singer taps . You can use any singer taps in your DAG as a task. To understand how to add a connection using an existing hook see Adding connections . Creating New Hooks If you need to make a new one its relatively straightforward. Let's make an example to get data from elasticsearch. Installing Es and Kibana & adding sample data Follow along here Docker and Elastic . You need Docker Desktop. There is an option to add data when you load Kibana browser UI. Creating the Hook interface You can publish new Hooks to the community in their own repo. Here we will just make one locally. First we will make go to the project folder of our typhoon $TYPHOON_HOME and create a file called elastic.py in the folder hooks (under the root folder). This will establish the interface. In this example we will ignore the security for brevity, but you can add login and passwords or secret keys easily in the same way. All Hooks extend the HookInterface and have 3 functions: - __init__ - __enter__ - usually the only part to change to address teh specific connection of the DB. - __exit__ So in __enter__ here we are adding our Elasticsearch package and setting up the paramaters from our Connections.yml . from typhoon.contrib.hooks.hook_interface import HookInterface from typhoon.connections import ConnectionParams class ElasticsearchHook ( HookInterface ): conn_type = 'elasticsearch' def __init__ ( self , conn_params : ConnectionParams , conn_type : str = 'client' ): self . conn_params = conn_params self . conn_type = conn_type def __enter__ ( self ): from elasticsearch import Elasticsearch conn_params = self . conn_params credentials = { 'host' : conn_params . host , 'port' : conn_params . port } self . es = Elasticsearch ([ credentials ]) return self . es def __exit__ ( self , exc_type , exc_val , exc_tb ): self . es = None Creating hook functions In this case we will only make the search function. You can of course add whatever you need, even combining other libraries (e.g. returning a Pandas Dataframe). In the functions folder (from root) of your project add elastic_functions.py . As you can see, this is very straightforward where we are just wrapping the hook and the elasticsearch search function. from elasticsearch import Elasticsearch from typing import Optional , List from hooks.elastic import ElasticsearchHook # .search(index=\"sw\", body={\"query\": {\"match\": {'name':'Darth Vader'}}}) def search ( hook : ElasticsearchHook , index : str , body : Optional [ str ] = None , ): with hook as es : return es . search ( index = index , body = body ) Adding to Connection.yml To your Connections.yml add: elastic_cluster : local : conn_type : elasticsearch host : localhost port : 9200 Note, you can add login and password and other extra attributes to the Hook and then add them here also. In your console use the typhoon cli to add teh connection in the normal way typhoon status typhoon connection add --conn-id elastic_cluster --conn-env local typhoon status - to check its now resolved A test DAG Finally, test this against your test data with a DAG called elastic_query.yml in the dags folder. name : elastic_query schedule_interval : rate(10 minutes) tasks : search_elastic : function : functions.elastic_functions.search args : hook : !Hook elastic_cluster body : '{ \"query\": { \"match_all\": { } } }' index : kibana_sample_data_flights echo_result : input : search_elastic function : typhoon.debug.echo args : data : !Py $BATCH Then run with typhoon dag run --dag-name elastic_query in your project home with the Typhoon cli. You should see the data echo to the console ready to use in a pipeline.","title":"New Hooks (connections)"},{"location":"getting-started/creating-hooks.html#hooks-connection-interfaces","text":"Typhoon hooks represent the same thing as airlflow hooks , so if you are familiar it's an easy concept. They are the interface to external platforms and databases.","title":"Hooks - Connection interfaces"},{"location":"getting-started/creating-hooks.html#why-create-a-new-hook","text":"You may want to access data sources or APIs that don't exist already in any of: Core or Community contributed Typhoon Hooks File system, AWS, Snowflake SQLAlchemy / Python DB API 2.0 allowing hundreds of DBs without much custom code. Singer taps . You can use any singer taps in your DAG as a task. To understand how to add a connection using an existing hook see Adding connections .","title":"Why create a new Hook?"},{"location":"getting-started/creating-hooks.html#creating-new-hooks","text":"If you need to make a new one its relatively straightforward. Let's make an example to get data from elasticsearch.","title":"Creating New Hooks"},{"location":"getting-started/creating-hooks.html#installing-es-and-kibana-adding-sample-data","text":"Follow along here Docker and Elastic . You need Docker Desktop. There is an option to add data when you load Kibana browser UI.","title":"Installing Es and Kibana &amp; adding sample data"},{"location":"getting-started/creating-hooks.html#creating-the-hook-interface","text":"You can publish new Hooks to the community in their own repo. Here we will just make one locally. First we will make go to the project folder of our typhoon $TYPHOON_HOME and create a file called elastic.py in the folder hooks (under the root folder). This will establish the interface. In this example we will ignore the security for brevity, but you can add login and passwords or secret keys easily in the same way. All Hooks extend the HookInterface and have 3 functions: - __init__ - __enter__ - usually the only part to change to address teh specific connection of the DB. - __exit__ So in __enter__ here we are adding our Elasticsearch package and setting up the paramaters from our Connections.yml . from typhoon.contrib.hooks.hook_interface import HookInterface from typhoon.connections import ConnectionParams class ElasticsearchHook ( HookInterface ): conn_type = 'elasticsearch' def __init__ ( self , conn_params : ConnectionParams , conn_type : str = 'client' ): self . conn_params = conn_params self . conn_type = conn_type def __enter__ ( self ): from elasticsearch import Elasticsearch conn_params = self . conn_params credentials = { 'host' : conn_params . host , 'port' : conn_params . port } self . es = Elasticsearch ([ credentials ]) return self . es def __exit__ ( self , exc_type , exc_val , exc_tb ): self . es = None","title":"Creating the Hook interface"},{"location":"getting-started/creating-hooks.html#creating-hook-functions","text":"In this case we will only make the search function. You can of course add whatever you need, even combining other libraries (e.g. returning a Pandas Dataframe). In the functions folder (from root) of your project add elastic_functions.py . As you can see, this is very straightforward where we are just wrapping the hook and the elasticsearch search function. from elasticsearch import Elasticsearch from typing import Optional , List from hooks.elastic import ElasticsearchHook # .search(index=\"sw\", body={\"query\": {\"match\": {'name':'Darth Vader'}}}) def search ( hook : ElasticsearchHook , index : str , body : Optional [ str ] = None , ): with hook as es : return es . search ( index = index , body = body )","title":"Creating hook functions"},{"location":"getting-started/creating-hooks.html#adding-to-connectionyml","text":"To your Connections.yml add: elastic_cluster : local : conn_type : elasticsearch host : localhost port : 9200 Note, you can add login and password and other extra attributes to the Hook and then add them here also. In your console use the typhoon cli to add teh connection in the normal way typhoon status typhoon connection add --conn-id elastic_cluster --conn-env local typhoon status - to check its now resolved","title":"Adding to Connection.yml"},{"location":"getting-started/creating-hooks.html#a-test-dag","text":"Finally, test this against your test data with a DAG called elastic_query.yml in the dags folder. name : elastic_query schedule_interval : rate(10 minutes) tasks : search_elastic : function : functions.elastic_functions.search args : hook : !Hook elastic_cluster body : '{ \"query\": { \"match_all\": { } } }' index : kibana_sample_data_flights echo_result : input : search_elastic function : typhoon.debug.echo args : data : !Py $BATCH Then run with typhoon dag run --dag-name elastic_query in your project home with the Typhoon cli. You should see the data echo to the console ready to use in a pipeline.","title":"A test DAG"},{"location":"getting-started/deployment.html","text":"Deployment airflow docker how to start testing - how to productionise it","title":"Deployment"},{"location":"getting-started/deployment.html#deployment","text":"airflow docker how to start testing - how to productionise it","title":"Deployment"},{"location":"getting-started/installation.html","text":"Installation Typhoon can be installed locally with pip or using docker. To test airflow (especially on Windows) we recommend using the docker version. Use the DEV version when installing it locally. The [dev] version comes with all the libraries and tools that make development easier. ` pip install typhoon - orchestrator [ dev ] ` The production version is lightweight for use with Lambda. Hello World - 5 min walkthrough After installation we recommend following Hello World example. You can use the rest of 'Getting Started' section here for details in each step should you need it. with pip Optionally - Install and activate virtualenv python3 -m venv typhoon_venv source ./typhoon_venv/bin/activate Install typhoon: pip install typhoon-orchestrator [ dev ] Creating your new project Inside your terminal navigate to where you want to create your new project directory. Then run: typhoon init hello_world cd hello_world This will create a directory named hello_world that serves as an example project. As in git, when we cd into the directory it will detect that it's a Typhoon project and consider that directory the base directory for Typhoon (TYPHOON_HOME). Checking 'typhoon status' typhoon status Result: We can see that it's detecting the project home as well as a SQLite metadata database that just got created. It's also warning us that our DAG uses a connection that is not defined in the metadata database and suggesting us a fix. We will see in the next section 'Connections' how to add these. Bash/ZSH/Fish auto-complete bash eval \"$(_TYPHOON_COMPLETE=source_bash typhoon)\" zsh eval \"$(_TYPHOON_COMPLETE=source_zsh typhoon)\" fish eval \"$(_TYPHOON_COMPLETE=source_fish typhoon)\" With Docker and Airflow To deploy Typhoon with Airflow you need: Docker / Docker Desktop (You must use WSL2 on Windows) Download the docker-compose.yaml (or use curl below) Create a directory for your TYPHOON_PROJECTS_HOME The following sets up your project directory and gets the docker-compose.yml: TYPHOON_PROJECTS_HOME = \"/tmp/typhoon_projects\" # Or any other path you prefer mkdir -p $TYPHOON_PROJECTS_HOME /typhoon_airflow_test cd $TYPHOON_PROJECTS_HOME /typhoon_airflow_test mkdir src curl -LfO 'https://github.com/typhoon-data-org/typhoon-orchestrator/blob/master/docker-compose-af.yml' docker compose up -d airflow-init # initiates airflow docker exec -it typhoon-af bash # Then you're in the typhoon home. airflow initdb # !! To initiate Airflow DB !! typhoon status # To see status of dags & connections You should be able to then check typhoon status and also the airlfow UI at http://localhost:8088 This runs a container with only 1 service, typhoon-af . This has both Airflow and Typhoon installed on it ready to work with. Directories Some directories are mounted which synchronizes files between your computer and the container. ./data_lake - for landing files (as a local dev environment) ./airflow_dags - where typhoon compiles your Airflow DAGs to ./src - Your Typhoon project ./src/dags - Where you develop your Typhoon YAML DAGs Development hints So you should develop your dags in ./src/dags using your local editor (not within the container - the files sync). You then access the docker container tyhpoon-af to use typhoon docker exec -it typhoon-af bash Inside the container bash: typhoon status typhoon dag build --all If they successfully compile they will appear in ./airflow_dags, and also in the Airflow UI. You may also need to restart the Airflow Container to see this list update. Next steps: add connections shown in typhoon status before running - Hello World - 5 min walkthrough After installation we recommend following Hello World example. This shows a ste-by-step of adding the connections, building and running our Hello World DAG. typhoon connection add --conn-id data_lake --conn-env local typhoon connection add --conn-id echo --conn-env local typhoon dag build --all","title":"Installation"},{"location":"getting-started/installation.html#installation","text":"Typhoon can be installed locally with pip or using docker. To test airflow (especially on Windows) we recommend using the docker version. Use the DEV version when installing it locally. The [dev] version comes with all the libraries and tools that make development easier. ` pip install typhoon - orchestrator [ dev ] ` The production version is lightweight for use with Lambda. Hello World - 5 min walkthrough After installation we recommend following Hello World example. You can use the rest of 'Getting Started' section here for details in each step should you need it.","title":"Installation"},{"location":"getting-started/installation.html#with-pip","text":"Optionally - Install and activate virtualenv python3 -m venv typhoon_venv source ./typhoon_venv/bin/activate Install typhoon: pip install typhoon-orchestrator [ dev ]","title":"with pip"},{"location":"getting-started/installation.html#creating-your-new-project","text":"Inside your terminal navigate to where you want to create your new project directory. Then run: typhoon init hello_world cd hello_world This will create a directory named hello_world that serves as an example project. As in git, when we cd into the directory it will detect that it's a Typhoon project and consider that directory the base directory for Typhoon (TYPHOON_HOME).","title":"Creating your new project"},{"location":"getting-started/installation.html#checking-typhoon-status","text":"typhoon status Result: We can see that it's detecting the project home as well as a SQLite metadata database that just got created. It's also warning us that our DAG uses a connection that is not defined in the metadata database and suggesting us a fix. We will see in the next section 'Connections' how to add these. Bash/ZSH/Fish auto-complete bash eval \"$(_TYPHOON_COMPLETE=source_bash typhoon)\" zsh eval \"$(_TYPHOON_COMPLETE=source_zsh typhoon)\" fish eval \"$(_TYPHOON_COMPLETE=source_fish typhoon)\"","title":"Checking 'typhoon status'"},{"location":"getting-started/installation.html#with-docker-and-airflow","text":"To deploy Typhoon with Airflow you need: Docker / Docker Desktop (You must use WSL2 on Windows) Download the docker-compose.yaml (or use curl below) Create a directory for your TYPHOON_PROJECTS_HOME The following sets up your project directory and gets the docker-compose.yml: TYPHOON_PROJECTS_HOME = \"/tmp/typhoon_projects\" # Or any other path you prefer mkdir -p $TYPHOON_PROJECTS_HOME /typhoon_airflow_test cd $TYPHOON_PROJECTS_HOME /typhoon_airflow_test mkdir src curl -LfO 'https://github.com/typhoon-data-org/typhoon-orchestrator/blob/master/docker-compose-af.yml' docker compose up -d airflow-init # initiates airflow docker exec -it typhoon-af bash # Then you're in the typhoon home. airflow initdb # !! To initiate Airflow DB !! typhoon status # To see status of dags & connections You should be able to then check typhoon status and also the airlfow UI at http://localhost:8088 This runs a container with only 1 service, typhoon-af . This has both Airflow and Typhoon installed on it ready to work with.","title":"With Docker and Airflow"},{"location":"getting-started/installation.html#directories","text":"Some directories are mounted which synchronizes files between your computer and the container. ./data_lake - for landing files (as a local dev environment) ./airflow_dags - where typhoon compiles your Airflow DAGs to ./src - Your Typhoon project ./src/dags - Where you develop your Typhoon YAML DAGs","title":"Directories"},{"location":"getting-started/installation.html#development-hints","text":"So you should develop your dags in ./src/dags using your local editor (not within the container - the files sync). You then access the docker container tyhpoon-af to use typhoon docker exec -it typhoon-af bash Inside the container bash: typhoon status typhoon dag build --all If they successfully compile they will appear in ./airflow_dags, and also in the Airflow UI. You may also need to restart the Airflow Container to see this list update. Next steps: add connections shown in typhoon status before running - Hello World - 5 min walkthrough After installation we recommend following Hello World example. This shows a ste-by-step of adding the connections, building and running our Hello World DAG. typhoon connection add --conn-id data_lake --conn-env local typhoon connection add --conn-id echo --conn-env local typhoon dag build --all","title":"Development hints"},{"location":"getting-started/typhoon-cli.html","text":"Typhoon cli First usage & help Inspired by other great command line interfaces, it will be instantly familiar to *nix and git users. Intelligent bash/zsh completion. typhoon init test_project typhoon status typhoon dag ls -l typhoon dag push test --dag-name example_dag Typing typhoon will show the list of options connection Manage Typhoon connections dag Manage Typhoon DAGs extension Manage Typhoon extensions init Create a new Typhoon project metadata Manage Typhoon metadata remote Manage Typhoon remotes status Information on project status variable Manage Typhoon variables You can use --help at each point, for example `typhoon connection --help will present: Options: --help Show this message and exit. Commands: add Add connection to the metadata store definition Connection definition in connections.yml ls List connections in the metadata store rm Remove connection from the metadata store Key cli usage: Starting a new project : typhoon init new_project (in your desired directory path) Checking status : typhoon status [ENV] This tells you information about the status of your project. Run typhoon status and it will find a typhoon.cfg file in the current directory. It is assumed that the typhoon project root is the directory that contains the typhoon.cfg. If you want to override that set the environment variable TYPHOON_HOME to the full path of the directory. Add a connection : e.g. typhoon connection add --conn-id data_lake --conn-env local Remember to set up your connections in connections.yml before you add them (data_lake is a default example). Build DAGs : typhoon build-dags . This will create the folder out/ in your Typhoon Home directory and also output to Airflow deployment if configured. Run a DAG : typhoon run --dag-name hello_world","title":"Typhoon Cli"},{"location":"getting-started/typhoon-cli.html#typhoon-cli","text":"","title":"Typhoon cli"},{"location":"getting-started/typhoon-cli.html#first-usage-help","text":"Inspired by other great command line interfaces, it will be instantly familiar to *nix and git users. Intelligent bash/zsh completion. typhoon init test_project typhoon status typhoon dag ls -l typhoon dag push test --dag-name example_dag Typing typhoon will show the list of options connection Manage Typhoon connections dag Manage Typhoon DAGs extension Manage Typhoon extensions init Create a new Typhoon project metadata Manage Typhoon metadata remote Manage Typhoon remotes status Information on project status variable Manage Typhoon variables You can use --help at each point, for example `typhoon connection --help will present: Options: --help Show this message and exit. Commands: add Add connection to the metadata store definition Connection definition in connections.yml ls List connections in the metadata store rm Remove connection from the metadata store","title":"First usage &amp; help"},{"location":"getting-started/typhoon-cli.html#key-cli-usage","text":"Starting a new project : typhoon init new_project (in your desired directory path) Checking status : typhoon status [ENV] This tells you information about the status of your project. Run typhoon status and it will find a typhoon.cfg file in the current directory. It is assumed that the typhoon project root is the directory that contains the typhoon.cfg. If you want to override that set the environment variable TYPHOON_HOME to the full path of the directory. Add a connection : e.g. typhoon connection add --conn-id data_lake --conn-env local Remember to set up your connections in connections.yml before you add them (data_lake is a default example). Build DAGs : typhoon build-dags . This will create the folder out/ in your Typhoon Home directory and also output to Airflow deployment if configured. Run a DAG : typhoon run --dag-name hello_world","title":"Key cli usage:"},{"location":"getting-started/typhoon-shell.html","text":"Typhoon Shell First usage & help To run the interactive Shell use typhoon shell . For example: typhoon shell --dag-name favorite_authors Note it must be run with the name of the DAG you want to use. Key Shell usage: Reload the DAG if you have altered it : %reload_dag Autocomplete list the Tasks : tasks. and tab to bring up tasks. Getting args of a task run tasks.get_author.get_args(dag_context, None, None, batch=\"test\") results in {'requested_author': 'test'} which is the arguement this task needs Running a task : run tasks.get_author.run(dag_context, None, None, \"J.K. Rowling\") results in returning the data from API. Inspecting the data : run tasks.get_author.broker.batches.keys() to get the batch key. run data = tasks.get_author.broker.batches['a4dd9048-4d0a-4b74-8b44-338608ddec47'] then you can review data . Note: dag_context object exists so you can use it to run tasks easily: tasks.get_author.run(dag_context, None, None, \"J.K. Rowling\")","title":"Typhoon Shell"},{"location":"getting-started/typhoon-shell.html#typhoon-shell","text":"","title":"Typhoon Shell"},{"location":"getting-started/typhoon-shell.html#first-usage-help","text":"To run the interactive Shell use typhoon shell . For example: typhoon shell --dag-name favorite_authors Note it must be run with the name of the DAG you want to use.","title":"First usage &amp; help"},{"location":"getting-started/typhoon-shell.html#key-shell-usage","text":"Reload the DAG if you have altered it : %reload_dag Autocomplete list the Tasks : tasks. and tab to bring up tasks. Getting args of a task run tasks.get_author.get_args(dag_context, None, None, batch=\"test\") results in {'requested_author': 'test'} which is the arguement this task needs Running a task : run tasks.get_author.run(dag_context, None, None, \"J.K. Rowling\") results in returning the data from API. Inspecting the data : run tasks.get_author.broker.batches.keys() to get the batch key. run data = tasks.get_author.broker.batches['a4dd9048-4d0a-4b74-8b44-338608ddec47'] then you can review data . Note: dag_context object exists so you can use it to run tasks easily: tasks.get_author.run(dag_context, None, None, \"J.K. Rowling\")","title":"Key Shell usage:"},{"location":"getting-started/using-component-ui.html","text":"Using the Component Builder UI Team members can self-serve complex DAGs quickly. Included components: Glob & Compress : To glob (e.g. '*.csv') files and compress them to gzip or zlib. FileSystem to DB : To read (glob) files on a schedule into an existing table. DB to FileSystem : To write a table to a file pattern. DB to Snowlfake : Multiple tables landed to snowflake ([idempotent][3] with added DHW staging metadata). Fully end-to-end production ready flow. Running the UI In your terminal use typhoon webserver Resulting in a message: You can now view your Streamlit app in your browser. Network URL: http://172.17.201.54:8501 External URL: http://31.13.188.150:8501 Using the UI The UI uses the Typhoon API to configure a DAG that uses a particular Component. This means the result of the UI use will be a normal DAG that has been built. You can then review and edit this DAG file as normal. Use the dynamic form (main screen) to enter the arguements (hooks etc) as you need. Note, you must add any connections to the connections.yml and add the connection to Typhoon using the CLI. Then you must click the 'Create Dag' button to generate the file and build it. The success notice includes the path of the yaml DAG file generated for minor editing, testing and use. Glob & Compress To glob (e.g. '*.csv') files and compress them to gzip or zlib. source_hook: FileSyestemHookInterface (i.e. local file or S3 etc) destination_hook: FileSyestemHookInterface pattern: str e.g. *.csv or a Jinja template that has been registered in Typhoon CLI destination_path: relative path to the hook root folder. compression: limited by menu to gzip | zlib. FileSystem to DB To read (glob) files on a schedule into an existing table. filesystem_hook: FileSystemHook pattern: str e.g. *.csv db_hook: SQLAlchemyHook table_name: str DB to FileSystem To write a single table to a file pattern. This is a whole table snapshot. db_hook: DBApiHook table_name: str batch_size: int filesystem_hook: FileSystemHook path_template: str - very likely to be '{table}_{file_num}.csv' or similar. create_intermediate_dirs: true | false DB to Snowlfake Multiple tables landed to snowflake ([idempotent][3] with added DHW staging metadata). Fully end-to-end production ready flow. source_schema_name: str i.e. the db schema source_hook: DbApiHook snowflake_hook: SnowflakeHook source_tables: List[str] - i.e. list all the tables in a Yaml string. quote_tables: true | false","title":"Component UI"},{"location":"getting-started/using-component-ui.html#using-the-component-builder-ui","text":"Team members can self-serve complex DAGs quickly.","title":"Using the Component Builder UI"},{"location":"getting-started/using-component-ui.html#included-components","text":"Glob & Compress : To glob (e.g. '*.csv') files and compress them to gzip or zlib. FileSystem to DB : To read (glob) files on a schedule into an existing table. DB to FileSystem : To write a table to a file pattern. DB to Snowlfake : Multiple tables landed to snowflake ([idempotent][3] with added DHW staging metadata). Fully end-to-end production ready flow.","title":"Included components:"},{"location":"getting-started/using-component-ui.html#running-the-ui","text":"In your terminal use typhoon webserver Resulting in a message: You can now view your Streamlit app in your browser. Network URL: http://172.17.201.54:8501 External URL: http://31.13.188.150:8501","title":"Running the UI"},{"location":"getting-started/using-component-ui.html#using-the-ui","text":"The UI uses the Typhoon API to configure a DAG that uses a particular Component. This means the result of the UI use will be a normal DAG that has been built. You can then review and edit this DAG file as normal. Use the dynamic form (main screen) to enter the arguements (hooks etc) as you need. Note, you must add any connections to the connections.yml and add the connection to Typhoon using the CLI. Then you must click the 'Create Dag' button to generate the file and build it. The success notice includes the path of the yaml DAG file generated for minor editing, testing and use.","title":"Using the UI"},{"location":"getting-started/using-component-ui.html#glob-compress","text":"To glob (e.g. '*.csv') files and compress them to gzip or zlib. source_hook: FileSyestemHookInterface (i.e. local file or S3 etc) destination_hook: FileSyestemHookInterface pattern: str e.g. *.csv or a Jinja template that has been registered in Typhoon CLI destination_path: relative path to the hook root folder. compression: limited by menu to gzip | zlib.","title":"Glob &amp; Compress"},{"location":"getting-started/using-component-ui.html#filesystem-to-db","text":"To read (glob) files on a schedule into an existing table. filesystem_hook: FileSystemHook pattern: str e.g. *.csv db_hook: SQLAlchemyHook table_name: str","title":"FileSystem to DB"},{"location":"getting-started/using-component-ui.html#db-to-filesystem","text":"To write a single table to a file pattern. This is a whole table snapshot. db_hook: DBApiHook table_name: str batch_size: int filesystem_hook: FileSystemHook path_template: str - very likely to be '{table}_{file_num}.csv' or similar. create_intermediate_dirs: true | false","title":"DB to FileSystem"},{"location":"getting-started/using-component-ui.html#db-to-snowlfake","text":"Multiple tables landed to snowflake ([idempotent][3] with added DHW staging metadata). Fully end-to-end production ready flow. source_schema_name: str i.e. the db schema source_hook: DbApiHook snowflake_hook: SnowflakeHook source_tables: List[str] - i.e. list all the tables in a Yaml string. quote_tables: true | false","title":"DB to Snowlfake"},{"location":"getting-started/using-components.html","text":"Using Components What is a component? Components are ways of packaging sets of regularly used tasks. This encourages modularity and re-use. This page focuses on using some packaged example components (rather than how to construct one ). Included components: Glob & Compress : To glob (e.g. '*.csv') files and compress them to gzip or zlib. FileSystem to DB : To read (glob) files on a schedule into an existing table. DB to FileSystem : To write a table to a file pattern. DB to Snowlfake : Multiple tables landed to snowflake ( idempotent with added DHW staging metadata). Fully end-to-end production ready flow. Example of using the Component UI. Team members can self-serve complex DAGs quickly. See the next section for using the Component UI . Example: Archive on Mondays Trivial example to choose a different path based on if its a Monday. $DAG_CONTEXT.ts is a datetime object representing the timestamp of runtime in the context. Calling component: typhoon.if imports the template structure defined in the component YAML. Your constributed components would be referenced component: components.my_component . This if component defines two outputs. These are accessed in the line choose_preprocessing.then and choose_preprocessing.else . name : conditional_process schedule_interval : rate(10 minutes) tasks : list_files : function : typhoon.filesystem.list_directory args : hook : !Hook ftp path : '/' choose_preprocessing : input : list_files component : typhoon.if args : data : !Py $BATCH condition : !Py $DAG_CONTEXT.ts.isoweekday() == 0 monday_processing_task : input : choose_preprocessing.then function : functions.my_process_monday args : data : !Py $BATCH otherday_processing_task : input : choose_preprocessing.else function : functions.my_process_other_day args : data : !Py $BATCH ... Example: Source DB to Snowflake DWH This is a non-trivial example of a full DWH load from multipler tables across 3 separate ERP systems. This calls the entire end to end flow that is packaged in the typhoon.db_to_snowflake flow. This component example is fully idempotent with added DHW staging metadata. Note even more productive is to have the table names as typhoon variables , keeping the YAML very clean and easy to read. We have kept this simple for the example by using a list. name : source_to_snowflake schedule_interval : rate(1 day) tasks : erps : function : typhoon.flow_control.branch args : branches : - name : sku_component_erp hook : !Hook ERP tables : - sku_list - sku_component - build_master - builds - component_extended_desc - name : transactions_erp hook : !Hook trans_ERP tables : - transactions - client_master - shipping - supplers - deliveries - factory_productivity - name : finance_erp hook : !Hook fin_erp tables : - payables - recievables - reconciliations - cost_to_serve get_erp : component : typhoon.db_to_snowflake args : source_name : !Py $BATCH['name'] source_hook : !Py $BATCH['hook'] snowflake_hook : !Hook data_warehouse quote_tables : true source_tables : !Py $BATCH['tables']","title":"Using Components"},{"location":"getting-started/using-components.html#using-components","text":"","title":"Using Components"},{"location":"getting-started/using-components.html#what-is-a-component","text":"Components are ways of packaging sets of regularly used tasks. This encourages modularity and re-use. This page focuses on using some packaged example components (rather than how to construct one ).","title":"What is a component?"},{"location":"getting-started/using-components.html#included-components","text":"Glob & Compress : To glob (e.g. '*.csv') files and compress them to gzip or zlib. FileSystem to DB : To read (glob) files on a schedule into an existing table. DB to FileSystem : To write a table to a file pattern. DB to Snowlfake : Multiple tables landed to snowflake ( idempotent with added DHW staging metadata). Fully end-to-end production ready flow. Example of using the Component UI. Team members can self-serve complex DAGs quickly. See the next section for using the Component UI .","title":"Included components:"},{"location":"getting-started/using-components.html#example-archive-on-mondays","text":"Trivial example to choose a different path based on if its a Monday. $DAG_CONTEXT.ts is a datetime object representing the timestamp of runtime in the context. Calling component: typhoon.if imports the template structure defined in the component YAML. Your constributed components would be referenced component: components.my_component . This if component defines two outputs. These are accessed in the line choose_preprocessing.then and choose_preprocessing.else . name : conditional_process schedule_interval : rate(10 minutes) tasks : list_files : function : typhoon.filesystem.list_directory args : hook : !Hook ftp path : '/' choose_preprocessing : input : list_files component : typhoon.if args : data : !Py $BATCH condition : !Py $DAG_CONTEXT.ts.isoweekday() == 0 monday_processing_task : input : choose_preprocessing.then function : functions.my_process_monday args : data : !Py $BATCH otherday_processing_task : input : choose_preprocessing.else function : functions.my_process_other_day args : data : !Py $BATCH ...","title":"Example: Archive on Mondays"},{"location":"getting-started/using-components.html#example-source-db-to-snowflake-dwh","text":"This is a non-trivial example of a full DWH load from multipler tables across 3 separate ERP systems. This calls the entire end to end flow that is packaged in the typhoon.db_to_snowflake flow. This component example is fully idempotent with added DHW staging metadata. Note even more productive is to have the table names as typhoon variables , keeping the YAML very clean and easy to read. We have kept this simple for the example by using a list. name : source_to_snowflake schedule_interval : rate(1 day) tasks : erps : function : typhoon.flow_control.branch args : branches : - name : sku_component_erp hook : !Hook ERP tables : - sku_list - sku_component - build_master - builds - component_extended_desc - name : transactions_erp hook : !Hook trans_ERP tables : - transactions - client_master - shipping - supplers - deliveries - factory_productivity - name : finance_erp hook : !Hook fin_erp tables : - payables - recievables - reconciliations - cost_to_serve get_erp : component : typhoon.db_to_snowflake args : source_name : !Py $BATCH['name'] source_hook : !Py $BATCH['hook'] snowflake_hook : !Hook data_warehouse quote_tables : true source_tables : !Py $BATCH['tables']","title":"Example: Source DB to Snowflake DWH"},{"location":"usage/components.html","text":"Components What is a component? Components are ways of packaging sets of regularly used tasks. This encourages modularity and re-use. This means a library of powerful workflows that are then simple dropping in 2-3 line YAML fragments. Components can be structured as complete workflows or to slot into larger ones: 'In-flow' using previous steps as inputs e.g. 'if', 'singer_multiplexer' 'Standalone' ones can be built as DAG full templates. The difference is this structure can use the Component Builder UI. e.g. 'db_to_snowflake' @TODO - screenshot of UI The rest of this page will focus on how to build a Component. See Using Components for examples on how to use existing components. Component structure As components are simply packaging a set of tasks they are simply a normal DAG but with some optional concepts: name: : Descriptive name for the component. args: : Any number of arguements that you need as inputs for teh parameter (e.g. table names, source hooks, target hooks etc.) $COMPONENT_INPUT : (Optional) If you are making a in-flow component, this is assigns that input output : (Optional) If the component results in output to downstream tasks (rather than writing to a target hook) this allows you to assign them. This example component shows the usage of all the above variables. name : if args : condition : Callable[[T], bool] data : T tasks : then : input : $COMPONENT_INPUT function : typhoon.flow_control.filter args : filter_func : !Py $ARG.condition data : !Py $ARG.data else : input : $COMPONENT_INPUT function : typhoon.flow_control.filter args : filter_func : !Py \"lambda x: not $ARG.condition(x)\" data : !Py $ARG.data output : - then - else","title":"Components"},{"location":"usage/components.html#components","text":"","title":"Components"},{"location":"usage/components.html#what-is-a-component","text":"Components are ways of packaging sets of regularly used tasks. This encourages modularity and re-use. This means a library of powerful workflows that are then simple dropping in 2-3 line YAML fragments. Components can be structured as complete workflows or to slot into larger ones: 'In-flow' using previous steps as inputs e.g. 'if', 'singer_multiplexer' 'Standalone' ones can be built as DAG full templates. The difference is this structure can use the Component Builder UI. e.g. 'db_to_snowflake' @TODO - screenshot of UI The rest of this page will focus on how to build a Component. See Using Components for examples on how to use existing components.","title":"What is a component?"},{"location":"usage/components.html#component-structure","text":"As components are simply packaging a set of tasks they are simply a normal DAG but with some optional concepts: name: : Descriptive name for the component. args: : Any number of arguements that you need as inputs for teh parameter (e.g. table names, source hooks, target hooks etc.) $COMPONENT_INPUT : (Optional) If you are making a in-flow component, this is assigns that input output : (Optional) If the component results in output to downstream tasks (rather than writing to a target hook) this allows you to assign them. This example component shows the usage of all the above variables. name : if args : condition : Callable[[T], bool] data : T tasks : then : input : $COMPONENT_INPUT function : typhoon.flow_control.filter args : filter_func : !Py $ARG.condition data : !Py $ARG.data else : input : $COMPONENT_INPUT function : typhoon.flow_control.filter args : filter_func : !Py \"lambda x: not $ARG.condition(x)\" data : !Py $ARG.data output : - then - else","title":"Component structure"},{"location":"usage/dag-context.html","text":"DAG Context, $Batch etc. Typhoon has some built in context variables: $BATCH: This is the data package passed to the next node. $BATCH_NUM: This is the iterator number if you are batching (queries for example). $HOOK: This represents your hook - you can use !Hook my_conn as syntax sugar or !Py $HOOK.myconn $VARIABLE: This is how you can access saved variables e.g. lists of tables, query templates etc. An example might be !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) $DAG_CONTEXT.interval_start & $DAG_CONTEXT.interval_end: Execution interval for example: $DAG_CONTEXT.interval_start \u2192 '2021-05-23 10:00:00' (inclusive) $DAG_CONTEXT.interval_end \u2192 '2021-05-23 11:00:00' (exclusive) write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True $BATCH This is the data package passed to the next node and can access the various features of the payload. For example if the data being passed is a JSON array: { \"filename\" : \"my_file.csv\" , \"contents\" : \"a, b, c, d ...etc\" } You can access these in the following way. In this example we are using the previous step's data for the path and the data step separately. write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py f'/store/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True $BATCH_NUM This is the iterator number if you are batching (queries for example). load : input : extract function : typhoon.filesystem.write_data args : hook : !Hook data_lake data : !Py typhoon.transformations.write_csv($BATCH.data) path : !Py f'{$BATCH.table_name}/part{$BATCH_NUM}.csv' $HOOK $HOOK represents your hook - you can use !Hook my_conn as syntax sugar or !Py $HOOK.myconn $VARIABLE This is how you can access saved variables e.g. lists of tables, query templates etc. To add a variable use the typhoon cli with typhoon variable add . Full list of options is available with typhoon variable --help . To call the variable, you can use !Var myvariable syntactic sugar or $VARIABLE.myvariable for example this DAG has 2 usages: - !Var sql_tables_list_1[\"my_db_tables\"] (getting the list of tables) - !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) (using a sql string .format() template) name : dwh_flow schedule_interval : rate(1 hours) tasks : list_tables : function : typhoon.flow_control.branch args : branches : !Var sql_tables_list_1[\"my_db_tables\"] extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Py $HOOK.transaction_db #hook: !Py $HOOK.echo batch_size : 10 query_params : table_name : \"tablen\" metadata : table_name : !Py $BATCH query : !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) debug_1 : input : extract_tables function : functions.debug.my_echo args : x : !Py $BATCH $DAG_CONTEXT interval_start - execution interval (e.g.'2021-05-23 10:00:00' inclusive) interval_end - execution interval (e.g.'2021-05-23 10:00:00' exclusive) ...","title":"Dag Context"},{"location":"usage/dag-context.html#dag-context-batch-etc","text":"Typhoon has some built in context variables: $BATCH: This is the data package passed to the next node. $BATCH_NUM: This is the iterator number if you are batching (queries for example). $HOOK: This represents your hook - you can use !Hook my_conn as syntax sugar or !Py $HOOK.myconn $VARIABLE: This is how you can access saved variables e.g. lists of tables, query templates etc. An example might be !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) $DAG_CONTEXT.interval_start & $DAG_CONTEXT.interval_end: Execution interval for example: $DAG_CONTEXT.interval_start \u2192 '2021-05-23 10:00:00' (inclusive) $DAG_CONTEXT.interval_end \u2192 '2021-05-23 11:00:00' (exclusive) write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True","title":"DAG Context, $Batch etc."},{"location":"usage/dag-context.html#batch","text":"This is the data package passed to the next node and can access the various features of the payload. For example if the data being passed is a JSON array: { \"filename\" : \"my_file.csv\" , \"contents\" : \"a, b, c, d ...etc\" } You can access these in the following way. In this example we are using the previous step's data for the path and the data step separately. write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py f'/store/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True","title":"$BATCH"},{"location":"usage/dag-context.html#batch_num","text":"This is the iterator number if you are batching (queries for example). load : input : extract function : typhoon.filesystem.write_data args : hook : !Hook data_lake data : !Py typhoon.transformations.write_csv($BATCH.data) path : !Py f'{$BATCH.table_name}/part{$BATCH_NUM}.csv'","title":"$BATCH_NUM"},{"location":"usage/dag-context.html#hook","text":"$HOOK represents your hook - you can use !Hook my_conn as syntax sugar or !Py $HOOK.myconn","title":"$HOOK"},{"location":"usage/dag-context.html#variable","text":"This is how you can access saved variables e.g. lists of tables, query templates etc. To add a variable use the typhoon cli with typhoon variable add . Full list of options is available with typhoon variable --help . To call the variable, you can use !Var myvariable syntactic sugar or $VARIABLE.myvariable for example this DAG has 2 usages: - !Var sql_tables_list_1[\"my_db_tables\"] (getting the list of tables) - !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) (using a sql string .format() template) name : dwh_flow schedule_interval : rate(1 hours) tasks : list_tables : function : typhoon.flow_control.branch args : branches : !Var sql_tables_list_1[\"my_db_tables\"] extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Py $HOOK.transaction_db #hook: !Py $HOOK.echo batch_size : 10 query_params : table_name : \"tablen\" metadata : table_name : !Py $BATCH query : !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) debug_1 : input : extract_tables function : functions.debug.my_echo args : x : !Py $BATCH","title":"$VARIABLE"},{"location":"usage/dag-context.html#dag_context","text":"interval_start - execution interval (e.g.'2021-05-23 10:00:00' inclusive) interval_end - execution interval (e.g.'2021-05-23 10:00:00' exclusive) ...","title":"$DAG_CONTEXT"},{"location":"usage/functions.html","text":"Functions Functions are the key building blocks of DAG tasks. Each task has to have a function. Usually functions act on Hooks to extract or load data. In the example below we are using a custom function exchange_rates_api.get_history that would in the file exchange_rates_api.py in the functions directory. In this file you would put any function that your custom ExchangeRate Hook might need. typhoon.function_file.function_name for built-in / contrib functions. functions.my_function_file.function_name for accessing your project functions. The two main task keys that interact with the function are the input (if required) and the args . These match the input paramater names of the function. So if you are unsure of how to make the task you can always check the function code. For example the write_data function has the following parameters that are matched in the YAML DAG below it. Optional ones are omitted. def write_data ( data : Union [ str , bytes , BytesIO ], hook : FileSystemHookInterface , path : Union [ Path , str ], create_intermediate_dirs : bool = False , metadata : Optional [ dict ] = None , return_path_format : str = 'relative' , ) -> Iterable [ str ]: Example built-in / contrib. function write_csv : input : exchange_rate function : typhoon.filesystem.write_data args : hook : !Hook echo data : !MultiStep - !Py transformations.xr.flatten_response($BATCH) - !Py typhoon.data.dicts_to_csv($1, delimiter='|') path : !Py f'{$DAG_CONTEXT.interval_start.isoformat()}_xr_data.csv' Example project function exchange_rate : function : functions.exchange_rates_api.get_history args : start_at : !Py $DAG_CONTEXT.interval_start end_at : !Py $DAG_CONTEXT.interval_end","title":"Functions"},{"location":"usage/functions.html#functions","text":"Functions are the key building blocks of DAG tasks. Each task has to have a function. Usually functions act on Hooks to extract or load data. In the example below we are using a custom function exchange_rates_api.get_history that would in the file exchange_rates_api.py in the functions directory. In this file you would put any function that your custom ExchangeRate Hook might need. typhoon.function_file.function_name for built-in / contrib functions. functions.my_function_file.function_name for accessing your project functions. The two main task keys that interact with the function are the input (if required) and the args . These match the input paramater names of the function. So if you are unsure of how to make the task you can always check the function code. For example the write_data function has the following parameters that are matched in the YAML DAG below it. Optional ones are omitted. def write_data ( data : Union [ str , bytes , BytesIO ], hook : FileSystemHookInterface , path : Union [ Path , str ], create_intermediate_dirs : bool = False , metadata : Optional [ dict ] = None , return_path_format : str = 'relative' , ) -> Iterable [ str ]:","title":"Functions"},{"location":"usage/functions.html#example-built-in-contrib-function","text":"write_csv : input : exchange_rate function : typhoon.filesystem.write_data args : hook : !Hook echo data : !MultiStep - !Py transformations.xr.flatten_response($BATCH) - !Py typhoon.data.dicts_to_csv($1, delimiter='|') path : !Py f'{$DAG_CONTEXT.interval_start.isoformat()}_xr_data.csv'","title":"Example built-in / contrib. function"},{"location":"usage/functions.html#example-project-function","text":"exchange_rate : function : functions.exchange_rates_api.get_history args : start_at : !Py $DAG_CONTEXT.interval_start end_at : !Py $DAG_CONTEXT.interval_end","title":"Example project function"},{"location":"usage/hooks.html","text":"Hooks Hook Interface & Parameters The minimum you must specify for all conecctions in your connections.yml file is the conn_type : conn_type : echo Most Hooks have parameters as well. The default ones which are common in the DB hooks: conn : conn_type : mysql login : my_login password : my_pass host : localhost port : 3306 In addition, many have extra parameters which you can list as follows: data_lake : test : conn_type : s3 extra : bucket : my-typhoon-test-bucket Existing Hooks Debug EchoHook - This simply echo's to the prompt what would be written to the file system. File system LocalStorageHook Local file system / disk. Wrapper around PyFileSystem FTPHook S3Hook GCSHook example_file_system_hooks : local_storage : conn_type : local_storage extra : base_path : /tmp/data_lake/ create : True s3_hook : conn_type : s3 extra : bucket : my-typhoon-test-bucket Http Basic Authentication (URL) AWS hooks AwsSessionHook DynamoDbHook DB API 2.0 Hooks and functions related to databases compatible with the Python Database API Specification v2.0 . You need to install the Typhoon DbAPI contrib package with: pip install typhoon-dbapi EchoDb (default install) SQLite (default install) Snowflake: pip install typhoon-dbapi[snowflake] Big Query: pip install typhoon-dbapi[bigquery] Postgres: pip install typhoon-dbapi[postgres] DuckDB: pip install typhoon-dbapi[duckdb] MySQL: pip install typhoon-dbapi[mysql] Any other DB that complies with this DBAPI 2.0 is very simple to add with minor customisation. Should cover vast majority of relational DBs. SQL Alchemy SQL Alchemy supports anything with DB API 2.0. Singer Singer taps can be used in tasks expanding the range of sources enourmously. Kafka KafkaConsumerHook KafkaProducerHook ElasticSearch See tutorial for quickly creating your own in less than 10 mins.","title":"Hooks"},{"location":"usage/hooks.html#hooks","text":"","title":"Hooks"},{"location":"usage/hooks.html#hook-interface-parameters","text":"The minimum you must specify for all conecctions in your connections.yml file is the conn_type : conn_type : echo Most Hooks have parameters as well. The default ones which are common in the DB hooks: conn : conn_type : mysql login : my_login password : my_pass host : localhost port : 3306 In addition, many have extra parameters which you can list as follows: data_lake : test : conn_type : s3 extra : bucket : my-typhoon-test-bucket","title":"Hook Interface &amp; Parameters"},{"location":"usage/hooks.html#existing-hooks","text":"","title":"Existing Hooks"},{"location":"usage/hooks.html#debug","text":"EchoHook - This simply echo's to the prompt what would be written to the file system.","title":"Debug"},{"location":"usage/hooks.html#file-system","text":"","title":"File system"},{"location":"usage/hooks.html#localstoragehook","text":"Local file system / disk. Wrapper around PyFileSystem FTPHook S3Hook GCSHook example_file_system_hooks : local_storage : conn_type : local_storage extra : base_path : /tmp/data_lake/ create : True s3_hook : conn_type : s3 extra : bucket : my-typhoon-test-bucket","title":"LocalStorageHook"},{"location":"usage/hooks.html#http","text":"Basic Authentication (URL)","title":"Http"},{"location":"usage/hooks.html#aws-hooks","text":"AwsSessionHook DynamoDbHook","title":"AWS hooks"},{"location":"usage/hooks.html#db-api-20","text":"Hooks and functions related to databases compatible with the Python Database API Specification v2.0 . You need to install the Typhoon DbAPI contrib package with: pip install typhoon-dbapi EchoDb (default install) SQLite (default install) Snowflake: pip install typhoon-dbapi[snowflake] Big Query: pip install typhoon-dbapi[bigquery] Postgres: pip install typhoon-dbapi[postgres] DuckDB: pip install typhoon-dbapi[duckdb] MySQL: pip install typhoon-dbapi[mysql] Any other DB that complies with this DBAPI 2.0 is very simple to add with minor customisation. Should cover vast majority of relational DBs.","title":"DB API 2.0"},{"location":"usage/hooks.html#sql-alchemy","text":"SQL Alchemy supports anything with DB API 2.0.","title":"SQL Alchemy"},{"location":"usage/hooks.html#singer","text":"Singer taps can be used in tasks expanding the range of sources enourmously.","title":"Singer"},{"location":"usage/hooks.html#kafka","text":"KafkaConsumerHook KafkaProducerHook","title":"Kafka"},{"location":"usage/hooks.html#elasticsearch","text":"See tutorial for quickly creating your own in less than 10 mins.","title":"ElasticSearch"},{"location":"usage/py-multistep.html","text":"Inline python Typhoon lets you run inline pytnon either in one line !Py print('Normal python here') or in multiple lines with !Multistep . !Py This lets you evaluate any python quickly, for example to use pandas or a transformation function. For example here we are using it to pass the first element of the $BATCH and also to use teh path_mapping regex transformation. write_df_to_sql : input : read_csv_to_df function : functions.pandas_data_helpers.df_write args : hook : !Hook transaction_db df : !Py $BATCH[0] table_name : !Py transformations.path_mapping.to_regex_name($BATCH[1]) !Multistep Sometimes you want to put a few lines in the DAG without making a function for it. You can create powerfull transformations in series in a nice readable way. In this example we are evaluating each !Py line in the YAML list and referencing the result of the 1st in the second with $1 (subsequent lines are $2 , $3 etc.). The result is flattening the response $BATCH data and then outputting as csv to the path that is prefixed with the start timestamp. write_csv : input : exchange_rate function : typhoon.filesystem.write_data args : hook : !Hook echo data : !MultiStep - !Py transformations.xr.flatten_response($BATCH) - !Py typhoon.data.dicts_to_csv($1, delimiter='|') path : !Py f'{$DAG_CONTEXT.interval_start.isoformat()}_xr_data.csv' Another example... write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True !Py $BATCH['filename'] will evaluate to \"users.txt' (for example). !Py $DAG_CONTEXT.interval_end will evaluate to the timestamp of the DAG run. This is a built in context variable. !Py f'/store/{$2}/{$1}' finally this references the first two lines (1, 2) and uses a normal Python f-string to make the path. Evaluating to '/store/ 2021-03-13T12:00:00/users.txt'","title":"!Py & !Multistep"},{"location":"usage/py-multistep.html#inline-python","text":"Typhoon lets you run inline pytnon either in one line !Py print('Normal python here') or in multiple lines with !Multistep .","title":"Inline python"},{"location":"usage/py-multistep.html#py","text":"This lets you evaluate any python quickly, for example to use pandas or a transformation function. For example here we are using it to pass the first element of the $BATCH and also to use teh path_mapping regex transformation. write_df_to_sql : input : read_csv_to_df function : functions.pandas_data_helpers.df_write args : hook : !Hook transaction_db df : !Py $BATCH[0] table_name : !Py transformations.path_mapping.to_regex_name($BATCH[1])","title":"!Py"},{"location":"usage/py-multistep.html#multistep","text":"Sometimes you want to put a few lines in the DAG without making a function for it. You can create powerfull transformations in series in a nice readable way. In this example we are evaluating each !Py line in the YAML list and referencing the result of the 1st in the second with $1 (subsequent lines are $2 , $3 etc.). The result is flattening the response $BATCH data and then outputting as csv to the path that is prefixed with the start timestamp. write_csv : input : exchange_rate function : typhoon.filesystem.write_data args : hook : !Hook echo data : !MultiStep - !Py transformations.xr.flatten_response($BATCH) - !Py typhoon.data.dicts_to_csv($1, delimiter='|') path : !Py f'{$DAG_CONTEXT.interval_start.isoformat()}_xr_data.csv' Another example... write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True !Py $BATCH['filename'] will evaluate to \"users.txt' (for example). !Py $DAG_CONTEXT.interval_end will evaluate to the timestamp of the DAG run. This is a built in context variable. !Py f'/store/{$2}/{$1}' finally this references the first two lines (1, 2) and uses a normal Python f-string to make the path. Evaluating to '/store/ 2021-03-13T12:00:00/users.txt'","title":"!Multistep"},{"location":"usage/task-flow-control.html","text":"Task flow control Basic usage of making of a trivial DAG of read_data -> write_data is expressed by naming the tasks and linking them with the input argument. name : write_to_csv schedule_interval : rate(1 hours) granularity : hour tasks : read_data : function : functions.filesystem.read_data args : hook : !Hook files path : /iris.data write_data : input : read_data function : typhoon.filesystem.write_data args : hook : !Hook files data : !Py $BATCH path : !Py f\"/out.csv\" However, we may also have more complex requirements of the logic flow. In this case the typhoon.flow_control functions are useful. - branch - to yield over each item in a sequence. - filter - to return only items that are True, skipping the rest Branching In some cases you need to make more advanced logic that iterates over many items (e.g. multiple tables in a DB to land in a DWH). Using the function typhoon.flow_control.branch we can yield each table from VARIABLES and run write_date (and all tasks downstream). name : read_many_tables schedule_interval : rate(1 hours) granularity : hour list_tables : function : typhoon.flow_control.branch args : branches : !Var sql_tables_list_1 write_data : input : list_tables function : typhoon.relational.execute_query args : hook : !Hook transaction_db batch_size : 10 metadata : table_name : !Py $BATCH query : !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) Filter We can easily apply a simple condition using typhoon.flow_control.filter . Items matching the condition will be passed to the next task while the rest skipped. This example shows how it can be used to filter those ending with .csv which are then passed to read them using Pandas and finally written to sql. You could easily make this into a Component which can use the UI. name : pickup_data_to_sql schedule_interval : rate(10 minutes) tasks : list_files : function : typhoon.filesystem.list_directory args : hook : !Hook ftp path : '/' filter_csv_files : input : list_files function : typhoon.flow_control.filter args : filter_func : !Py \"lambda x: x.endswith('.csv')\" data : !Py $BATCH read_csv_to_df : input : filter_csv_files function : functions.pandas_data_helpers.csv_to_df args : hook : !Py $HOOK.ftp path : !Py $BATCH write_df_to_sql : input : read_csv_to_df function : functions.pandas_data_helpers.df_write args : hook : !Py $HOOK.transaction_db df : !Py $BATCH[0] table_name : !Py transformations.path_mapping.to_regex_name($BATCH[1]) tests : read_csv_to_df : batch : '/file.csv' expected : path : '/file.csv' write_df_to_sql : batch : - !Py \"pd.DataFrame({'a': [1,2,3,4], 'b': [6,7,8,9]})\" - \"data_clients_2021-02-18T03_00_00.csv\" expected : df : !Py \"pd.DataFrame({'a': [1,2,3,4], 'b': [6,7,8,9]})\" table_name : \"clients\" If statements You can use the if component to do if-else logic: name : test_if schedule_interval : rate(1 hour) tasks : names : function : typhoon.flow_control.branch args : branches : - John - Martha - Mathew - Karla is_girl : input : names component : typhoon.if args : condition : !Py \"lambda name: name.endswith('a')\" data : !Py $BATCH print_girl : input : is_girl.then function : functions.debug.echo args : data : !Py f'{$BATCH} is a girl' print_boy : input : is_girl.else function : functions.debug.echo args : data : !Py f'{$BATCH} is a boy' The key parts above are the use of the typhoon.if component and that the tasks are linked using the condition.if and condition.else .","title":"Task flow control"},{"location":"usage/task-flow-control.html#task-flow-control","text":"Basic usage of making of a trivial DAG of read_data -> write_data is expressed by naming the tasks and linking them with the input argument. name : write_to_csv schedule_interval : rate(1 hours) granularity : hour tasks : read_data : function : functions.filesystem.read_data args : hook : !Hook files path : /iris.data write_data : input : read_data function : typhoon.filesystem.write_data args : hook : !Hook files data : !Py $BATCH path : !Py f\"/out.csv\" However, we may also have more complex requirements of the logic flow. In this case the typhoon.flow_control functions are useful. - branch - to yield over each item in a sequence. - filter - to return only items that are True, skipping the rest","title":"Task flow control"},{"location":"usage/task-flow-control.html#branching","text":"In some cases you need to make more advanced logic that iterates over many items (e.g. multiple tables in a DB to land in a DWH). Using the function typhoon.flow_control.branch we can yield each table from VARIABLES and run write_date (and all tasks downstream). name : read_many_tables schedule_interval : rate(1 hours) granularity : hour list_tables : function : typhoon.flow_control.branch args : branches : !Var sql_tables_list_1 write_data : input : list_tables function : typhoon.relational.execute_query args : hook : !Hook transaction_db batch_size : 10 metadata : table_name : !Py $BATCH query : !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH)","title":"Branching"},{"location":"usage/task-flow-control.html#filter","text":"We can easily apply a simple condition using typhoon.flow_control.filter . Items matching the condition will be passed to the next task while the rest skipped. This example shows how it can be used to filter those ending with .csv which are then passed to read them using Pandas and finally written to sql. You could easily make this into a Component which can use the UI. name : pickup_data_to_sql schedule_interval : rate(10 minutes) tasks : list_files : function : typhoon.filesystem.list_directory args : hook : !Hook ftp path : '/' filter_csv_files : input : list_files function : typhoon.flow_control.filter args : filter_func : !Py \"lambda x: x.endswith('.csv')\" data : !Py $BATCH read_csv_to_df : input : filter_csv_files function : functions.pandas_data_helpers.csv_to_df args : hook : !Py $HOOK.ftp path : !Py $BATCH write_df_to_sql : input : read_csv_to_df function : functions.pandas_data_helpers.df_write args : hook : !Py $HOOK.transaction_db df : !Py $BATCH[0] table_name : !Py transformations.path_mapping.to_regex_name($BATCH[1]) tests : read_csv_to_df : batch : '/file.csv' expected : path : '/file.csv' write_df_to_sql : batch : - !Py \"pd.DataFrame({'a': [1,2,3,4], 'b': [6,7,8,9]})\" - \"data_clients_2021-02-18T03_00_00.csv\" expected : df : !Py \"pd.DataFrame({'a': [1,2,3,4], 'b': [6,7,8,9]})\" table_name : \"clients\"","title":"Filter"},{"location":"usage/task-flow-control.html#if-statements","text":"You can use the if component to do if-else logic: name : test_if schedule_interval : rate(1 hour) tasks : names : function : typhoon.flow_control.branch args : branches : - John - Martha - Mathew - Karla is_girl : input : names component : typhoon.if args : condition : !Py \"lambda name: name.endswith('a')\" data : !Py $BATCH print_girl : input : is_girl.then function : functions.debug.echo args : data : !Py f'{$BATCH} is a girl' print_boy : input : is_girl.else function : functions.debug.echo args : data : !Py f'{$BATCH} is a boy' The key parts above are the use of the typhoon.if component and that the tasks are linked using the condition.if and condition.else .","title":"If statements"},{"location":"usage/task-testing.html","text":"Testing With Typhoon you can test your DAG tasks by including tests into the YAML. This is very helpful for development. It allows you to see how the task might react to different data structures. You can also write PyTest tests for you functions, transformations, and hooks in the normal way that you would for any other code. This is allows you to write robust flows. These are included in the tests folder, either as a unit or integration test. You can also debug and test the DAG compiled python code in the normal way. DAG Task Test You can add a set of tests (as many as you like) in the DAG at the bottom using the key tests and then the name of the task you are testing. You need to provide the input data into batch and the interval_start and interval_end . The test then runs using this input (i.e. not running data from preceding tasks) and compares the output to the expected key. In expected you need to assign the keys that the function outputs. @TODO BELOW - Fix after refactor and get a better examples write_csv : input : exchange_rate function : typhoon.filesystem.write_data args : hook : !Hook echo data : !MultiStep - !Py transformations.xr.flatten_response($BATCH) - !Py typhoon.data.dicts_to_csv($1, delimiter='|') path : !Py f'{$DAG_CONTEXT.interval_start.isoformat()}_xr_data.csv' tests : write_csv : batch : a : \"a_pay\" b : \"b_pay\" interval_start : 2021-02-03T01:01:01 interval_end : 2021-02-03T02:01:01 expected : data : '[{\"cola\": 1, \"colb\": 2, \"colc\": 3}, {\"cola\": 2, \"colb\": 4, \"colc\": 5}]' path : 'data_abc_batch_num_2_2021-02-03T01_01_01.json'","title":"Task testing"},{"location":"usage/task-testing.html#testing","text":"With Typhoon you can test your DAG tasks by including tests into the YAML. This is very helpful for development. It allows you to see how the task might react to different data structures. You can also write PyTest tests for you functions, transformations, and hooks in the normal way that you would for any other code. This is allows you to write robust flows. These are included in the tests folder, either as a unit or integration test. You can also debug and test the DAG compiled python code in the normal way.","title":"Testing"},{"location":"usage/task-testing.html#dag-task-test","text":"You can add a set of tests (as many as you like) in the DAG at the bottom using the key tests and then the name of the task you are testing. You need to provide the input data into batch and the interval_start and interval_end . The test then runs using this input (i.e. not running data from preceding tasks) and compares the output to the expected key. In expected you need to assign the keys that the function outputs.","title":"DAG Task Test"},{"location":"usage/task-testing.html#todo-below-fix-after-refactor-and-get-a-better-examples","text":"write_csv : input : exchange_rate function : typhoon.filesystem.write_data args : hook : !Hook echo data : !MultiStep - !Py transformations.xr.flatten_response($BATCH) - !Py typhoon.data.dicts_to_csv($1, delimiter='|') path : !Py f'{$DAG_CONTEXT.interval_start.isoformat()}_xr_data.csv' tests : write_csv : batch : a : \"a_pay\" b : \"b_pay\" interval_start : 2021-02-03T01:01:01 interval_end : 2021-02-03T02:01:01 expected : data : '[{\"cola\": 1, \"colb\": 2, \"colc\": 3}, {\"cola\": 2, \"colb\": 4, \"colc\": 5}]' path : 'data_abc_batch_num_2_2021-02-03T01_01_01.json'","title":"@TODO BELOW - Fix after refactor and get a better examples"},{"location":"usage/tasks.html","text":"","title":"Tasks"},{"location":"usage/templates.html","text":"Variables & Templates Variables let us define a value outside our DAG. In general it is a good idea not to hard code magic values into our code, and DAGs in Typhoon are no exception. This makes it easier to change or tune the value without requiring us to re-deploy our DAG. You can set simple Variables or use the same functionality to create Templates. These are useful for: variables for logic control boilerplate sql (using jinja or params) lists of tables or schemas (e.g. list of tables to run a dag on) Similar to Airflow's variables If you're familiar with Airflow you'll notice that it's the same concept as Airflow variables. As much as possible Typhoon tries to depart from tried and tested solutions only when it makes . Variables and connections are something that for the most part just works in Airflow, so we saw no reason to change it adding more cognitive burden on developers. Types of variables With that said it's worth pointing out the differences. In Typhoon variables have types associated with them. If a variable is of type JSON or YAML it will get loaded into a python object. If you need it to be loaded as a string prefer to choose the TEXT type. Options: ['string', 'jinja', 'number', 'json', 'yaml'] Quick usage To list, load or add variables in the Typhoon cli use: typhoon variable --help The id you assign or file name used is how you reference it in the DAG You can reference your variables with !Py $VARIABLE.my_variable or !Var myvariable (syntactic sugar) As you develop check for issues regularly using typhoon status List your variables using typhoon variable ls -l Built-ins Introducing our built in context variables: $BATCH: This is the data package passed to the next node. $BATCH_NUM: This is the iterator number if you are batching (queries for example). $HOOK: This represents your hook - you can use !Hook my_conn as syntax sugar or !Py $HOOK.myconn $VARIABLE: This is how you can access saved variables e.g. lists of tables, query templates etc. An example might be !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) $DAG_CONTEXT.interval_start & $DAG_CONTEXT.interval_end: Execution interval for example: $DAG_CONTEXT.interval_start \u2192 '2021-05-23 10:00:00' (inclusive) $DAG_CONTEXT.interval_end \u2192 '2021-05-23 11:00:00' (exclusive) Examples of setting custom a $VARIABLE Simple Variable In the typhoon cli use: typhoon variable add --var-id weekday_of_archive --var-type string --contents '1' You will get a response of Variable weekday_of_archive added . You can reference in a DAG using !Py $DAG_CONTEXT.interval_start.weekday() == $VARIABLE.weekday_of_archive . For example can be used to control if a process runs on a certain weekday with the if component. name : conditional_process schedule_interval : rate(10 minutes) tasks : list_tables : function : typhoon.flow_control.branch args : branches : - customers - transactions choose_preprocessing : input : list_tables component : typhoon.if args : data : !Py $BATCH condition : !Py \"lambda table: $DAG_CONTEXT.interval_start.weekday() == $VARIABLE.weekday_of_archive\" archive_day : input : choose_preprocessing.then function : typhoon.debug.echo args : data : !MultiStep - !Py print('processing archive') - !Py $BATCH otherday_processing_task : input : choose_preprocessing.else function : typhoon.debug.echo args : data : !MultiStep - !Py print('processing normal day') - !Py $BATCH List of tables In the typhoon cli use the following to add a simple list: typhoon variable add --var-id tables_to_land --var-type yaml --contents \" - clients - sales \" Adding variables from STDIN We can add variables from STDIN by leaving out the --contents argument. This will prompt us to give the value for the variable. We can also pipe the value UNIX style. Eg: cat query_file.sql | typhoon variable add --var-id tutorial_insert_query --var-type jinja or typhoon variable add --var-id tutorial_insert_query2 --var-type string < query_file.sql List of tables from a file You can also load a yaml file using: typhoon variable load --file \"my_table_list.yml\" You can reference in a DAG as the name of the file 'my_table_list'. You can then iterate over them like this (using them to load each table for example in future steps): name : variable_example schedule_interval : rate(10 minutes) tasks : list_tables : function : typhoon.flow_control.branch args : branches : !Py $VARIABLE.my_table_list monday_processing_task : input : list_tables function : typhoon.debug.echo args : data : !Py $BATCH This will output: ** data = transactions ** data = agents ** data = orders SQL Template We load the following jinja SQL template: SELECT * FROM {{ table_name }} WHERE start_date >= {{ start_date }} AND end_date < {{ end_date }} Using the following to load, check for issues, and list the variables loaded: typhoon variable load --file \"sql_template_1.sql\"` typhoon status typhoon variable ls Finally we can add this very easily to the DAG: name : variable_example schedule_interval : rate(1 hour) tasks : list_tables : function : typhoon.flow_control.branch args : branches : !Py $VARIABLE.my_table_list process_sql : input : list_tables function : typhoon.debug.echo args : data : !MultiStep - !Py $BATCH - !Py $DAG_CONTEXT.interval_start - !Py $DAG_CONTEXT.interval_end - !Py typhoon.templates.render($VARIABLE.sql_template_1, table_name=$1, start_date=$2, end_date=$3) Running this will echo the following for each table in our tables list. This can create powerful automation easily. ** data = SELECT * FROM orders WHERE start_date >= 2021-08-28 16:00:00 AND end_date < 2021-08-28 17:00:00 ;","title":"Variables & Templates"},{"location":"usage/templates.html#variables-templates","text":"Variables let us define a value outside our DAG. In general it is a good idea not to hard code magic values into our code, and DAGs in Typhoon are no exception. This makes it easier to change or tune the value without requiring us to re-deploy our DAG. You can set simple Variables or use the same functionality to create Templates. These are useful for: variables for logic control boilerplate sql (using jinja or params) lists of tables or schemas (e.g. list of tables to run a dag on) Similar to Airflow's variables If you're familiar with Airflow you'll notice that it's the same concept as Airflow variables. As much as possible Typhoon tries to depart from tried and tested solutions only when it makes . Variables and connections are something that for the most part just works in Airflow, so we saw no reason to change it adding more cognitive burden on developers.","title":"Variables &amp; Templates"},{"location":"usage/templates.html#types-of-variables","text":"With that said it's worth pointing out the differences. In Typhoon variables have types associated with them. If a variable is of type JSON or YAML it will get loaded into a python object. If you need it to be loaded as a string prefer to choose the TEXT type. Options: ['string', 'jinja', 'number', 'json', 'yaml']","title":"Types of variables"},{"location":"usage/templates.html#quick-usage","text":"To list, load or add variables in the Typhoon cli use: typhoon variable --help The id you assign or file name used is how you reference it in the DAG You can reference your variables with !Py $VARIABLE.my_variable or !Var myvariable (syntactic sugar) As you develop check for issues regularly using typhoon status List your variables using typhoon variable ls -l","title":"Quick usage"},{"location":"usage/templates.html#built-ins","text":"Introducing our built in context variables: $BATCH: This is the data package passed to the next node. $BATCH_NUM: This is the iterator number if you are batching (queries for example). $HOOK: This represents your hook - you can use !Hook my_conn as syntax sugar or !Py $HOOK.myconn $VARIABLE: This is how you can access saved variables e.g. lists of tables, query templates etc. An example might be !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) $DAG_CONTEXT.interval_start & $DAG_CONTEXT.interval_end: Execution interval for example: $DAG_CONTEXT.interval_start \u2192 '2021-05-23 10:00:00' (inclusive) $DAG_CONTEXT.interval_end \u2192 '2021-05-23 11:00:00' (exclusive)","title":"Built-ins"},{"location":"usage/templates.html#examples-of-setting-custom-a-variable","text":"","title":"Examples of setting custom a $VARIABLE"},{"location":"usage/templates.html#simple-variable","text":"In the typhoon cli use: typhoon variable add --var-id weekday_of_archive --var-type string --contents '1' You will get a response of Variable weekday_of_archive added . You can reference in a DAG using !Py $DAG_CONTEXT.interval_start.weekday() == $VARIABLE.weekday_of_archive . For example can be used to control if a process runs on a certain weekday with the if component. name : conditional_process schedule_interval : rate(10 minutes) tasks : list_tables : function : typhoon.flow_control.branch args : branches : - customers - transactions choose_preprocessing : input : list_tables component : typhoon.if args : data : !Py $BATCH condition : !Py \"lambda table: $DAG_CONTEXT.interval_start.weekday() == $VARIABLE.weekday_of_archive\" archive_day : input : choose_preprocessing.then function : typhoon.debug.echo args : data : !MultiStep - !Py print('processing archive') - !Py $BATCH otherday_processing_task : input : choose_preprocessing.else function : typhoon.debug.echo args : data : !MultiStep - !Py print('processing normal day') - !Py $BATCH","title":"Simple Variable"},{"location":"usage/templates.html#list-of-tables","text":"In the typhoon cli use the following to add a simple list: typhoon variable add --var-id tables_to_land --var-type yaml --contents \" - clients - sales \" Adding variables from STDIN We can add variables from STDIN by leaving out the --contents argument. This will prompt us to give the value for the variable. We can also pipe the value UNIX style. Eg: cat query_file.sql | typhoon variable add --var-id tutorial_insert_query --var-type jinja or typhoon variable add --var-id tutorial_insert_query2 --var-type string < query_file.sql","title":"List of tables"},{"location":"usage/templates.html#list-of-tables-from-a-file","text":"You can also load a yaml file using: typhoon variable load --file \"my_table_list.yml\" You can reference in a DAG as the name of the file 'my_table_list'. You can then iterate over them like this (using them to load each table for example in future steps): name : variable_example schedule_interval : rate(10 minutes) tasks : list_tables : function : typhoon.flow_control.branch args : branches : !Py $VARIABLE.my_table_list monday_processing_task : input : list_tables function : typhoon.debug.echo args : data : !Py $BATCH This will output: ** data = transactions ** data = agents ** data = orders","title":"List of tables from a file"},{"location":"usage/templates.html#sql-template","text":"We load the following jinja SQL template: SELECT * FROM {{ table_name }} WHERE start_date >= {{ start_date }} AND end_date < {{ end_date }} Using the following to load, check for issues, and list the variables loaded: typhoon variable load --file \"sql_template_1.sql\"` typhoon status typhoon variable ls Finally we can add this very easily to the DAG: name : variable_example schedule_interval : rate(1 hour) tasks : list_tables : function : typhoon.flow_control.branch args : branches : !Py $VARIABLE.my_table_list process_sql : input : list_tables function : typhoon.debug.echo args : data : !MultiStep - !Py $BATCH - !Py $DAG_CONTEXT.interval_start - !Py $DAG_CONTEXT.interval_end - !Py typhoon.templates.render($VARIABLE.sql_template_1, table_name=$1, start_date=$2, end_date=$3) Running this will echo the following for each table in our tables list. This can create powerful automation easily. ** data = SELECT * FROM orders WHERE start_date >= 2021-08-28 16:00:00 AND end_date < 2021-08-28 17:00:00 ;","title":"SQL Template"},{"location":"usage/transformations.html","text":"Transformations Transformations allow you to transform data as it is passed between tasks. You can do this inline using !Py, in multi-line snippets using !Multistep or encapsulated in a normal python function. The difference between transformations and functions is that transformations are applied before and can be to a specific argument. In-line and Multi-line are described in depth in !Py and !Multistep . Similarly to functions, transformations can be included in the project or referenced from the contributed / built-in repos. typhoon.transformation_file.transformation_name for built-in / contrib transformations. transformations.my_transformation_file.transformation_name for accessing your project transformations. Transformation example Applying a regex transformation to the data (element 1 which is the path) before its passed to the function. write_df_to_sql : input : read_csv_to_df function : functions.pandas_data_helpers.df_write args : hook : !Py $HOOK.transaction_db df : !Py $BATCH[0] table_name : !Py transformations.path_mapping.to_regex_name($BATCH[1]) Mixing !Multistep inline transformations and a flatten transformation. The dictionary will be flattened and then passed into csv in two lines before being written by the function. Note you can apply transformations independently to different arguments. In this case data path has a separate in-line transformation using !Py. This can make very powerful tasks that are also concise and readable. write_csv : input : exchange_rate function : typhoon.filesystem.write_data args : hook : !Hook echo data : !MultiStep - !Py transformations.xr.flatten_response($BATCH) - !Py typhoon.data.dicts_to_csv($1, delimiter='|') path : !Py f'{$DAG_CONTEXT.interval_start.isoformat()}_xr_data.csv'","title":"Transformations"},{"location":"usage/transformations.html#transformations","text":"Transformations allow you to transform data as it is passed between tasks. You can do this inline using !Py, in multi-line snippets using !Multistep or encapsulated in a normal python function. The difference between transformations and functions is that transformations are applied before and can be to a specific argument. In-line and Multi-line are described in depth in !Py and !Multistep . Similarly to functions, transformations can be included in the project or referenced from the contributed / built-in repos. typhoon.transformation_file.transformation_name for built-in / contrib transformations. transformations.my_transformation_file.transformation_name for accessing your project transformations.","title":"Transformations"},{"location":"usage/transformations.html#transformation-example","text":"Applying a regex transformation to the data (element 1 which is the path) before its passed to the function. write_df_to_sql : input : read_csv_to_df function : functions.pandas_data_helpers.df_write args : hook : !Py $HOOK.transaction_db df : !Py $BATCH[0] table_name : !Py transformations.path_mapping.to_regex_name($BATCH[1]) Mixing !Multistep inline transformations and a flatten transformation. The dictionary will be flattened and then passed into csv in two lines before being written by the function. Note you can apply transformations independently to different arguments. In this case data path has a separate in-line transformation using !Py. This can make very powerful tasks that are also concise and readable. write_csv : input : exchange_rate function : typhoon.filesystem.write_data args : hook : !Hook echo data : !MultiStep - !Py transformations.xr.flatten_response($BATCH) - !Py typhoon.data.dicts_to_csv($1, delimiter='|') path : !Py f'{$DAG_CONTEXT.interval_start.isoformat()}_xr_data.csv'","title":"Transformation example"}]}