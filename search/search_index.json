{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Typhoon Orchestrator Typhoon is a Data Worfklow tool (ETL / ELT pipelines) for composing Airflow or AWS Lambda DAGs in YAML. Supercharge your existing Airflow workflow by using Typhoon to create your DAGs with complete simplicity and re-usability. Deploying to your existing Airflow you can upgrade with zero risk, migrations or loss of existing Airflow functionality. If you like Airflow, you will Ariflow + Typhoon more. Workflow : Typhoon YAML DAG --> transpile --> Airflow deployment Key Principles Elegant : YAML; low-code and easy to pick up. Data sharing - data flows between tasks making it intuitive and easy to build tasks. Composability - Functions combine like Lego. Effortless to extend for more sources and connections. Components - reduce complex tasks (e.g. CSV\u2192S3\u2192Snowflake) to 1 re-usable task. UI : Component UI for sharing DAG configuration with your DWH, Analyst or Data Sci. teams. Rich Cli : Inspired by other great command line interfaces and instantly familiar. Intelligent bash/zsh completion. Testable Python - automated PyTest to for more robust flows. Flexible deployment : deploy to Airflow - large reduction in effort, without breaking existing production. deploy to AWS Lambda - completely serverless Layers Pristine : Pre-built (OSS) components and UI that can be shared to your team Core : Python (OSS) core Extensible and hackable. Components allow you to share extensions widely in the Pristine layer. Example DAG Typhoon DAG (YAML) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 name : exchange_rates schedule_interval : rate(1 day) tasks : exchange_rate : function : functions.exchange_rates_api.get_history args : start_at : !Py $DAG_CONTEXT.interval_start end_at : !Py $DAG_CONTEXT.interval_end write_csv : input : exchange_rate function : typhoon.filesystem.write_data args : hook : !Hook echo data : !MultiStep - !Py transformations.xr.flatten_response($BATCH) - !Py typhoon.data.dicts_to_csv($1, delimiter='|') path : xr_data.csv Airflow DAG (python) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 import datetime import typhoon.contrib.functions as typhoon_functions # for a fair comparison import typhoon.contrib.transformations as typhoon_transformations from airflow import DAG from airflow.hooks.base_hook import BaseHook from airflow.operators.python_operator import PythonOperator import transformations.xr from functions import exchange_rates_api from out.new_people.typhoon.contrib.hooks.filesystem_hooks import LocalStorageHook def exchange_rate ( base : str , ** context ): result = exchange_rates_api . get_history ( start_at = context [ 'execution_date' ], end_at = context [ 'next_execution_date' ], base = base , ) context [ 'ti' ] . xcom_push ( 'result' , list ( result )) def write_csv ( source_task_id , ** context ): conn_params = BaseHook . get_connection ( 'data_lake' ) hook = LocalStorageHook ( conn_params ) # Note how we're hardcoding the class batches = context [ 'ti' ] . xcom_pull ( task_ids = source_task_id , key = 'result' ) for batch in batches : flattened = transformations . xr . flatten_response ( response ) data = typhoon_transformations . data . dicts_to_csv ( flattened , delimiter = '|' ) path = context [ 'ds' ] + '_xr_data.csv' typhoon_functions . filesystem . write_data ( hook = hook , data = data , path = path ) with DAG ( dag_id = 'exchange_rates' , default_args = { 'owner' : 'typhoon' }, schedule_interval = '*/1 * * * *' , start_date = datetime . datetime ( 2021 , 3 , 25 , 21 , 10 ) ) as dag : for base in [ 'EUR' , 'USD' , 'AUD' ]: exchange_rate_task_id = f 'exchange_rate_ { base } ' exchange_rate_task = PythonOperator ( task_id = exchange_rate_task_id , python_callable = exchange_rate , op_kwargs = { 'base' : base }, provide_context = True ) dag >> exchange_rate_task write_csv_task = PythonOperator ( task_id = f 'write_csv_ { base } ' , python_callable = write_csv , op_kwargs = { 'source_task_id' : exchange_rate_task_id }, provide_context = True ) exchange_rate_task >> write_csv_task Above is an example of two tasks: Extracting the exchange rates from an API call function for a 1-day range Writing CSV to a filesystem. This example actually echos it; to put it to S3 change the Hook connection name. Within the edge between task 1 and 2 it transforms the data: It flattens the data Then transforms it from a dict to a pipe delimited CSV. Using with Airflow Building the above DAG using typhoon dag build --all . Airflow UI will then show: Component UI Put in screenshot of UI","title":"Overview"},{"location":"index.html#typhoon-orchestrator","text":"Typhoon is a Data Worfklow tool (ETL / ELT pipelines) for composing Airflow or AWS Lambda DAGs in YAML. Supercharge your existing Airflow workflow by using Typhoon to create your DAGs with complete simplicity and re-usability. Deploying to your existing Airflow you can upgrade with zero risk, migrations or loss of existing Airflow functionality. If you like Airflow, you will Ariflow + Typhoon more. Workflow : Typhoon YAML DAG --> transpile --> Airflow deployment Key Principles Elegant : YAML; low-code and easy to pick up. Data sharing - data flows between tasks making it intuitive and easy to build tasks. Composability - Functions combine like Lego. Effortless to extend for more sources and connections. Components - reduce complex tasks (e.g. CSV\u2192S3\u2192Snowflake) to 1 re-usable task. UI : Component UI for sharing DAG configuration with your DWH, Analyst or Data Sci. teams. Rich Cli : Inspired by other great command line interfaces and instantly familiar. Intelligent bash/zsh completion. Testable Python - automated PyTest to for more robust flows. Flexible deployment : deploy to Airflow - large reduction in effort, without breaking existing production. deploy to AWS Lambda - completely serverless","title":"Typhoon Orchestrator"},{"location":"index.html#layers","text":"Pristine : Pre-built (OSS) components and UI that can be shared to your team Core : Python (OSS) core Extensible and hackable. Components allow you to share extensions widely in the Pristine layer.","title":"Layers"},{"location":"index.html#example-dag","text":"Typhoon DAG (YAML) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 name : exchange_rates schedule_interval : rate(1 day) tasks : exchange_rate : function : functions.exchange_rates_api.get_history args : start_at : !Py $DAG_CONTEXT.interval_start end_at : !Py $DAG_CONTEXT.interval_end write_csv : input : exchange_rate function : typhoon.filesystem.write_data args : hook : !Hook echo data : !MultiStep - !Py transformations.xr.flatten_response($BATCH) - !Py typhoon.data.dicts_to_csv($1, delimiter='|') path : xr_data.csv Airflow DAG (python) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 import datetime import typhoon.contrib.functions as typhoon_functions # for a fair comparison import typhoon.contrib.transformations as typhoon_transformations from airflow import DAG from airflow.hooks.base_hook import BaseHook from airflow.operators.python_operator import PythonOperator import transformations.xr from functions import exchange_rates_api from out.new_people.typhoon.contrib.hooks.filesystem_hooks import LocalStorageHook def exchange_rate ( base : str , ** context ): result = exchange_rates_api . get_history ( start_at = context [ 'execution_date' ], end_at = context [ 'next_execution_date' ], base = base , ) context [ 'ti' ] . xcom_push ( 'result' , list ( result )) def write_csv ( source_task_id , ** context ): conn_params = BaseHook . get_connection ( 'data_lake' ) hook = LocalStorageHook ( conn_params ) # Note how we're hardcoding the class batches = context [ 'ti' ] . xcom_pull ( task_ids = source_task_id , key = 'result' ) for batch in batches : flattened = transformations . xr . flatten_response ( response ) data = typhoon_transformations . data . dicts_to_csv ( flattened , delimiter = '|' ) path = context [ 'ds' ] + '_xr_data.csv' typhoon_functions . filesystem . write_data ( hook = hook , data = data , path = path ) with DAG ( dag_id = 'exchange_rates' , default_args = { 'owner' : 'typhoon' }, schedule_interval = '*/1 * * * *' , start_date = datetime . datetime ( 2021 , 3 , 25 , 21 , 10 ) ) as dag : for base in [ 'EUR' , 'USD' , 'AUD' ]: exchange_rate_task_id = f 'exchange_rate_ { base } ' exchange_rate_task = PythonOperator ( task_id = exchange_rate_task_id , python_callable = exchange_rate , op_kwargs = { 'base' : base }, provide_context = True ) dag >> exchange_rate_task write_csv_task = PythonOperator ( task_id = f 'write_csv_ { base } ' , python_callable = write_csv , op_kwargs = { 'source_task_id' : exchange_rate_task_id }, provide_context = True ) exchange_rate_task >> write_csv_task Above is an example of two tasks: Extracting the exchange rates from an API call function for a 1-day range Writing CSV to a filesystem. This example actually echos it; to put it to S3 change the Hook connection name. Within the edge between task 1 and 2 it transforms the data: It flattens the data Then transforms it from a dict to a pipe delimited CSV.","title":"Example DAG"},{"location":"index.html#using-with-airflow","text":"Building the above DAG using typhoon dag build --all . Airflow UI will then show:","title":"Using with Airflow"},{"location":"index.html#component-ui","text":"Put in screenshot of UI","title":"Component UI"},{"location":"dag-development-improving.html","text":"Improving our DAG In the previous section we created a DAG that reads from a local directory and writes to a SQLite file. We will now improve on that DAG by introducing new concepts that will help us make it more flexible and testable. Variables Variables let us define a value outside our DAG. In general it is a good idea not to hard code magic values into our code, and DAGs in Typhoon are no exception. This makes it easier to change or tune the value without requiring us to re-deploy our DAG. Similar to Airflow's variables If you're familiar with Airflow you'll notice that it's the same concept as Airflow variables. As much as possible Typhoon tries to depart from tried and tested solutions only when it makes . Variables and connections are something that for the most part just works in Airflow, so we saw no reason to change it adding more cognitive burden on developers. With that said it's worth pointing out the differences. In Typhoon variables have types associated with them. If a variable is of type JSON or YAML it will get loaded into a python object. If you need it to be loaded as a string prefer to choose the TEXT type. SQL as variable Let's move our insert statement into a variable so we can change it in the future if we need to. We'll change the line query : INSERT INTO typhoon_tutorial VALUES (?, ?, ?) for: query : $VARIABLE.tutorial_insert_query We named our variable insert_query but we could have given it any name. Typhoon status If we run typhoon status now we will see the following warning: \u2022 Found variables in DAGs that are not defined in the metadata database - Variable tutorial_insert_query is not set. Try typhoon variable add --var-id tutorial_insert_query --var-type VAR_TYPE --contents VALUE None It's useful to run typhoon status often as it will warn us about problems on our project. Lets define the variable now as suggested: Adding the variable We will also turn the variable into a Jinja template which we will render later. typhoon variable add --var-id tutorial_insert_query --var-type jinja --contents \"INSERT INTO TABLE typhoon_tutorial VALUES ({{ file_name }}, {{ num_words }}, {{ ts }})\" Adding variables from STDIN We can add variables from STDIN by leaving out the --contents argument. This will prompt us to give the value for the variable. We can also pipe the value UNIX style. Eg: cat query_file.sql | typhoon variable add --var-id tutorial_insert_query --var-type jinja or typhoon variable add --var-id tutorial_insert_query2 --var-type string < query_file.sql We can now run typhoon status again to see there is no issue, and check our newly created variable with: typhoon variable ls -l Web UI DAG Editor The web UI has an experimental DAG editor with intelligent code completion for things like variable and connection names, as well as functions, transformations and node names. However it's sometimes a little behind the CLI and can have some bugs caused by syntax checking. It will be re-written at some point in the future, but in the mean time check it out with typhoon webserver . It's really useful when it works correctly. Connections In our functions we hardcoded the logic to read from the filesystem and to write to SQLite. This is not ideal for a few reasons: Limited re-usability: Overly specific functions means we are unlikely to find them useful in other DAGs. Limited flexibility: If the logic stays the same but the data source changes (eg: Postgres instead of SQLite) we need to modify the function. Security: Hard-coding connection details in functions is never a good idea. Passing them as parameters in DAGs is not much better as you probably want to version control your DAGs. To solve this Typhoon provides Connections which let you define a type (eg: SQLite) and connection parameters. Connections are stored in the database and are used to instantiate hooks. The connection type determines which hook class needs to be instantiated. Lets now change both the source and destination to use connections. Similarities with Airflow Connections in Typhoon are also very similar to Airflow's connections, down to naming of their parameters and the inclusion of an extra field for parameters that don't fit in the default ones. We depart from Airflow in some ways however: In contrast to Airflow it lets the user define new connection types that are first class (ie: you get code completion for them in the CLI and you can select them in the UI). It attempts to group similar connections in interfaces/protocols so that they can be swapped for one another, simplifying testing or data source changes. Airflow does this somewhat, for example with DbApiHook, but we take this concept a lot further by grouping other similar data sources such as filesystems (Local Storage, FTP, SFTP, S3 etc can all be used interchangeably). Of course you can always access the underlying connection if you need specific functionality that is not easily re-usable. FileSystem Hook To read from the directory we will use a FileSystemHook . Specifically a LocalStorageHook though as we'll see this can be changed later. We'll also use a pre-defined Typhoon function called list_directory to list files in the directory and another one called read_data to read data from these files. Changing the ls node We need to change the line ls : function : functions.tutorial.get_files_in_directory config : directory : /tmp/typhoon/landing/ to: ls : function : typhoon.filesystem.list_directory config : hook : $HOOK.tutorial_fs path : landing Notice how we no longer define the full path in the DAG. FileSystem hooks have a base path that gets prepended to it (eg: In the S3 hook that's the bucket, in an FTP hook you might want a different base directory for testing and another for production etc.). Adding the hook We will add the hook to our connections.yml file: tutorial_fs : local : conn_type : local_storage extra : base_path : /tmp/typhoon/ Then add it to our metadata database: typhoon connection add --conn-id tutorial_fs --conn-env local Reading the files We will add an additional node to read the file contents using another predefined Typhoon function: read_file : function : typhoon.filesystem.read_data config : hook : $hook.tutorial_fs Changing the count_words node and function We will change our function to receive a text and just count the words, removing the file reading logic from it. It is generally a good idea for a function to do one and only one thing. We will also take a dict of metadata so we can keep track of where the text came from. T = TypeVar ( 'T' ) def words_in_text ( text : str , metadata : T ) -> Tuple [ int , T ]: num_words = len ( re . split ( '\\s+' , text )) return num_words , metadata count_words : function : functions.tutorial.words_in_text DB-API Hook We will continue making our DAG more generic by using the Typhoon function execute_query, which takes a DbApiHook , which can be any hook supporting the DB-API interface . Almost every database library in Python supports it, so we will be able to use them interchangeably (if the SQL query is valid in both databases). Changing the write_to_db node We will change write_to_db : function : functions.tutorial.execute_on_sqlite config : sqlite_db : /tmp/typhoon/tutorial.db query : $VARIABLE.tutorial_insert_query to: write_to_db : function : typhoon.relational.execute_query config : hook : $HOOK.tutorial_db query : $VARIABLE.tutorial_insert_query Adding the hook First we define the connection details in the connections.yml file: tutorial_db : dev : conn_type : sqlite extra : database : /tmp/typhoon/tutorial.db Then we add the connection with the CLI: typhoon connection add --conn-id tutorial_db --conn-env local Edges: putting it all together Finally to make all of this come together we need toWe need to change the edges. The final DAG will be: name : tutorial schedule_interval : \"@hourly\" nodes : ls : function : typhoon.filesystem.list_directory config : hook : $HOOK.tutorial_fs path : landing read_file : function : typhoon.filesystem.read_data config : hook : $hook.tutorial_fs count_words : function : functions.tutorial.words_in_text write_to_db : function : typhoon.relational.execute_query config : hook : $HOOK.tutorial_db query : $VARIABLE.tutorial_insert_query edges : e1 : source : ls adapter : file_path => APPLY : $BATCH destination : count_words metadata : file_path => APPLY : $ADAPTER.file_path e2 : source : count_words adapter : parameters => APPLY : \"[$BATCH[0], $BATCH_METADATA.file_path, $DAG_CONTEXT.execution_date.isoformat()]\" destination : write_to_db","title":"Improving our DAG"},{"location":"dag-development-improving.html#improving-our-dag","text":"In the previous section we created a DAG that reads from a local directory and writes to a SQLite file. We will now improve on that DAG by introducing new concepts that will help us make it more flexible and testable.","title":"Improving our DAG"},{"location":"dag-development-improving.html#variables","text":"Variables let us define a value outside our DAG. In general it is a good idea not to hard code magic values into our code, and DAGs in Typhoon are no exception. This makes it easier to change or tune the value without requiring us to re-deploy our DAG. Similar to Airflow's variables If you're familiar with Airflow you'll notice that it's the same concept as Airflow variables. As much as possible Typhoon tries to depart from tried and tested solutions only when it makes . Variables and connections are something that for the most part just works in Airflow, so we saw no reason to change it adding more cognitive burden on developers. With that said it's worth pointing out the differences. In Typhoon variables have types associated with them. If a variable is of type JSON or YAML it will get loaded into a python object. If you need it to be loaded as a string prefer to choose the TEXT type.","title":"Variables"},{"location":"dag-development-improving.html#sql-as-variable","text":"Let's move our insert statement into a variable so we can change it in the future if we need to. We'll change the line query : INSERT INTO typhoon_tutorial VALUES (?, ?, ?) for: query : $VARIABLE.tutorial_insert_query We named our variable insert_query but we could have given it any name. Typhoon status If we run typhoon status now we will see the following warning: \u2022 Found variables in DAGs that are not defined in the metadata database - Variable tutorial_insert_query is not set. Try typhoon variable add --var-id tutorial_insert_query --var-type VAR_TYPE --contents VALUE None It's useful to run typhoon status often as it will warn us about problems on our project. Lets define the variable now as suggested:","title":"SQL as variable"},{"location":"dag-development-improving.html#adding-the-variable","text":"We will also turn the variable into a Jinja template which we will render later. typhoon variable add --var-id tutorial_insert_query --var-type jinja --contents \"INSERT INTO TABLE typhoon_tutorial VALUES ({{ file_name }}, {{ num_words }}, {{ ts }})\" Adding variables from STDIN We can add variables from STDIN by leaving out the --contents argument. This will prompt us to give the value for the variable. We can also pipe the value UNIX style. Eg: cat query_file.sql | typhoon variable add --var-id tutorial_insert_query --var-type jinja or typhoon variable add --var-id tutorial_insert_query2 --var-type string < query_file.sql We can now run typhoon status again to see there is no issue, and check our newly created variable with: typhoon variable ls -l Web UI DAG Editor The web UI has an experimental DAG editor with intelligent code completion for things like variable and connection names, as well as functions, transformations and node names. However it's sometimes a little behind the CLI and can have some bugs caused by syntax checking. It will be re-written at some point in the future, but in the mean time check it out with typhoon webserver . It's really useful when it works correctly.","title":"Adding the variable"},{"location":"dag-development-improving.html#connections","text":"In our functions we hardcoded the logic to read from the filesystem and to write to SQLite. This is not ideal for a few reasons: Limited re-usability: Overly specific functions means we are unlikely to find them useful in other DAGs. Limited flexibility: If the logic stays the same but the data source changes (eg: Postgres instead of SQLite) we need to modify the function. Security: Hard-coding connection details in functions is never a good idea. Passing them as parameters in DAGs is not much better as you probably want to version control your DAGs. To solve this Typhoon provides Connections which let you define a type (eg: SQLite) and connection parameters. Connections are stored in the database and are used to instantiate hooks. The connection type determines which hook class needs to be instantiated. Lets now change both the source and destination to use connections. Similarities with Airflow Connections in Typhoon are also very similar to Airflow's connections, down to naming of their parameters and the inclusion of an extra field for parameters that don't fit in the default ones. We depart from Airflow in some ways however: In contrast to Airflow it lets the user define new connection types that are first class (ie: you get code completion for them in the CLI and you can select them in the UI). It attempts to group similar connections in interfaces/protocols so that they can be swapped for one another, simplifying testing or data source changes. Airflow does this somewhat, for example with DbApiHook, but we take this concept a lot further by grouping other similar data sources such as filesystems (Local Storage, FTP, SFTP, S3 etc can all be used interchangeably). Of course you can always access the underlying connection if you need specific functionality that is not easily re-usable.","title":"Connections"},{"location":"dag-development-improving.html#filesystem-hook","text":"To read from the directory we will use a FileSystemHook . Specifically a LocalStorageHook though as we'll see this can be changed later. We'll also use a pre-defined Typhoon function called list_directory to list files in the directory and another one called read_data to read data from these files.","title":"FileSystem Hook"},{"location":"dag-development-improving.html#changing-the-ls-node","text":"We need to change the line ls : function : functions.tutorial.get_files_in_directory config : directory : /tmp/typhoon/landing/ to: ls : function : typhoon.filesystem.list_directory config : hook : $HOOK.tutorial_fs path : landing Notice how we no longer define the full path in the DAG. FileSystem hooks have a base path that gets prepended to it (eg: In the S3 hook that's the bucket, in an FTP hook you might want a different base directory for testing and another for production etc.).","title":"Changing the ls node"},{"location":"dag-development-improving.html#adding-the-hook","text":"We will add the hook to our connections.yml file: tutorial_fs : local : conn_type : local_storage extra : base_path : /tmp/typhoon/ Then add it to our metadata database: typhoon connection add --conn-id tutorial_fs --conn-env local","title":"Adding the hook"},{"location":"dag-development-improving.html#reading-the-files","text":"We will add an additional node to read the file contents using another predefined Typhoon function: read_file : function : typhoon.filesystem.read_data config : hook : $hook.tutorial_fs","title":"Reading the files"},{"location":"dag-development-improving.html#changing-the-count_words-node-and-function","text":"We will change our function to receive a text and just count the words, removing the file reading logic from it. It is generally a good idea for a function to do one and only one thing. We will also take a dict of metadata so we can keep track of where the text came from. T = TypeVar ( 'T' ) def words_in_text ( text : str , metadata : T ) -> Tuple [ int , T ]: num_words = len ( re . split ( '\\s+' , text )) return num_words , metadata count_words : function : functions.tutorial.words_in_text","title":"Changing the count_words node and function"},{"location":"dag-development-improving.html#db-api-hook","text":"We will continue making our DAG more generic by using the Typhoon function execute_query, which takes a DbApiHook , which can be any hook supporting the DB-API interface . Almost every database library in Python supports it, so we will be able to use them interchangeably (if the SQL query is valid in both databases).","title":"DB-API Hook"},{"location":"dag-development-improving.html#changing-the-write_to_db-node","text":"We will change write_to_db : function : functions.tutorial.execute_on_sqlite config : sqlite_db : /tmp/typhoon/tutorial.db query : $VARIABLE.tutorial_insert_query to: write_to_db : function : typhoon.relational.execute_query config : hook : $HOOK.tutorial_db query : $VARIABLE.tutorial_insert_query","title":"Changing the write_to_db node"},{"location":"dag-development-improving.html#adding-the-hook_1","text":"First we define the connection details in the connections.yml file: tutorial_db : dev : conn_type : sqlite extra : database : /tmp/typhoon/tutorial.db Then we add the connection with the CLI: typhoon connection add --conn-id tutorial_db --conn-env local","title":"Adding the hook"},{"location":"dag-development-improving.html#edges-putting-it-all-together","text":"Finally to make all of this come together we need toWe need to change the edges. The final DAG will be: name : tutorial schedule_interval : \"@hourly\" nodes : ls : function : typhoon.filesystem.list_directory config : hook : $HOOK.tutorial_fs path : landing read_file : function : typhoon.filesystem.read_data config : hook : $hook.tutorial_fs count_words : function : functions.tutorial.words_in_text write_to_db : function : typhoon.relational.execute_query config : hook : $HOOK.tutorial_db query : $VARIABLE.tutorial_insert_query edges : e1 : source : ls adapter : file_path => APPLY : $BATCH destination : count_words metadata : file_path => APPLY : $ADAPTER.file_path e2 : source : count_words adapter : parameters => APPLY : \"[$BATCH[0], $BATCH_METADATA.file_path, $DAG_CONTEXT.execution_date.isoformat()]\" destination : write_to_db","title":"Edges: putting it all together"},{"location":"dag-development-tutorial.html","text":"DAG Development Tutorial Let's start from scratch with a very basic DAG. Every hour we will read all the files under the directory /tmp/typhoon/landing/ , count the number of words and write that number in a SQLite database. The SQLite table we write to will have the following shape: file_name num_words ts a.txt 13 2020-01-06T13:01:15 b.txt 20 2020-01-06T13:01:17 c.txt 10 2020-01-06T13:01:19 Learning example This is just an example for learning purposes. In a production deployment this would not be a useful DAG since in a Lambda instance there won't be any data under tmp . We also won't be able to retrieve the SQLite database because it's persisted to a local file, which will disappear once the lambda finishes running. However, it is still a good example to learn the basics in a local environment as it's a simple example and not that far off from real applications. Just replace the function that reads locally with a function that reads from an FTP location and the function that writes to SQLite with a function that writes to any other remote database. Functions and transformations When trying to solve a problem in python it is useful to write some functions and test them until we get our solution right, whether it is in a jupyter notebook or REPL. Typhoon encourages this kind of interactive development as we can trivially incorporate these functions into our DAGs. Reading from a directory Our first function will search inside a directory and yield the .txt file paths in batches: from typing import Iterator from pathlib import Path def get_files_in_directory ( directory : str ) -> Iterator [ str ]: yield from map ( str , Path ( directory ) . glob ( '*.txt' )) Let's place this code inside a file called functions/tutorial.py . Pathlib If this is the first time you see the pathlib library check it out , it is one of the nicest additions to Python 3. Counting number of lines Given a file path we will read it and count the number of words in it. A naive implementation might be the following: import re from typing import Tuple from pathlib import Path def words_in_file ( file_path : str ) -> Tuple [ str , int ]: contents = Path ( file_path ) . read_text () num_words = len ( re . split ( '\\s+' , contents )) return file_path , num_words We just split the contents when there's one or more separators and count the length of the resulting list. Notice that we also returned the file_path as well as the number of words, since it's likely that we will need it upstream. This is a common and useful pattern in Typhoon functions. Let's also place this code inside the previous file called functions/tutorial.py . Writing to SQLite Once we have this information we want to write it to a SQLite table in tmp/typhoon/tutorial.db that we will call typhoon_tutorial . However if we write a function that does exactly that there's very little chance we will ever use it again. With very little effort we can make a function that can execute any query against a SQLite database. This is much more likely to be useful in the future. import sqlite3 from contextlib import closing from typing import Sequence def execute_on_sqlite ( sqlite_db : str , query : str , parameters : Sequence ): conn = sqlite3 . connect ( sqlite_db ) with closing ( conn . cursor ()) as cursor : cursor . execute ( query , parameters ) conn . commit () Finally we will also place this code inside the previous file called functions/tutorial.py . Split functions in different files Usually we would split functions in files that are descriptive of what they do so they are organised in modules. Eg: filesystem.get_files_in_directory . Since this is just a tutorial we keep them in the same file for simplicity. Testing As you may have noticed, the fact that we wrote our code as regular python functions that just take some parameters and return some values means that it is trivially testable. Our testing framework of choice for this example is Pytest, but any other framework would work equally well. \"Pure\" functions We can't say that our functions are pure in the strict meaning of the term since they have side effects, but they share some advantages such as having no global state and being repeatable as much as possible (ie: given the same input returning the same output). import sqlite3 from contextlib import closing from functions.tutorial import get_files_in_directory , words_in_file , execute_on_sqlite def test_get_files_in_directory ( tmp_path ): file1 = tmp_path / 'a.txt' file1 . write_text ( 'foo' ) file2 = tmp_path / 'b.txt' file2 . write_text ( 'bar' ) assert set ( get_files_in_directory ( str ( tmp_path ))) == { str ( file1 ), str ( file2 )} def test_words_in_file ( tmp_path ): test_file = ( tmp_path / 'a.txt' ) test_file . write_text ( 'To be, or not to be, that is the question' ) assert words_in_file ( str ( test_file )) == ( str ( test_file ), 10 ) def test_execute_on_sqlite ( tmp_path ): sqlite_db = str ( tmp_path / 'test.db' ) conn = sqlite3 . connect ( sqlite_db ) with closing ( conn . cursor ()) as cursor : cursor . execute ( '''CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)''' ) insert_query = '''INSERT INTO stocks VALUES (?,?,?,?,?)''' insert_query_params = ( '2006-01-05' , 'BUY' , 'RHAR' , 11 , 35.14 ) execute_on_sqlite ( sqlite_db , insert_query , insert_query_params ) with closing ( conn . cursor ()) as cursor : cursor . execute ( 'SELECT * FROM stocks' ) assert cursor . fetchone () == insert_query_params conn . close () We can create directory tests/ and put it in a file named tutorial_tests.py . Run them with pytest and check that they pass. Now we can be confident that our functions work as intended. Running tests To run the tests go to the base directory of the Typhoon project and run PYTHONPATH=\"PYTHONPATH:$PWD\" python -m pytest tests . This will fix the python path so the imports work correctly in the tests. DAG Now that we have all the necessary functions we just need to write the DAG. Overview To get an intuitive understanding of what DAG definitions do we will first show you how we would write the code in a regular python script that we want to run from the console. After that we will write the real Typhoon DAG code so the reader can compare. Not real code This is just an intuitive understanding of what the DAG compiles to. It's a useful model to have even though it's not entirely accurate as we will see later. Take a moment to understand this code, it will help you as a reference once we introduce the DAG code. from datetime import datetime DIRECTORY = '/tmp/typhoon/landing/' SQLITE_DB = '/tmp/typhoon/tutorial.db' INSERT_QUERY = 'INSERT INTO TABLE typhoon_tutorial VALUES (?, ?, ?)' result = get_files_in_directory ( directory = DIRECTORY ) for batch in result : result = words_in_file ( file_path = batch ) parameters = [ batch [ 0 ], batch [ 1 ], datetime . now () . isoformat ()] execute_on_sqlite ( sqlite_db = SQLITE_DB , query = INSERT_QUERY , parameters = parameters ) Graph representation We can visualize these tasks in a Direct Acyclig Graph (DAG). This is how we could define the DAG: graph LR subgraph DAG ls -- e1 --> count_words count_words -- e2 --> write_to_db end Typhoon uses graph terminology in DAGs, defining tasks in terms of nodes and edges . However, at run-time ls will yield a batch for each file it finds. Assuming if finds a.txt , b.txt and c.txt then it will yield three batches and there will be a count_words task for each. The count words node just returns one value/batch so it won't branch any further. We can visualize the run-time shape of this DAG as the following: graph TB subgraph DAG ls-- a.txt -->count_words ls-- b.txt -->count_words2 ls-- c.txt -->count_words3 subgraph Branch one count_words -- a.txt, 12 --> write_to_db end subgraph Branch two count_words2 -- b.txt, 9 --> write_to_db2 end subgraph Branch three count_words3 -- c.txt, 17 --> write_to_db3 end end DAG Code Now that we have an idea of what we want to do let's write the real DAG code that does the same. name : tutorial schedule_interval : \"@hourly\" nodes : ls : function : functions.tutorial.get_files_in_directory config : directory : /tmp/typhoon/landing/ count_words : function : functions.tutorial.words_in_file write_to_db : function : functions.tutorial.execute_on_sqlite config : sqlite_db : /tmp/typhoon/tutorial.db insert_query : INSERT INTO TABLE typhoon_tutorial VALUES (?, ?, ?) edges : e1 : source : ls adapter : file_path => APPLY : $BATCH destination : count_words e2 : source : count_words adapter : parameters => APPLY : \"[$BATCH[0], $BATCH[1], $DAG_CONTEXT.execution_date.isoformat()]\" destination : write_to_db Save this code in dags/tutorial.yml . Config Vs Adapter Notice how in config we are passing the static parameters, while in the adapter we are constructing the dynamic parameters from the source node's batch. Think of config as parameters that are static configuration, while adapter adapts the output of the source node to the dynamic input parameter of the destination node. Testing the edges It can seem like the downside to having a YAML file is that code in the adapters can't be unit tested like functions can. To fix that Typhoon lets you test edges on the CLI by providing an input batch. Our DAG does not have much transformation logic, but it does have some in edge e2 which we can test with: typhoon dag edge test --dag-name tutorial --edge-name e2 --input \"['a.txt', 3]\" --eval Eval Without the --eval flag our input batch would be a string. We need it to evaluate that string as python code. Testing different execution dates Since the result uses the execution date we can pass it as a parameter to test with --execution-date '2019-01-02' . It accepts dates with any of the following formats %Y-%m-%d , %Y-%m-%dT%H:%M:%S or %Y-%m-%d %H:%M:%S . Since we didn't use any in the previous example it used the current time. Running the DAG Creating the SQLite table In order for the DAG to work our database needs to have the typhoon_tutorial table defined. > sqlite3 /tmp/typhoon/tutorial.db SQLite version 3.30.1 2019-10-10 20:19:45 Enter \".help\" for usage hints. sqlite> CREATE TABLE typhoon_tutorial (file_name TEXT, num_words INT, ts TEXT); DAG run We can now run the DAG to check that it works as expected. typhoon dag run --dag-name tutorial Result: Running tutorial from local build... Cleaning out directory... Build all DAGs... Writing file to /Users/biellls/Desktop/typhoon/hello_world/out/template.yml Writing file to /Users/biellls/Desktop/typhoon/hello_world/out/tutorial/requirements.txt Typhoon package is in editable mode. Copying to lambda package... Setting up user defined code as symlink for debugging... Finished building DAGs Checking results Lets read the table to make sure we inserted the data correctly: > sqlite3 /tmp/typhoon/tutorial.db SQLite version 3.30.1 2019-10-10 20:19:45 Enter \".help\" for usage hints. sqlite> .headers on sqlite> .mode column sqlite> select * from typhoon_tutorial; file_name num_words ts -------------------------- ---------- -------------------------- /tmp/typhoon/landing/a.txt 5 2020-01-17T18:36:39.489866","title":"DAG Development Tutorial"},{"location":"dag-development-tutorial.html#dag-development-tutorial","text":"Let's start from scratch with a very basic DAG. Every hour we will read all the files under the directory /tmp/typhoon/landing/ , count the number of words and write that number in a SQLite database. The SQLite table we write to will have the following shape: file_name num_words ts a.txt 13 2020-01-06T13:01:15 b.txt 20 2020-01-06T13:01:17 c.txt 10 2020-01-06T13:01:19 Learning example This is just an example for learning purposes. In a production deployment this would not be a useful DAG since in a Lambda instance there won't be any data under tmp . We also won't be able to retrieve the SQLite database because it's persisted to a local file, which will disappear once the lambda finishes running. However, it is still a good example to learn the basics in a local environment as it's a simple example and not that far off from real applications. Just replace the function that reads locally with a function that reads from an FTP location and the function that writes to SQLite with a function that writes to any other remote database.","title":"DAG Development Tutorial"},{"location":"dag-development-tutorial.html#functions-and-transformations","text":"When trying to solve a problem in python it is useful to write some functions and test them until we get our solution right, whether it is in a jupyter notebook or REPL. Typhoon encourages this kind of interactive development as we can trivially incorporate these functions into our DAGs.","title":"Functions and transformations"},{"location":"dag-development-tutorial.html#reading-from-a-directory","text":"Our first function will search inside a directory and yield the .txt file paths in batches: from typing import Iterator from pathlib import Path def get_files_in_directory ( directory : str ) -> Iterator [ str ]: yield from map ( str , Path ( directory ) . glob ( '*.txt' )) Let's place this code inside a file called functions/tutorial.py . Pathlib If this is the first time you see the pathlib library check it out , it is one of the nicest additions to Python 3.","title":"Reading from a directory"},{"location":"dag-development-tutorial.html#counting-number-of-lines","text":"Given a file path we will read it and count the number of words in it. A naive implementation might be the following: import re from typing import Tuple from pathlib import Path def words_in_file ( file_path : str ) -> Tuple [ str , int ]: contents = Path ( file_path ) . read_text () num_words = len ( re . split ( '\\s+' , contents )) return file_path , num_words We just split the contents when there's one or more separators and count the length of the resulting list. Notice that we also returned the file_path as well as the number of words, since it's likely that we will need it upstream. This is a common and useful pattern in Typhoon functions. Let's also place this code inside the previous file called functions/tutorial.py .","title":"Counting number of lines"},{"location":"dag-development-tutorial.html#writing-to-sqlite","text":"Once we have this information we want to write it to a SQLite table in tmp/typhoon/tutorial.db that we will call typhoon_tutorial . However if we write a function that does exactly that there's very little chance we will ever use it again. With very little effort we can make a function that can execute any query against a SQLite database. This is much more likely to be useful in the future. import sqlite3 from contextlib import closing from typing import Sequence def execute_on_sqlite ( sqlite_db : str , query : str , parameters : Sequence ): conn = sqlite3 . connect ( sqlite_db ) with closing ( conn . cursor ()) as cursor : cursor . execute ( query , parameters ) conn . commit () Finally we will also place this code inside the previous file called functions/tutorial.py . Split functions in different files Usually we would split functions in files that are descriptive of what they do so they are organised in modules. Eg: filesystem.get_files_in_directory . Since this is just a tutorial we keep them in the same file for simplicity.","title":"Writing to SQLite"},{"location":"dag-development-tutorial.html#testing","text":"As you may have noticed, the fact that we wrote our code as regular python functions that just take some parameters and return some values means that it is trivially testable. Our testing framework of choice for this example is Pytest, but any other framework would work equally well. \"Pure\" functions We can't say that our functions are pure in the strict meaning of the term since they have side effects, but they share some advantages such as having no global state and being repeatable as much as possible (ie: given the same input returning the same output). import sqlite3 from contextlib import closing from functions.tutorial import get_files_in_directory , words_in_file , execute_on_sqlite def test_get_files_in_directory ( tmp_path ): file1 = tmp_path / 'a.txt' file1 . write_text ( 'foo' ) file2 = tmp_path / 'b.txt' file2 . write_text ( 'bar' ) assert set ( get_files_in_directory ( str ( tmp_path ))) == { str ( file1 ), str ( file2 )} def test_words_in_file ( tmp_path ): test_file = ( tmp_path / 'a.txt' ) test_file . write_text ( 'To be, or not to be, that is the question' ) assert words_in_file ( str ( test_file )) == ( str ( test_file ), 10 ) def test_execute_on_sqlite ( tmp_path ): sqlite_db = str ( tmp_path / 'test.db' ) conn = sqlite3 . connect ( sqlite_db ) with closing ( conn . cursor ()) as cursor : cursor . execute ( '''CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)''' ) insert_query = '''INSERT INTO stocks VALUES (?,?,?,?,?)''' insert_query_params = ( '2006-01-05' , 'BUY' , 'RHAR' , 11 , 35.14 ) execute_on_sqlite ( sqlite_db , insert_query , insert_query_params ) with closing ( conn . cursor ()) as cursor : cursor . execute ( 'SELECT * FROM stocks' ) assert cursor . fetchone () == insert_query_params conn . close () We can create directory tests/ and put it in a file named tutorial_tests.py . Run them with pytest and check that they pass. Now we can be confident that our functions work as intended. Running tests To run the tests go to the base directory of the Typhoon project and run PYTHONPATH=\"PYTHONPATH:$PWD\" python -m pytest tests . This will fix the python path so the imports work correctly in the tests.","title":"Testing"},{"location":"dag-development-tutorial.html#dag","text":"Now that we have all the necessary functions we just need to write the DAG.","title":"DAG"},{"location":"dag-development-tutorial.html#overview","text":"To get an intuitive understanding of what DAG definitions do we will first show you how we would write the code in a regular python script that we want to run from the console. After that we will write the real Typhoon DAG code so the reader can compare. Not real code This is just an intuitive understanding of what the DAG compiles to. It's a useful model to have even though it's not entirely accurate as we will see later. Take a moment to understand this code, it will help you as a reference once we introduce the DAG code. from datetime import datetime DIRECTORY = '/tmp/typhoon/landing/' SQLITE_DB = '/tmp/typhoon/tutorial.db' INSERT_QUERY = 'INSERT INTO TABLE typhoon_tutorial VALUES (?, ?, ?)' result = get_files_in_directory ( directory = DIRECTORY ) for batch in result : result = words_in_file ( file_path = batch ) parameters = [ batch [ 0 ], batch [ 1 ], datetime . now () . isoformat ()] execute_on_sqlite ( sqlite_db = SQLITE_DB , query = INSERT_QUERY , parameters = parameters )","title":"Overview"},{"location":"dag-development-tutorial.html#graph-representation","text":"We can visualize these tasks in a Direct Acyclig Graph (DAG). This is how we could define the DAG: graph LR subgraph DAG ls -- e1 --> count_words count_words -- e2 --> write_to_db end Typhoon uses graph terminology in DAGs, defining tasks in terms of nodes and edges . However, at run-time ls will yield a batch for each file it finds. Assuming if finds a.txt , b.txt and c.txt then it will yield three batches and there will be a count_words task for each. The count words node just returns one value/batch so it won't branch any further. We can visualize the run-time shape of this DAG as the following: graph TB subgraph DAG ls-- a.txt -->count_words ls-- b.txt -->count_words2 ls-- c.txt -->count_words3 subgraph Branch one count_words -- a.txt, 12 --> write_to_db end subgraph Branch two count_words2 -- b.txt, 9 --> write_to_db2 end subgraph Branch three count_words3 -- c.txt, 17 --> write_to_db3 end end","title":"Graph representation"},{"location":"dag-development-tutorial.html#dag-code","text":"Now that we have an idea of what we want to do let's write the real DAG code that does the same. name : tutorial schedule_interval : \"@hourly\" nodes : ls : function : functions.tutorial.get_files_in_directory config : directory : /tmp/typhoon/landing/ count_words : function : functions.tutorial.words_in_file write_to_db : function : functions.tutorial.execute_on_sqlite config : sqlite_db : /tmp/typhoon/tutorial.db insert_query : INSERT INTO TABLE typhoon_tutorial VALUES (?, ?, ?) edges : e1 : source : ls adapter : file_path => APPLY : $BATCH destination : count_words e2 : source : count_words adapter : parameters => APPLY : \"[$BATCH[0], $BATCH[1], $DAG_CONTEXT.execution_date.isoformat()]\" destination : write_to_db Save this code in dags/tutorial.yml . Config Vs Adapter Notice how in config we are passing the static parameters, while in the adapter we are constructing the dynamic parameters from the source node's batch. Think of config as parameters that are static configuration, while adapter adapts the output of the source node to the dynamic input parameter of the destination node.","title":"DAG Code"},{"location":"dag-development-tutorial.html#testing-the-edges","text":"It can seem like the downside to having a YAML file is that code in the adapters can't be unit tested like functions can. To fix that Typhoon lets you test edges on the CLI by providing an input batch. Our DAG does not have much transformation logic, but it does have some in edge e2 which we can test with: typhoon dag edge test --dag-name tutorial --edge-name e2 --input \"['a.txt', 3]\" --eval Eval Without the --eval flag our input batch would be a string. We need it to evaluate that string as python code. Testing different execution dates Since the result uses the execution date we can pass it as a parameter to test with --execution-date '2019-01-02' . It accepts dates with any of the following formats %Y-%m-%d , %Y-%m-%dT%H:%M:%S or %Y-%m-%d %H:%M:%S . Since we didn't use any in the previous example it used the current time.","title":"Testing the edges"},{"location":"dag-development-tutorial.html#running-the-dag","text":"","title":"Running the DAG"},{"location":"dag-development-tutorial.html#creating-the-sqlite-table","text":"In order for the DAG to work our database needs to have the typhoon_tutorial table defined. > sqlite3 /tmp/typhoon/tutorial.db SQLite version 3.30.1 2019-10-10 20:19:45 Enter \".help\" for usage hints. sqlite> CREATE TABLE typhoon_tutorial (file_name TEXT, num_words INT, ts TEXT);","title":"Creating the SQLite table"},{"location":"dag-development-tutorial.html#dag-run","text":"We can now run the DAG to check that it works as expected. typhoon dag run --dag-name tutorial Result: Running tutorial from local build... Cleaning out directory... Build all DAGs... Writing file to /Users/biellls/Desktop/typhoon/hello_world/out/template.yml Writing file to /Users/biellls/Desktop/typhoon/hello_world/out/tutorial/requirements.txt Typhoon package is in editable mode. Copying to lambda package... Setting up user defined code as symlink for debugging... Finished building DAGs","title":"DAG run"},{"location":"dag-development-tutorial.html#checking-results","text":"Lets read the table to make sure we inserted the data correctly: > sqlite3 /tmp/typhoon/tutorial.db SQLite version 3.30.1 2019-10-10 20:19:45 Enter \".help\" for usage hints. sqlite> .headers on sqlite> .mode column sqlite> select * from typhoon_tutorial; file_name num_words ts -------------------------- ---------- -------------------------- /tmp/typhoon/landing/a.txt 5 2020-01-17T18:36:39.489866","title":"Checking results"},{"location":"examples/airflow-examples.html","text":"","title":"Airflow examples"},{"location":"examples/automating-pandas.html","text":"","title":"Pandas automation"},{"location":"examples/hello-world.html","text":"Hello World - a non-trivial example This tutorial will take you through how to write a simple (but non-trivial) Hello World example. This is the final DAG we will build. We will step through each part below. The DAG takes a YAML list of dictonaties (our data) and writes each dictionary to a file. It stores the file by run_date (hourly). name : hello_world schedule_interval : rate(1 hours) granularity : hour tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True Creating a Task name : hello_world schedule_interval : rate(10 minutes) granularity : hour Very simply this sets you flow name (no spaces) and the schedule interval in a rate to run at. It will use a timestamp truncated to the hour for the intervals. tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot Here we are setting up our tasks in our flow. This is a DAG structure similar to many other workflow tools (e.g. Task A \u2192 Task B \u2192 Task C). Lets examine our first task, send_data , which simply outputs 3 'files' containing CSV formatted strings represented as a YAML list of dictionaries. We use flow_control.branch to yield our 3 branches to the next node. Connections & typhoon status Lets start now by building this very basic task, and inspecting the output using a few options. First in your terminal, in the directory you have initialised your project, use typhoon-cli to run typhoon status . We encourage you to run this after your steps to see if any errors, or unset variables are there. Here we can see that we are initially missing a connection file without connections so lets add this. You need to add a connections.yml file to the root directory of your typhoon project. In this case we are calling our data_lake which is a local filesystem in local and a cloud S3 bucket in prod . To choose our environment we simply swap connections. We will also add an echo connection which we will use in a moment. echo : local : conn_type : echo data_lake : prod : conn_type : s3 extra : base_path : / bucket : test-corp-data-lake region_name : eu-west-1 login : <your login> passowrd : <your key> local : conn_type : local_storage extra : base_path : /tmp/data_lake Now we have this lets add our missing connections. We can get help on the client by using typhoon \u2014-help And then typhoon connection -\u2014help Finally, we can use typhoon connection add --help So now we can see how to add our connections from the connections.yml file. typhoon connection add --conn-id data_lake --conn-env **local** typhoon connection add --conn-id echo --conn-env **local** Now again typhoon status A Ok! Build, Run and debug Let's build our hello_world DAG and see some output. As a quick way lets add on echo task after our send_data task: echo_my_output : input : send_data function : typhoon.debug.echo args : mydata : !Py $BATCH $BATCH is the data packet as a consistent default reference. It can be any type so depending on your data can be shaped differently. e.g. $BATCH['key], $BATCH[0] etc. Lets build it in typhoon-cli! We can skip the help as you can now navigate the cli help the same way as for connections ( hint: typhoon dag --help ) typhoon dag build hello_world ... and run it typhoon dag run --dag-name hello_world Output of the YAML list as a list of dictionaries Now we can see what will be passed to the next node (which in our case is just echoing to the prompt). Writing to a file write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True Our next task write_data receives each branch (asynchronously - more on this later for performance) from its input task. Setting a task as an input creates an edge for the data to flow linking them: send_data \u2192 write_data Next you notice we are writing to a filesystem using a standard python function filesystem.write_data . Into this function we are passing 3 arguments, the connection hook, a transformation of the data, and the path (similar to airflow ones - reusable?), Introducing our built in context variables $BATCH: This is the data package passed to the next node. $BATCH_NUM: This is the iterator number if you are batching (queries for example). $HOOK: This represents your hook - you can use !Hook my_conn as syntax sugar or !Py $HOOK.myconn $VARIABLE: This is how you can access saved variables e.g. lists of tables, query templates etc. An example might be !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) $DAG_CONTEXT.interval_start & $DAG_CONTEXT.interval_end: Execution interval for example: $DAG_CONTEXT.interval_start \u2192 '2021-05-23 10:00:00' (inclusive) $DAG_CONTEXT.interval_end \u2192 '2021-05-23 11:00:00' (exclusive) Passing data The notation !Py indicates that what comes after is evaluated as normal python code. $BATCH is the data packet as a consistent default reference. It can be any type so depending on your data can be shaped differently e.g. $BATCH['key], $BATCH[0] etc. In our case remember each branch yields a list of dictionaries with two keys [{'filename': 'users.txt', 'contents':'['John', 'Amy', 'Adam', 'Jane']},{'filename':...}] The most complex item here is a !Multistep process showing how you can do multiple small python transformations in series in a a nice readable way (or you can put it in one line of course): !Py $BATCH['filename'] will evaluate to \"users.txt' !Py $DAG_CONTEXT.interval_end will evaluate to the timestamp of the DAG run. This is a built in context variable. !Py f'/store/{$2}/{$1}' finally this references the first two lines (1, 2) and uses a normal Python f-string to make the path. Evaluating to '/store/ 2021-03-13T12:00:00/users.txt' Lastly we want to deliver the right data, which is the 'contents' key: data: !Py $BATCH['contents'] Here is our result! Animals and fruits, along with our users!","title":"Hello World"},{"location":"examples/hello-world.html#hello-world-a-non-trivial-example","text":"This tutorial will take you through how to write a simple (but non-trivial) Hello World example. This is the final DAG we will build. We will step through each part below. The DAG takes a YAML list of dictonaties (our data) and writes each dictionary to a file. It stores the file by run_date (hourly). name : hello_world schedule_interval : rate(1 hours) granularity : hour tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True","title":"Hello World - a non-trivial example"},{"location":"examples/hello-world.html#creating-a-task","text":"name : hello_world schedule_interval : rate(10 minutes) granularity : hour Very simply this sets you flow name (no spaces) and the schedule interval in a rate to run at. It will use a timestamp truncated to the hour for the intervals. tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot Here we are setting up our tasks in our flow. This is a DAG structure similar to many other workflow tools (e.g. Task A \u2192 Task B \u2192 Task C). Lets examine our first task, send_data , which simply outputs 3 'files' containing CSV formatted strings represented as a YAML list of dictionaries. We use flow_control.branch to yield our 3 branches to the next node.","title":"Creating a Task"},{"location":"examples/hello-world.html#connections-typhoon-status","text":"Lets start now by building this very basic task, and inspecting the output using a few options. First in your terminal, in the directory you have initialised your project, use typhoon-cli to run typhoon status . We encourage you to run this after your steps to see if any errors, or unset variables are there. Here we can see that we are initially missing a connection file without connections so lets add this. You need to add a connections.yml file to the root directory of your typhoon project. In this case we are calling our data_lake which is a local filesystem in local and a cloud S3 bucket in prod . To choose our environment we simply swap connections. We will also add an echo connection which we will use in a moment. echo : local : conn_type : echo data_lake : prod : conn_type : s3 extra : base_path : / bucket : test-corp-data-lake region_name : eu-west-1 login : <your login> passowrd : <your key> local : conn_type : local_storage extra : base_path : /tmp/data_lake Now we have this lets add our missing connections. We can get help on the client by using typhoon \u2014-help And then typhoon connection -\u2014help Finally, we can use typhoon connection add --help So now we can see how to add our connections from the connections.yml file. typhoon connection add --conn-id data_lake --conn-env **local** typhoon connection add --conn-id echo --conn-env **local** Now again typhoon status A Ok!","title":"Connections &amp; typhoon status"},{"location":"examples/hello-world.html#build-run-and-debug","text":"Let's build our hello_world DAG and see some output. As a quick way lets add on echo task after our send_data task: echo_my_output : input : send_data function : typhoon.debug.echo args : mydata : !Py $BATCH $BATCH is the data packet as a consistent default reference. It can be any type so depending on your data can be shaped differently. e.g. $BATCH['key], $BATCH[0] etc. Lets build it in typhoon-cli! We can skip the help as you can now navigate the cli help the same way as for connections ( hint: typhoon dag --help ) typhoon dag build hello_world ... and run it typhoon dag run --dag-name hello_world Output of the YAML list as a list of dictionaries Now we can see what will be passed to the next node (which in our case is just echoing to the prompt).","title":"Build, Run and debug"},{"location":"examples/hello-world.html#writing-to-a-file","text":"write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True Our next task write_data receives each branch (asynchronously - more on this later for performance) from its input task. Setting a task as an input creates an edge for the data to flow linking them: send_data \u2192 write_data Next you notice we are writing to a filesystem using a standard python function filesystem.write_data . Into this function we are passing 3 arguments, the connection hook, a transformation of the data, and the path (similar to airflow ones - reusable?),","title":"Writing to a file"},{"location":"examples/hello-world.html#introducing-our-built-in-context-variables","text":"$BATCH: This is the data package passed to the next node. $BATCH_NUM: This is the iterator number if you are batching (queries for example). $HOOK: This represents your hook - you can use !Hook my_conn as syntax sugar or !Py $HOOK.myconn $VARIABLE: This is how you can access saved variables e.g. lists of tables, query templates etc. An example might be !Py $VARIABLE.mysql_read_3.format(table_name=$BATCH) $DAG_CONTEXT.interval_start & $DAG_CONTEXT.interval_end: Execution interval for example: $DAG_CONTEXT.interval_start \u2192 '2021-05-23 10:00:00' (inclusive) $DAG_CONTEXT.interval_end \u2192 '2021-05-23 11:00:00' (exclusive)","title":"Introducing our built in context variables"},{"location":"examples/hello-world.html#passing-data","text":"The notation !Py indicates that what comes after is evaluated as normal python code. $BATCH is the data packet as a consistent default reference. It can be any type so depending on your data can be shaped differently e.g. $BATCH['key], $BATCH[0] etc. In our case remember each branch yields a list of dictionaries with two keys [{'filename': 'users.txt', 'contents':'['John', 'Amy', 'Adam', 'Jane']},{'filename':...}] The most complex item here is a !Multistep process showing how you can do multiple small python transformations in series in a a nice readable way (or you can put it in one line of course): !Py $BATCH['filename'] will evaluate to \"users.txt' !Py $DAG_CONTEXT.interval_end will evaluate to the timestamp of the DAG run. This is a built in context variable. !Py f'/store/{$2}/{$1}' finally this references the first two lines (1, 2) and uses a normal Python f-string to make the path. Evaluating to '/store/ 2021-03-13T12:00:00/users.txt' Lastly we want to deliver the right data, which is the 'contents' key: data: !Py $BATCH['contents'] Here is our result! Animals and fruits, along with our users!","title":"Passing data"},{"location":"examples/mysql-to-snowflake.html","text":"MySql to Snowflake Now we know the basics, lets make something more useful. For brevity the full connections YAML is included below for you to copy and replace your keys. However, we will be quite thorough to make sure each step is clear. In this example, I need to take two tables ['clients', 'sales'] from our production MySql (replace with your favourite RDBMS) to Snowflake (replace with your favourite cloud DWH). Sample data for MySQL is included at the bottom. We are going to load our data into a VARIANT field in Snowflake as this pattern is very fault tolerant . Overview of what we want to do: List the tables from Typhoon variables. Extract each table from MySQL Write each table to S3 in JSON Creating a function to output to JSON Switching our connection to S3 Copy the files into snowflake DAG : list_tables \u2192 extract_tables \u2192 write_data_S3 \u2192 copy_to_snowflake_stage We will learn how to sample and test the flows as we build. Here is the final expected output: name : dwh_flow schedule_interval : rate(1 hours) granularity : hour tasks : list_tables : function : typhoon.flow_control.branch args : branches : - clients - sales extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Hook transaction_db batch_size : 10 query_params : interval_start : !Py $DAG_CONTEXT.interval_start interval_end : !Py $DAG_CONTEXT.interval_end metadata : table_name : !Py $BATCH query : !MultiStep - !Py table_name=$BATCH - !Py f\"SELECT * FROM {table_name} where creation_timestamp between %(interval_start)s and %(interval_end)s\" write_data_S3 : input : extract_tables function : typhoon.filesystem.write_data args : hook : !Py $HOOK.data_lake data : !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) path : !Py f\"data_{$BATCH.metadata['table_name']}_batch_num_\" + str($BATCH_NUM) + \"_\" + str($DAG_CONTEXT.interval_end).replace(\":\", \"_\") + \".json\" metadata : !Py $BATCH.metadata copy_to_snowflake_stage : input : write_data_S3 function : typhoon.snowflake.copy_into args : hook : !Py $HOOK.snowflake table : !Py $BATCH.metadata['table_name'] stage_name : stagetestcorpdatalake s3_path : '' Lets create a file in the /my_project_root/dags folder called dwh_flow.yml . List_tables name : dwh_flow schedule_interval : rate(1 hours) add granularity : hour tasks : list_tables : function : typhoon.flow_control.branch args : branches : - clients - sales echo_my_tables : input : list_tables function : typhoon.debug.echo args : mydata : !Py $BATCH Life is simple when you only have two tables - if only! Let's extract these hourly. We will improve this with variables later. Let's run typhoon status , build it and run it: typhoon dag build dwh_flow typhoon dag run --dag-name dwh_flow You can see we are simply outputting the tables in the list. Extracting from MySQL extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Py $HOOK.transaction_db batch_size : 10 metadata : table_name : !Py $BATCH query : !Py \"SELECT * FROM {table_name}\" .format(table_name=$BATCH) Ok, here there are a few things going on: We are applying relational.execute_query on each table to query on our $HOOK.transaction_db connection. This is pointing to our MySQL database that records our customer transactions. We are saving the table names for use downstream in the flow in our metadata dictionary. We are formatting our query using normal python. Again, later in this tutorial we will use Variables to store our dynamic SQL . Ok, so we will output the whole table each hour. Perhaps not so smart , so lets add something more reasonable. extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Hook echodb batch_size : 10 query_params : interval_start : !Py $DAG_CONTEXT.interval_start interval_end : !Py $DAG_CONTEXT.interval_end metadata : table_name : !Py $BATCH query : !MultiStep - !Py table_name=$BATCH - !Py f\"SELECT * FROM {table_name} where creation_timestamp between %(interval_start)s and %(interval_end)s\" Here we are making two changes. The obvious one is to apply the $DAG_CONTEXT.interval_start and $DAG_CONTEXT.interval_end as query parameters in the query (note its MySQL paramstyle). This means we will pull those transactions created within the hour interval. Much more sensible! Note: you could of course treat this like the table as a string format (but not the other way around; you cannot use query parameters for the table or schema). The second thing is we are temporarily using a new hook: echodb . This is similar to our echo task but will show us the sql that would be passed to MySQL. We need to add the connection (make sure its in the connection.yml which is available to copy at the bottom). typhoon connection add --conn-id echodb --conn-env local Looks correct SQL. Remember to change your hook back to hook: !Py $HOOK.transaction_db Lets add our Mysql hook and run to an echo task: typhoon connection add --conn-id transaction_db --conn-env prod Because we are developing we might want to select a specific execution date: typhoon dag build dwh_flow typhoon status typhoon dag run --dag-name dwh_flow --execution-date 2021-05-20T14:00:00 Our (faked) data from the MySql echoed Do so now its outputting our selected tables within the time interval in batches of 10 rows (of course you would raise this). Next lets land this in files to S3. Write each table to S3 in JSON write_data_S3 : input : extract_tables function : typhoon.filesystem.write_data args : hook : !Py $HOOK.data_lake data : !Py $BATCH.batch path : !Py f\"data_{$BATCH.metadata['table_name']}_batch_num_\" + str($BATCH_NUM) + \"_\" + str($DAG_CONTEXT.ts).replace(\":\", \"_\") + \".json\" metadata : !Py $BATCH.metadata Ok, now its starting to look familiar (I hope). Our function filesystem.write_data will write to a filesystem. Notice this is the same connection YAML used in hello_world. All our functions, connections and components are re-usable. Lets start with the local environment to test it to a local filepath. typhoon connection add --conn-id data_lake --conn-env **local** Running this we should now see the following (see the connection for where this will land): So we must transform the data to bytes. In fact we need to write it into JSON so that we can use Snowflake's VARIANT column for ingestion. We strongly recommend this pattern. We are passing this function data from the extract_tables task above. MySQL outputs some tuples so we need a transformation function that will turn these tuples into JSON (1 JSON object per row). (Optional) reating a function to output to JSON Info This step is optional as it already is included in typhoon. So its only to illustrate how to roll your own function. So, let's make our own function for this. In the transformations folder we can add function to the data file (or you can make a new one): Typhoon is easily extensible. from io import StringIO , BytesIO from typing import Union import simplejson as json def list_of_tuples_to_json ( data : Union [ list ], field_names : list ): list_of_dicts = [ dict ( zip ( field_names , tup )) for tup in data ] d_out = BytesIO () for d in list_of_dicts : d_out . write ( json . dumps ( d , cls = TestEncoder , namedtuple_as_object = True ) . encode ()) d_out . seek ( 0 ) return d_out Then we can call this in the DAG by replacing data with: data: !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) So now, before we write to the file we are transforming our tuples to return rows of json doc as bytes. Building and running the DAG now gives us: So lets see how we can debug the Python code directly. One key advantage of developing in Typhoon is that the YAML compiles to very readable, normal python functions. You can debug in an IDE of your choice as normal or run from prompt. Building a DAG compiles to the 'out' folder Note that we can set the example task to debug with in dwh_flow.py . For example here we are setting a test execution date. by altering the example_event datetime. if __name__ == '__main__' : import os ** example_event = { 'time' : '2021-05-20T14:00:00Z' } ** example_event_task = { 'type' : 'task' , 'dag_name' : '' , 'task_name' : 'list_tables_branches' , 'trigger' : 'dag' , 'attempt' : 1 , 'args' : [], 'kwargs' : { 'dag_context' : DagContext . from_cron_and_event_time ( schedule_interval = 'rate(1 hours)' , event_time = example_event [ 'time' ], granularity = 'hour' , ) . dict () }, } dwh_flow_main ( example_event , None ) We can find the point in the code that is relevant for our JSON config [ 'data' ] = transformations . data . list_of_tuples_to_json ( list ( batch . batch ), batch . columns ) So you can develop your function to correctly encode the datetime: from io import StringIO , BytesIO from typing import Union import pandas as pd import simplejson as json from datetime import datetime class DateTimeEncoder ( json . JSONEncoder ): def default ( self , obj ): if hasattr ( obj , 'isoformat' ): return obj . isoformat () else : return json . JSONEncoder . default ( self , obj ) def list_of_tuples_to_json ( data : Union [ list ], field_names : list ): list_of_dicts = [ dict ( zip ( field_names , tup )) for tup in data ] d_out = BytesIO () for d in list_of_dicts : d_out . write ( json . dumps ( d , cls = DateTimeEncoder , namedtuple_as_object = True ) . encode ()) d_out . write ( str ( ' \\n ' ) . encode ()) d_out . seek ( 0 ) return d_out Then we should add this to our YAML: data: !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) This is passing $BATCH.batch and the $BATCH.columns to this function that we will add. It is very simply transforming tuples from the resultset to JSON using our new function above. Now if we run it again, either in the cli or the dwh_flow.py directly we should get an output to our local filesystem like: Our (fake) data exported as JSON in batches of 10 rows Switching to S3 The final step of this section is to switch the connection to our S3 bucket (i.e. production): typhoon connection add --conn-id data_lake --conn-env **prod** When we re-run the DAG we will see the results are now landing in S3 (see YAML connections at end of this for hints on getting the connection right). And in the bucket : Copy data to Snowflake Ok, now for the final step - we need to upload these files to snowflake. To prepare snowflake to receive from S3 you need to set up an S3 Stage ( See Snowflake docs) . Let's add our final task to the DAG YAML: copy_to_snowflake_stage : input : write_data_S3 function : typhoon.snowflake.copy_into args : hook : !Py $HOOK.snowflake table : !Py $BATCH.metadata['table_name'] stage_name : stagetestcorpdatalake s3_path : '' Before we run it we need to create our staging tables. We are using a VARIANT loading field to land the data: CREATE OR REPLACE TABLE clients ( data_var variant , etl_id varchar ( 100 ), etl_timestamp datetime , etl_filename varchar ( 100 ) ) ; CREATE OR REPLACE TABLE sales ( data_var variant , etl_id varchar ( 100 ), etl_timestamp datetime , etl_filename varchar ( 100 ) ) ; We need to add our connection (see YAML at the end for the connection): typhoon connection add --conn-id snowflake --conn-env prod Now we can build our flow for the final time and run it typhoon dag build dwh_flow typhoon dag run --dag-name dwh_flow --execution-date 2021-05-20T14:00:00 ... and we have data in Snowflake! Some potential optimisations We are running COPY for each file we land. This isn't optimal, even though snowflake is protecting against duplicating the data. We can tackle this chaining together DAGS by writing a 'success file' . Listing tables in a variable More templating Creating a Component! We will cover these in other blogs. For now...well done you made it! Helper Assets YAML connections echo : local : conn_type : echo echodb : local : conn_type : echodb schema : main data_lake : prod : conn_type : s3 extra : base_path : / bucket : your-bucket-here region_name : your-region-here login : SECRETLOGIN passowrd : SECRETKEY local : conn_type : local_storage extra : base_path : /tmp/data_lake transaction_db : dev : conn_type : sqlite schema : main extra : database : /tmp/web_trans_local.db prod : conn_type : sqlalchemy login : your-login-name password : your-password host : localhost port : 3306 schema : prod_web_ecom extra : dialect : mysql driver : mysqldb database : prod_web_ecom snowflake : prod : conn_type : snowflake login : your-login password : your-password schema : staging extra : account : your-account region : your-region database : your-db warehouse : WAREHOUSE_NAME role : your-role Mock data Sample MySQL Data.zip","title":"MySql to Snowflake"},{"location":"examples/mysql-to-snowflake.html#mysql-to-snowflake","text":"Now we know the basics, lets make something more useful. For brevity the full connections YAML is included below for you to copy and replace your keys. However, we will be quite thorough to make sure each step is clear. In this example, I need to take two tables ['clients', 'sales'] from our production MySql (replace with your favourite RDBMS) to Snowflake (replace with your favourite cloud DWH). Sample data for MySQL is included at the bottom. We are going to load our data into a VARIANT field in Snowflake as this pattern is very fault tolerant . Overview of what we want to do: List the tables from Typhoon variables. Extract each table from MySQL Write each table to S3 in JSON Creating a function to output to JSON Switching our connection to S3 Copy the files into snowflake DAG : list_tables \u2192 extract_tables \u2192 write_data_S3 \u2192 copy_to_snowflake_stage We will learn how to sample and test the flows as we build. Here is the final expected output: name : dwh_flow schedule_interval : rate(1 hours) granularity : hour tasks : list_tables : function : typhoon.flow_control.branch args : branches : - clients - sales extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Hook transaction_db batch_size : 10 query_params : interval_start : !Py $DAG_CONTEXT.interval_start interval_end : !Py $DAG_CONTEXT.interval_end metadata : table_name : !Py $BATCH query : !MultiStep - !Py table_name=$BATCH - !Py f\"SELECT * FROM {table_name} where creation_timestamp between %(interval_start)s and %(interval_end)s\" write_data_S3 : input : extract_tables function : typhoon.filesystem.write_data args : hook : !Py $HOOK.data_lake data : !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) path : !Py f\"data_{$BATCH.metadata['table_name']}_batch_num_\" + str($BATCH_NUM) + \"_\" + str($DAG_CONTEXT.interval_end).replace(\":\", \"_\") + \".json\" metadata : !Py $BATCH.metadata copy_to_snowflake_stage : input : write_data_S3 function : typhoon.snowflake.copy_into args : hook : !Py $HOOK.snowflake table : !Py $BATCH.metadata['table_name'] stage_name : stagetestcorpdatalake s3_path : '' Lets create a file in the /my_project_root/dags folder called dwh_flow.yml .","title":"MySql to Snowflake"},{"location":"examples/mysql-to-snowflake.html#list_tables","text":"name : dwh_flow schedule_interval : rate(1 hours) add granularity : hour tasks : list_tables : function : typhoon.flow_control.branch args : branches : - clients - sales echo_my_tables : input : list_tables function : typhoon.debug.echo args : mydata : !Py $BATCH Life is simple when you only have two tables - if only! Let's extract these hourly. We will improve this with variables later. Let's run typhoon status , build it and run it: typhoon dag build dwh_flow typhoon dag run --dag-name dwh_flow You can see we are simply outputting the tables in the list.","title":"List_tables"},{"location":"examples/mysql-to-snowflake.html#extracting-from-mysql","text":"extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Py $HOOK.transaction_db batch_size : 10 metadata : table_name : !Py $BATCH query : !Py \"SELECT * FROM {table_name}\" .format(table_name=$BATCH) Ok, here there are a few things going on: We are applying relational.execute_query on each table to query on our $HOOK.transaction_db connection. This is pointing to our MySQL database that records our customer transactions. We are saving the table names for use downstream in the flow in our metadata dictionary. We are formatting our query using normal python. Again, later in this tutorial we will use Variables to store our dynamic SQL . Ok, so we will output the whole table each hour. Perhaps not so smart , so lets add something more reasonable. extract_tables : input : list_tables function : typhoon.relational.execute_query args : hook : !Hook echodb batch_size : 10 query_params : interval_start : !Py $DAG_CONTEXT.interval_start interval_end : !Py $DAG_CONTEXT.interval_end metadata : table_name : !Py $BATCH query : !MultiStep - !Py table_name=$BATCH - !Py f\"SELECT * FROM {table_name} where creation_timestamp between %(interval_start)s and %(interval_end)s\" Here we are making two changes. The obvious one is to apply the $DAG_CONTEXT.interval_start and $DAG_CONTEXT.interval_end as query parameters in the query (note its MySQL paramstyle). This means we will pull those transactions created within the hour interval. Much more sensible! Note: you could of course treat this like the table as a string format (but not the other way around; you cannot use query parameters for the table or schema). The second thing is we are temporarily using a new hook: echodb . This is similar to our echo task but will show us the sql that would be passed to MySQL. We need to add the connection (make sure its in the connection.yml which is available to copy at the bottom). typhoon connection add --conn-id echodb --conn-env local Looks correct SQL. Remember to change your hook back to hook: !Py $HOOK.transaction_db Lets add our Mysql hook and run to an echo task: typhoon connection add --conn-id transaction_db --conn-env prod Because we are developing we might want to select a specific execution date: typhoon dag build dwh_flow typhoon status typhoon dag run --dag-name dwh_flow --execution-date 2021-05-20T14:00:00 Our (faked) data from the MySql echoed Do so now its outputting our selected tables within the time interval in batches of 10 rows (of course you would raise this). Next lets land this in files to S3.","title":"Extracting from MySQL"},{"location":"examples/mysql-to-snowflake.html#write-each-table-to-s3-in-json","text":"write_data_S3 : input : extract_tables function : typhoon.filesystem.write_data args : hook : !Py $HOOK.data_lake data : !Py $BATCH.batch path : !Py f\"data_{$BATCH.metadata['table_name']}_batch_num_\" + str($BATCH_NUM) + \"_\" + str($DAG_CONTEXT.ts).replace(\":\", \"_\") + \".json\" metadata : !Py $BATCH.metadata Ok, now its starting to look familiar (I hope). Our function filesystem.write_data will write to a filesystem. Notice this is the same connection YAML used in hello_world. All our functions, connections and components are re-usable. Lets start with the local environment to test it to a local filepath. typhoon connection add --conn-id data_lake --conn-env **local** Running this we should now see the following (see the connection for where this will land): So we must transform the data to bytes. In fact we need to write it into JSON so that we can use Snowflake's VARIANT column for ingestion. We strongly recommend this pattern. We are passing this function data from the extract_tables task above. MySQL outputs some tuples so we need a transformation function that will turn these tuples into JSON (1 JSON object per row).","title":"Write each table to S3 in JSON"},{"location":"examples/mysql-to-snowflake.html#optional-reating-a-function-to-output-to-json","text":"Info This step is optional as it already is included in typhoon. So its only to illustrate how to roll your own function. So, let's make our own function for this. In the transformations folder we can add function to the data file (or you can make a new one): Typhoon is easily extensible. from io import StringIO , BytesIO from typing import Union import simplejson as json def list_of_tuples_to_json ( data : Union [ list ], field_names : list ): list_of_dicts = [ dict ( zip ( field_names , tup )) for tup in data ] d_out = BytesIO () for d in list_of_dicts : d_out . write ( json . dumps ( d , cls = TestEncoder , namedtuple_as_object = True ) . encode ()) d_out . seek ( 0 ) return d_out Then we can call this in the DAG by replacing data with: data: !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) So now, before we write to the file we are transforming our tuples to return rows of json doc as bytes. Building and running the DAG now gives us: So lets see how we can debug the Python code directly. One key advantage of developing in Typhoon is that the YAML compiles to very readable, normal python functions. You can debug in an IDE of your choice as normal or run from prompt. Building a DAG compiles to the 'out' folder Note that we can set the example task to debug with in dwh_flow.py . For example here we are setting a test execution date. by altering the example_event datetime. if __name__ == '__main__' : import os ** example_event = { 'time' : '2021-05-20T14:00:00Z' } ** example_event_task = { 'type' : 'task' , 'dag_name' : '' , 'task_name' : 'list_tables_branches' , 'trigger' : 'dag' , 'attempt' : 1 , 'args' : [], 'kwargs' : { 'dag_context' : DagContext . from_cron_and_event_time ( schedule_interval = 'rate(1 hours)' , event_time = example_event [ 'time' ], granularity = 'hour' , ) . dict () }, } dwh_flow_main ( example_event , None ) We can find the point in the code that is relevant for our JSON config [ 'data' ] = transformations . data . list_of_tuples_to_json ( list ( batch . batch ), batch . columns ) So you can develop your function to correctly encode the datetime: from io import StringIO , BytesIO from typing import Union import pandas as pd import simplejson as json from datetime import datetime class DateTimeEncoder ( json . JSONEncoder ): def default ( self , obj ): if hasattr ( obj , 'isoformat' ): return obj . isoformat () else : return json . JSONEncoder . default ( self , obj ) def list_of_tuples_to_json ( data : Union [ list ], field_names : list ): list_of_dicts = [ dict ( zip ( field_names , tup )) for tup in data ] d_out = BytesIO () for d in list_of_dicts : d_out . write ( json . dumps ( d , cls = DateTimeEncoder , namedtuple_as_object = True ) . encode ()) d_out . write ( str ( ' \\n ' ) . encode ()) d_out . seek ( 0 ) return d_out Then we should add this to our YAML: data: !Py transformations.data.list_of_tuples_to_json(list($BATCH.batch), $BATCH.columns) This is passing $BATCH.batch and the $BATCH.columns to this function that we will add. It is very simply transforming tuples from the resultset to JSON using our new function above. Now if we run it again, either in the cli or the dwh_flow.py directly we should get an output to our local filesystem like: Our (fake) data exported as JSON in batches of 10 rows","title":"(Optional) reating a function to output to JSON"},{"location":"examples/mysql-to-snowflake.html#switching-to-s3","text":"The final step of this section is to switch the connection to our S3 bucket (i.e. production): typhoon connection add --conn-id data_lake --conn-env **prod** When we re-run the DAG we will see the results are now landing in S3 (see YAML connections at end of this for hints on getting the connection right). And in the bucket :","title":"Switching to S3"},{"location":"examples/mysql-to-snowflake.html#copy-data-to-snowflake","text":"Ok, now for the final step - we need to upload these files to snowflake. To prepare snowflake to receive from S3 you need to set up an S3 Stage ( See Snowflake docs) . Let's add our final task to the DAG YAML: copy_to_snowflake_stage : input : write_data_S3 function : typhoon.snowflake.copy_into args : hook : !Py $HOOK.snowflake table : !Py $BATCH.metadata['table_name'] stage_name : stagetestcorpdatalake s3_path : '' Before we run it we need to create our staging tables. We are using a VARIANT loading field to land the data: CREATE OR REPLACE TABLE clients ( data_var variant , etl_id varchar ( 100 ), etl_timestamp datetime , etl_filename varchar ( 100 ) ) ; CREATE OR REPLACE TABLE sales ( data_var variant , etl_id varchar ( 100 ), etl_timestamp datetime , etl_filename varchar ( 100 ) ) ; We need to add our connection (see YAML at the end for the connection): typhoon connection add --conn-id snowflake --conn-env prod Now we can build our flow for the final time and run it typhoon dag build dwh_flow typhoon dag run --dag-name dwh_flow --execution-date 2021-05-20T14:00:00 ... and we have data in Snowflake! Some potential optimisations We are running COPY for each file we land. This isn't optimal, even though snowflake is protecting against duplicating the data. We can tackle this chaining together DAGS by writing a 'success file' . Listing tables in a variable More templating Creating a Component! We will cover these in other blogs. For now...well done you made it!","title":"Copy data to Snowflake"},{"location":"examples/mysql-to-snowflake.html#helper-assets","text":"","title":"Helper Assets"},{"location":"examples/mysql-to-snowflake.html#yaml-connections","text":"echo : local : conn_type : echo echodb : local : conn_type : echodb schema : main data_lake : prod : conn_type : s3 extra : base_path : / bucket : your-bucket-here region_name : your-region-here login : SECRETLOGIN passowrd : SECRETKEY local : conn_type : local_storage extra : base_path : /tmp/data_lake transaction_db : dev : conn_type : sqlite schema : main extra : database : /tmp/web_trans_local.db prod : conn_type : sqlalchemy login : your-login-name password : your-password host : localhost port : 3306 schema : prod_web_ecom extra : dialect : mysql driver : mysqldb database : prod_web_ecom snowflake : prod : conn_type : snowflake login : your-login password : your-password schema : staging extra : account : your-account region : your-region database : your-db warehouse : WAREHOUSE_NAME role : your-role","title":"YAML connections"},{"location":"examples/mysql-to-snowflake.html#mock-data","text":"Sample MySQL Data.zip","title":"Mock data"},{"location":"getting-started/connections.html","text":"Connections During development, we may want to use different connections to test our DAGs. Also, for the same connections we might want different details per environment . For example, we may start development writing to a local file and after that is working correctly switch the connection to S3 to finish testing. Finally we want to move this to the production S3 bucket in a seamless way. data_lake workflow local -> disk dev -> dev S3 bucket prod -> production S3 bucket For this specific purpose there is a file called connections.yml . This is where we will define all our connection details for our project for all environments. Connections YAML For example in this project we will write data using a connection called data_lake which has two connection environments, local which writes to a local file and test which writes to S3. It is possible to use them interchangeably since they both implement the same interface: FileSystemHook . connections.yml: data_lake : local : conn_type : local_storage extra : base_path : /tmp/data_lake/ test : conn_type : s3 extra : bucket : my-typhoon-test-bucket connections.yml is not versioned The connections.yml file may contain passwords so it should never be versioned. That is why it's included in the .gitignore file for projects generated with the CLI. Adding the connection Following the advice we got from the typhoon status command we will now add the connection to the metadata database. We will add the local data_lake connection with the command: typhoon connection add --conn-id data_lake --conn-env local # Check that it was added typhoon connection ls -l # add the echo one too for completeness typhoon connection add --conn-id echo --conn-env local If we run the status command again we will see that everything is ok in our project now: typhoon status Connections types (Hooks) Typhoon hooks represent the same thing as airlflow hooks , so if you are familiar it's an easy concept. They are the interface to external platforms and databases. You use the connections.yml to choose which hook to use and configure it. You can extend typhoon to connect to new platforms by adding Hooks. It comes with many popular ones already. Hooks available: File System - local, S3, GC storage, Ftp DB API - most DBs can use this, e.g. MySql, MSSQL, Postgres, Redshift Snowflake specific flavour = SQLAlchemy - most DBs can use this AWS - AWS Session, DynamoDB Singer - so you can use anything singer can connect to Any you can add/extend your own, of course.","title":"Connections"},{"location":"getting-started/connections.html#connections","text":"During development, we may want to use different connections to test our DAGs. Also, for the same connections we might want different details per environment . For example, we may start development writing to a local file and after that is working correctly switch the connection to S3 to finish testing. Finally we want to move this to the production S3 bucket in a seamless way. data_lake workflow local -> disk dev -> dev S3 bucket prod -> production S3 bucket For this specific purpose there is a file called connections.yml . This is where we will define all our connection details for our project for all environments.","title":"Connections"},{"location":"getting-started/connections.html#connections-yaml","text":"For example in this project we will write data using a connection called data_lake which has two connection environments, local which writes to a local file and test which writes to S3. It is possible to use them interchangeably since they both implement the same interface: FileSystemHook . connections.yml: data_lake : local : conn_type : local_storage extra : base_path : /tmp/data_lake/ test : conn_type : s3 extra : bucket : my-typhoon-test-bucket connections.yml is not versioned The connections.yml file may contain passwords so it should never be versioned. That is why it's included in the .gitignore file for projects generated with the CLI.","title":"Connections YAML"},{"location":"getting-started/connections.html#adding-the-connection","text":"Following the advice we got from the typhoon status command we will now add the connection to the metadata database. We will add the local data_lake connection with the command: typhoon connection add --conn-id data_lake --conn-env local # Check that it was added typhoon connection ls -l # add the echo one too for completeness typhoon connection add --conn-id echo --conn-env local If we run the status command again we will see that everything is ok in our project now: typhoon status","title":"Adding the connection"},{"location":"getting-started/connections.html#connections-types-hooks","text":"Typhoon hooks represent the same thing as airlflow hooks , so if you are familiar it's an easy concept. They are the interface to external platforms and databases. You use the connections.yml to choose which hook to use and configure it. You can extend typhoon to connect to new platforms by adding Hooks. It comes with many popular ones already. Hooks available: File System - local, S3, GC storage, Ftp DB API - most DBs can use this, e.g. MySql, MSSQL, Postgres, Redshift Snowflake specific flavour = SQLAlchemy - most DBs can use this AWS - AWS Session, DynamoDB Singer - so you can use anything singer can connect to Any you can add/extend your own, of course.","title":"Connections types (Hooks)"},{"location":"getting-started/creating-a-dag.html","text":"Creating a DAG Simply, the DAG is the workflow task you are composing. This is a DAG structure similar to many other workflow tools (e.g. Task A \u2192 Task B \u2192 Task C). However, the writing of a DAG in Typhoon can be much quicker and more intuitive due to the simpler YAML syntax and that it shares data. Concepts: tasks: this defines each node (or step) in the DAG workflow (i.e. do something). functions : this is the operation to carry out (e.g. read data, write data, branches, if, etc.). transformations : stored functions that are re-usable and testable (e.g. gzip, dicts_to_csv). components : re-usable sets of multi-task DAGS (e.g. glob_compress_and_copy, if, db_to_swowflake). Can be used as standalone or in a task. DAG code We can check an example DAG in dags/hello_world.yml . You can also check it with the CLI by running typhoon dag definition --dag-name hello_world . Tip To get some editor completion and hints you can configure the JSON schema in dag_schema.json as the schema for the DAG YAML file. This can be done in VS Code with the Red Hat YAML extension or in PyCharm natively . name : hello_world schedule_interval : rate(1 hours) # Can also be a Cron expression granularity : hour tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True This DAG has three branches, each with a file name and list (YAML list of dictionaries). This will write 3 files; users.txt, animals.txt and fruits.csv. Typically, this first step might be a database, set of CSVs or an API. This is just a trivial example including data. Skim the code For a step-by-step on this example - please start on our example Hello World walkthrough . DAG basic concepts The DAGs are composed in YAML . They are compiled to python. This means that once you have written your YAML and you build it the output is a normal, testable python file. It transpiles to Airflow compatible DAGS also. Basic components: name: ... schedule_interval: can be a Rate or a Cron expression to express how often it should run. granularity: minutes, hour, hours, day, days ... or a Cron expression. tasks: this defines each node (or step) in the DAG workflow (i.e. do something) input: here we are connecting the input from the previous task (A -> B; B.input = A). function: this is the operation to carry out (e.g. read data, write data, branches, if, etc.). Full function list for reference. args: these are the specific arguments for the function (so they will differ). Common ones are below: hook: this is the connection to use that you have added from your connectioons.yml path: this is the path, typical in filesystem hooks to read / write files data: this is the batch of data we are passing between tasks, referenced by $BATCH Syntax sugars: - !Hook: e.g. !Hook data_lake is equivilent of $HOOK.data_lake . I.e. to get the connection. - !MultiStep: allows you to make multi-line scripts easily (see example). Useful for chaining together a few Pandas transformations for example. - !Py: this evaluates the line as normal python (allowing total flexibility therefore to apply your own transformations). Running the DAG We can run this with: typhoon dag run --dag-name hello_world We can check the files that have been written to /tmp/data_lake/store/ where /tmp/data_lake/ was the base directory defined for our connection (go back to connections.yml to check) and the DAG wrote to /store/[FILE_NAME] .","title":"Creating your DAG"},{"location":"getting-started/creating-a-dag.html#creating-a-dag","text":"Simply, the DAG is the workflow task you are composing. This is a DAG structure similar to many other workflow tools (e.g. Task A \u2192 Task B \u2192 Task C). However, the writing of a DAG in Typhoon can be much quicker and more intuitive due to the simpler YAML syntax and that it shares data. Concepts: tasks: this defines each node (or step) in the DAG workflow (i.e. do something). functions : this is the operation to carry out (e.g. read data, write data, branches, if, etc.). transformations : stored functions that are re-usable and testable (e.g. gzip, dicts_to_csv). components : re-usable sets of multi-task DAGS (e.g. glob_compress_and_copy, if, db_to_swowflake). Can be used as standalone or in a task.","title":"Creating a DAG"},{"location":"getting-started/creating-a-dag.html#dag-code","text":"We can check an example DAG in dags/hello_world.yml . You can also check it with the CLI by running typhoon dag definition --dag-name hello_world . Tip To get some editor completion and hints you can configure the JSON schema in dag_schema.json as the schema for the DAG YAML file. This can be done in VS Code with the Red Hat YAML extension or in PyCharm natively . name : hello_world schedule_interval : rate(1 hours) # Can also be a Cron expression granularity : hour tasks : send_data : function : typhoon.flow_control.branch args : branches : - filename : users.txt contents : John, Amy, Adam, Jane - filename : animals.txt contents : dog, cat, mouse, elephant, giraffe - filename : fruits.csv contents : apple,pear,apricot write_data : input : send_data function : typhoon.filesystem.write_data args : hook : !Hook data_lake path : !MultiStep - !Py $BATCH['filename'] - !Py $DAG_CONTEXT.interval_end - !Py f'/store/{$2}/{$1}' data : !Py $BATCH['contents'] create_intermediate_dirs : True This DAG has three branches, each with a file name and list (YAML list of dictionaries). This will write 3 files; users.txt, animals.txt and fruits.csv. Typically, this first step might be a database, set of CSVs or an API. This is just a trivial example including data. Skim the code For a step-by-step on this example - please start on our example Hello World walkthrough .","title":"DAG code"},{"location":"getting-started/creating-a-dag.html#dag-basic-concepts","text":"The DAGs are composed in YAML . They are compiled to python. This means that once you have written your YAML and you build it the output is a normal, testable python file. It transpiles to Airflow compatible DAGS also. Basic components: name: ... schedule_interval: can be a Rate or a Cron expression to express how often it should run. granularity: minutes, hour, hours, day, days ... or a Cron expression. tasks: this defines each node (or step) in the DAG workflow (i.e. do something) input: here we are connecting the input from the previous task (A -> B; B.input = A). function: this is the operation to carry out (e.g. read data, write data, branches, if, etc.). Full function list for reference. args: these are the specific arguments for the function (so they will differ). Common ones are below: hook: this is the connection to use that you have added from your connectioons.yml path: this is the path, typical in filesystem hooks to read / write files data: this is the batch of data we are passing between tasks, referenced by $BATCH Syntax sugars: - !Hook: e.g. !Hook data_lake is equivilent of $HOOK.data_lake . I.e. to get the connection. - !MultiStep: allows you to make multi-line scripts easily (see example). Useful for chaining together a few Pandas transformations for example. - !Py: this evaluates the line as normal python (allowing total flexibility therefore to apply your own transformations).","title":"DAG basic concepts"},{"location":"getting-started/creating-a-dag.html#running-the-dag","text":"We can run this with: typhoon dag run --dag-name hello_world We can check the files that have been written to /tmp/data_lake/store/ where /tmp/data_lake/ was the base directory defined for our connection (go back to connections.yml to check) and the DAG wrote to /store/[FILE_NAME] .","title":"Running the DAG"},{"location":"getting-started/creating-components.html","text":"Creating Components @todo - extended guide on making a Pandas example @todo - extended guide on making a csv exporter to ftp @todo - extended guide on making a csv import to db","title":"Creating Components"},{"location":"getting-started/creating-components.html#creating-components","text":"@todo - extended guide on making a Pandas example @todo - extended guide on making a csv exporter to ftp @todo - extended guide on making a csv import to db","title":"Creating Components"},{"location":"getting-started/creating-hooks.html","text":"Creating Hooks (Adding New Connections) @Todo - when you need to add hooks and how to check - all DB api ones or mysql - example - all the singer connections - example @Todo - when you need to Create a Hook - or could it be a function if its an API? @how to create a new hook (e.g. ) - elastic search or mongo or mysql","title":"Hooks (connections)"},{"location":"getting-started/creating-hooks.html#creating-hooks-adding-new-connections","text":"@Todo - when you need to add hooks and how to check - all DB api ones or mysql - example - all the singer connections - example @Todo - when you need to Create a Hook - or could it be a function if its an API? @how to create a new hook (e.g. ) - elastic search or mongo or mysql","title":"Creating Hooks (Adding New Connections)"},{"location":"getting-started/deployment.html","text":"Deployment airflow docker how to start testing - how to productionise it","title":"Deployment"},{"location":"getting-started/deployment.html#deployment","text":"airflow docker how to start testing - how to productionise it","title":"Deployment"},{"location":"getting-started/installation.html","text":"Installation Typhoon can be installed locally with pip or using docker. To test airflow (especially on Windows) we recommend using the docker version. Use the DEV version when installing it locally. The [dev] version comes with all the libraries and tools that make development easier. ` pip install typhoon - orchestrator [ dev ] ` The production version is lightweight for use with Lambda. Installation with pip pip install typhoon-orchestrator [ dev ] Creating your new project Inside your terminal navigate to where you want to create your new project directory. Then run: typhoon init hello_world cd hello_world This will create a directory named hello_world that serves as an example project. As in git, when we cd into the directory it will detect that it's a Typhoon project and consider that directory the base directory for Typhoon (TYPHOON_HOME). Checking 'typhoon status' typhoon status Result: We can see that it's detecting the project home as well as a SQLite metadata database that just got created. It's also warning us that our DAG uses a connection that is not defined in the metadata database and suggesting us a fix. We will see in the next section 'Connections' how to add these. Bash/ZSH/Fish auto-complete bash eval \"$(_TYPHOON_COMPLETE=source_bash typhoon)\" zsh eval \"$(_TYPHOON_COMPLETE=source_zsh typhoon)\" fish eval \"$(_TYPHOON_COMPLETE=source_fish typhoon)\" with docker @TODO DOCKER update - also in Readme. with docker and Airflow @TODO DOCKER update - also in Readme.","title":"Installation"},{"location":"getting-started/installation.html#installation","text":"Typhoon can be installed locally with pip or using docker. To test airflow (especially on Windows) we recommend using the docker version. Use the DEV version when installing it locally. The [dev] version comes with all the libraries and tools that make development easier. ` pip install typhoon - orchestrator [ dev ] ` The production version is lightweight for use with Lambda.","title":"Installation"},{"location":"getting-started/installation.html#installation_1","text":"","title":"Installation"},{"location":"getting-started/installation.html#with-pip","text":"pip install typhoon-orchestrator [ dev ]","title":"with pip"},{"location":"getting-started/installation.html#creating-your-new-project","text":"Inside your terminal navigate to where you want to create your new project directory. Then run: typhoon init hello_world cd hello_world This will create a directory named hello_world that serves as an example project. As in git, when we cd into the directory it will detect that it's a Typhoon project and consider that directory the base directory for Typhoon (TYPHOON_HOME).","title":"Creating your new project"},{"location":"getting-started/installation.html#checking-typhoon-status","text":"typhoon status Result: We can see that it's detecting the project home as well as a SQLite metadata database that just got created. It's also warning us that our DAG uses a connection that is not defined in the metadata database and suggesting us a fix. We will see in the next section 'Connections' how to add these. Bash/ZSH/Fish auto-complete bash eval \"$(_TYPHOON_COMPLETE=source_bash typhoon)\" zsh eval \"$(_TYPHOON_COMPLETE=source_zsh typhoon)\" fish eval \"$(_TYPHOON_COMPLETE=source_fish typhoon)\"","title":"Checking 'typhoon status'"},{"location":"getting-started/installation.html#with-docker","text":"@TODO DOCKER update - also in Readme.","title":"with docker"},{"location":"getting-started/installation.html#with-docker-and-airflow","text":"@TODO DOCKER update - also in Readme.","title":"with docker and Airflow"},{"location":"getting-started/typhoon-cli.html","text":"Typhoon cli First usage & help Inspired by other great command line interfaces, it will be instantly familiar to *nix and git users. Intelligent bash/zsh completion. typhoon init test_project typhoon status typhoon dag ls -l typhoon dag push test --dag-name example_dag Typing typhoon will show the list of options connection Manage Typhoon connections dag Manage Typhoon DAGs extension Manage Typhoon extensions init Create a new Typhoon project metadata Manage Typhoon metadata remote Manage Typhoon remotes status Information on project status variable Manage Typhoon variables You can use --help at each point, for example `typhoon connection --help will present: Options: --help Show this message and exit. Commands: add Add connection to the metadata store definition Connection definition in connections.yml ls List connections in the metadata store rm Remove connection from the metadata store Key cli usage: Starting a new project : typhoon init new_project (in your desired directory path) Checking status : typhoon status [ENV] This tells you information about the status of your project. Run typhoon status and it will find a typhoon.cfg file in the current directory. It is assumed that the typhoon project root is the directory that contains the typhoon.cfg. If you want to override that set the environment variable TYPHOON_HOME to the full path of the directory. Add a connection : e.g. typhoon connection add --conn-id data_lake --conn-env local Remember to set up your connections in connections.yml before you add them (data_lake is a default example). Build DAGs : typhoon build-dags . This will create the folder out/ in your Typhoon Home directory and also output to Airflow deployment if configured. Run a DAG : typhoon run --dag-name hello_world","title":"Typhoon Cli"},{"location":"getting-started/typhoon-cli.html#typhoon-cli","text":"","title":"Typhoon cli"},{"location":"getting-started/typhoon-cli.html#first-usage-help","text":"Inspired by other great command line interfaces, it will be instantly familiar to *nix and git users. Intelligent bash/zsh completion. typhoon init test_project typhoon status typhoon dag ls -l typhoon dag push test --dag-name example_dag Typing typhoon will show the list of options connection Manage Typhoon connections dag Manage Typhoon DAGs extension Manage Typhoon extensions init Create a new Typhoon project metadata Manage Typhoon metadata remote Manage Typhoon remotes status Information on project status variable Manage Typhoon variables You can use --help at each point, for example `typhoon connection --help will present: Options: --help Show this message and exit. Commands: add Add connection to the metadata store definition Connection definition in connections.yml ls List connections in the metadata store rm Remove connection from the metadata store","title":"First usage &amp; help"},{"location":"getting-started/typhoon-cli.html#key-cli-usage","text":"Starting a new project : typhoon init new_project (in your desired directory path) Checking status : typhoon status [ENV] This tells you information about the status of your project. Run typhoon status and it will find a typhoon.cfg file in the current directory. It is assumed that the typhoon project root is the directory that contains the typhoon.cfg. If you want to override that set the environment variable TYPHOON_HOME to the full path of the directory. Add a connection : e.g. typhoon connection add --conn-id data_lake --conn-env local Remember to set up your connections in connections.yml before you add them (data_lake is a default example). Build DAGs : typhoon build-dags . This will create the folder out/ in your Typhoon Home directory and also output to Airflow deployment if configured. Run a DAG : typhoon run --dag-name hello_world","title":"Key cli usage:"},{"location":"getting-started/using-component-ui.html","text":"Using the Component Builder UI @TODO - here something short about what its for. @TODO - more extended demo and a few use cases snowflake Pandas","title":"UI configurator"},{"location":"getting-started/using-component-ui.html#using-the-component-builder-ui","text":"@TODO - here something short about what its for. @TODO - more extended demo and a few use cases snowflake Pandas","title":"Using the Component Builder UI"},{"location":"getting-started/using-components.html","text":"Using Components What is a component? Components are ways of packaging up sets of regularly used tasks. This encourages modularity and re-use. This means a library of powerful workflows that are then simple dropping in 2-3 line YAML fragments. Components can be 'in-flow' using previous steps as inputs. Alternatively, 'standalone' ones can be built as DAG full templates. 'Standalone' components can use the Component Builder UI Example components: in-flow : if, glob_complress_and_copy, signer_demultiplexer standalone : db_to_snowflake, csv_my_pandas_transform @TODO - screenshot of UI In-flow component usage @ TOdo - usage of if and glob_compress Standalone Component usage @ Todo -usage of a simple component of transform @ Todo usage of Snowflake in code","title":"Using Components"},{"location":"getting-started/using-components.html#using-components","text":"","title":"Using Components"},{"location":"getting-started/using-components.html#what-is-a-component","text":"Components are ways of packaging up sets of regularly used tasks. This encourages modularity and re-use. This means a library of powerful workflows that are then simple dropping in 2-3 line YAML fragments. Components can be 'in-flow' using previous steps as inputs. Alternatively, 'standalone' ones can be built as DAG full templates. 'Standalone' components can use the Component Builder UI Example components: in-flow : if, glob_complress_and_copy, signer_demultiplexer standalone : db_to_snowflake, csv_my_pandas_transform @TODO - screenshot of UI","title":"What is a component?"},{"location":"getting-started/using-components.html#in-flow-component-usage","text":"@ TOdo - usage of if and glob_compress","title":"In-flow component usage"},{"location":"getting-started/using-components.html#standalone-component-usage","text":"@ Todo -usage of a simple component of transform @ Todo usage of Snowflake in code","title":"Standalone Component usage"},{"location":"usage/components.html","text":"","title":"Components"},{"location":"usage/dag-context.html","text":"","title":"Dag Context"},{"location":"usage/functions.html","text":"","title":"Functions"},{"location":"usage/hooks.html","text":"","title":"Hooks"},{"location":"usage/tasks.html","text":"","title":"Tasks"},{"location":"usage/transformations.html","text":"","title":"Transformations"}]}